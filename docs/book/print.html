<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>The-Block Documentation</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">The-Block Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>The Block is the unification layer for storage, compute, networking, and governance that turns verifiable work into CT rewards. Everything in the workspace is owned by the maintainers—no third-party stacks in consensus or networking—so the documentation describes what already ships in <code>main</code>, not a roadmap.</p>
<h2 id="mission"><a class="header" href="#mission">Mission</a></h2>
<ul>
<li>Operate a one-second base layer that notarizes micro-shard roots while keeping the L1 deterministic and audit-friendly.</li>
<li>Pay operators for real service (<code>STORAGE_SUB_CT</code>, <code>READ_SUB_CT</code>, <code>COMPUTE_SUB_CT</code>) instead of speculative gas schedules.</li>
<li>Treat governance as an engineering surface: the same crate powers the node, CLI, explorer, and telemetry so proposals, fee-floor policies, and service-badge status never drift.</li>
<li>Ship first-party clients: the in-house HTTP/TLS stack (<code>crates/httpd</code> + <code>crates/transport</code>) fronts every RPC, gateway, and gossip surface, and dependency pivots move through governance before they land in production.</li>
</ul>
<h2 id="responsibility-domains"><a class="header" href="#responsibility-domains">Responsibility Domains</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Domain</th><th>Repository roots</th><th>In-flight scope</th></tr></thead><tbody>
<tr><td>Consensus &amp; Ledger</td><td><code>node/src/consensus</code>, <code>node/src/blockchain</code>, <code>bridges</code>, <code>ledger</code>, <code>poh</code></td><td>Hybrid PoW/PoS leader schedule, macro-block checkpoints, Kalman retarget, ledger invariants, bridge proofs.</td></tr>
<tr><td>Serialization &amp; Tooling</td><td><code>crates/foundation_serialization</code>, <code>crates/codec</code>, <code>docs/spec/*.json</code></td><td>Canonical binary layout, cross-language vectors, CLI/SDK adapters.</td></tr>
<tr><td>Cryptography &amp; Identity</td><td><code>crypto</code>, <code>crates/crypto_suite</code>, <code>node/src/identity</code>, <code>dkg</code>, <code>zkp</code>, <code>remote_signer</code></td><td>Hash/signature primitives, DKG, commit–reveal, identity registries, PQ hooks.</td></tr>
<tr><td>Core Tooling &amp; UX</td><td><code>cli</code>, <code>gateway</code>, <code>explorer</code>, <code>metrics-aggregator</code>, <code>monitoring</code>, <code>docs/apis_and_tooling.md</code></td><td>RPC &amp; CLI surfaces, gateways, dashboards, probe CLI, release tooling.</td></tr>
</tbody></table>
</div>
<h2 id="design-pillars"><a class="header" href="#design-pillars">Design Pillars</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Pillar</th><th>Enforcement</th><th>Evidence</th></tr></thead><tbody>
<tr><td>Determinism</td><td><code>#![forbid(unsafe_code)]</code>, <code>codec::profiles</code>, ledger replay tests cross <code>x86_64</code>/<code>aarch64</code>.</td><td><code>cargo test -p the_block --test replay</code> and mdBook specs under <code>docs/architecture.md</code>.</td></tr>
<tr><td>Memory &amp; Thread Safety</td><td>First-party runtime, no <code>unsafe</code>, concurrency helpers in <code>crates/concurrency</code>.</td><td><code>miri</code>/ASan gates in CI, locking helpers (<code>MutexExt</code>, <code>DashMap</code>) wrap every shared structure.</td></tr>
<tr><td>Portability</td><td>Build matrix (Linux glibc/musl, macOS, Windows/WSL) plus <code>scripts/bootstrap.*</code>.</td><td><code>Justfile</code> + <code>Makefile</code> run the same steps locally and in CI; provenance signatures gate releases.</td></tr>
</tbody></table>
</div>
<h2 id="end-to-end-flow"><a class="header" href="#end-to-end-flow">End-to-End Flow</a></h2>
<ol>
<li><strong>Ingress</strong> – Gateways accept blobs and RPCs over the in-house <code>httpd</code> router, encrypt/store via <code>node/src/storage</code> and <code>storage_market</code> receipts, and emit signed <code>ReadAck</code> acknowledgements.</li>
<li><strong>Mempool &amp; Scheduling</strong> – <code>node/src/mempool</code> feeds the multi-lane scheduler (<code>node/src/scheduler.rs</code>) that batches consumer/industrial traffic, applies fee-floor policy, and records QoS counters.</li>
<li><strong>Consensus</strong> – The hybrid PoW/PoS engine (<code>node/src/consensus</code>) enforces macro-block checkpoints, PoH ticks, VDF randomness, and difficulty retune while gossip/range-boost propagate blocks.</li>
<li><strong>Rewarding &amp; Treasury</strong> – Subsidy accounting, service-badge tracking, treasury streaming, and governance DAG state live in <code>node/src/governance</code> and the shared <code>governance</code> crate; snapshots stream through CLI, explorer, aggregates, and telemetry.</li>
<li><strong>Observability &amp; Audits</strong> – Runtime telemetry (<code>node/src/telemetry.rs</code>), the metrics aggregator, dashboards under <code>monitoring/</code>, and runbooks in <code>docs/operations.md</code> keep operators in sync with governance hooks and incident tooling.</li>
</ol>
<h2 id="repository-layout-live-tree"><a class="header" href="#repository-layout-live-tree">Repository Layout (live tree)</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Path</th><th>Highlights</th></tr></thead><tbody>
<tr><td><code>node/</code></td><td>Full node, gateway stack, compute/storage/bridge/mempool modules, RPC server.</td></tr>
<tr><td><code>crates/</code></td><td>First-party libraries: transport, HTTP, serialization, overlay, runtime, coding/erasure, wallet SDKs.</td></tr>
<tr><td><code>cli/</code></td><td><code>tb-cli</code> binary with governance, bridge, wallet, identity, compute, telemetry, and remediation commands.</td></tr>
<tr><td><code>metrics-aggregator/</code></td><td>Aggregates Prometheus-style metrics, publishes dashboards, verifies TLS &amp; governance state.</td></tr>
<tr><td><code>monitoring/</code></td><td>Grafana/Prometheus templates and scripts (build via <code>npm ci --prefix monitoring</code>).</td></tr>
<tr><td><code>storage_market/</code>, <code>dex/</code>, <code>bridges/</code>, <code>gateway/</code></td><td>Dedicated crates for specialized subsystems referenced throughout the docs.</td></tr>
<tr><td><code>docs/</code></td><td>The consolidated handbook you are reading (mdBook enabled).</td></tr>
</tbody></table>
</div>
<h2 id="energy-market-snapshot"><a class="header" href="#energy-market-snapshot">Energy Market Snapshot</a></h2>
<ul>
<li><strong>Code surface</strong> — <code>crates/energy-market</code> implements providers, credits, receipts, and telemetry; <code>node/src/energy.rs</code> persists them in sled (<code>SimpleDb::open_named(names::ENERGY_MARKET, …)</code>), applies governance hooks, and exposes health checks. RPC handlers live in <code>node/src/rpc/energy.rs</code>, the CLI entry point is <code>cli/src/energy.rs</code>, and oracle ingestion goes through <code>crates/oracle-adapter</code> plus the <code>services/mock-energy-oracle</code> binary used by the World OS drill.</li>
<li><strong>State &amp; persistence</strong> — Energy state is serialized with <code>foundation_serialization::binary::{encode,decode}</code> and stored wherever <code>TB_ENERGY_MARKET_DIR</code> points (default <code>energy_market/</code>). Snapshots occur after every mutation, mirroring the fsync+rename workflow the rest of <code>SimpleDb</code> uses so restarts replay identical providers/credits/receipts. Governance parameters (<code>energy_min_stake</code>, <code>energy_oracle_timeout_blocks</code>, <code>energy_slashing_rate_bps</code>) share the same proposal pipeline as other params; once a proposal activates, <code>node::energy::set_governance_params</code> updates the runtime config and re-snapshots the sled DB.</li>
<li><strong>RPC &amp; CLI</strong> — The JSON-RPC namespace exposes <code>energy.register_provider</code>, <code>energy.market_state</code>, <code>energy.submit_reading</code>, and <code>energy.settle</code>. Requests use the exact schema documented in <code>docs/apis_and_tooling.md#energy-rpc-payloads-auth-and-error-contracts</code>, including the shared <code>MeterReadingPayload</code> used by oracle adapters, CLI tooling, and explorers. <code>tb-cli energy</code> prints tabular output by default, toggles JSON via <code>--verbose</code>/<code>--format json</code>, and pipes raw payloads to automation without diverging from the node schema.</li>
<li><strong>Observability &amp; operations</strong> — Runtime metrics include gauges (<code>energy_providers_count</code>, <code>energy_avg_price</code>), counters (<code>energy_kwh_traded_total</code>, <code>energy_settlements_total{provider}</code>), and histograms (<code>energy_provider_fulfillment_ms</code>, <code>oracle_reading_latency_seconds</code>). <code>node::energy::check_energy_market_health</code> logs warnings when pending credits pile up or settlements stall. <code>docs/testnet/ENERGY_QUICKSTART.md</code> plus <code>scripts/deploy-worldos-testnet.sh</code> describe the canonical bootstrap procedure (node + mock oracle + telemetry stack); <code>docs/operations.md#energy-market-operations</code> extends the runbook with backup, dispute, and alerting guidance.</li>
<li><strong>Security &amp; governance alignment</strong> — The outstanding work (oracle signature enforcement, dispute RPCs, explorer timelines, QUIC chaos drills, sled snapshot drills, release-provenance gates) is tracked in <code>docs/architecture.md#energy-governance-and-rpc-next-tasks</code> and summarized in <code>AGENTS.md</code>. <code>docs/security_and_privacy.md#energy-oracle-safety</code> documents key hygiene, secret sourcing, and telemetry redaction requirements for oracle adapters.</li>
</ul>
<h2 id="reference-workflow"><a class="header" href="#reference-workflow">Reference Workflow</a></h2>
<ol>
<li>Read <code>AGENTS.md</code> and this overview once—then work like you wrote them.</li>
<li>Run <code>scripts/bootstrap.sh</code> (or <code>.ps1</code>) to install Rust 1.86+, <code>cargo-nextest</code>, Python 3.12.3 venv, and toolchain shims.</li>
<li>Use <code>just lint</code>, <code>just fmt</code>, <code>just test-fast</code>, and <code>just test-full</code> to stay in sync with CI.</li>
<li>Keep dependency policy artifacts (<code>docs/dependency_inventory*.json</code>) up to date via <code>cargo run -p dependency_registry</code> or <code>just dependency-audit</code>.</li>
<li>Wire telemetry locally: <code>metrics-aggregator</code>, <code>monitoring/</code>, and <code>crates/probe</code> exercise the same endpoints operators rely on.</li>
</ol>
<h2 id="document-map"><a class="header" href="#document-map">Document Map</a></h2>
<p>All remaining detail sits in six focused guides:</p>
<ul>
<li><a href="architecture.html"><code>docs/architecture.md</code></a> — ledger, networking, storage, compute, bridges, gateway, telemetry.</li>
<li><a href="economics_and_governance.html"><code>docs/economics_and_governance.md</code></a> — CT supply, fees, treasury, proposals, service badges, kill switches.</li>
<li><a href="operations.html"><code>docs/operations.md</code></a> — bootstrap, deployments, telemetry wiring, dashboards, runbooks, chaos &amp; recovery.</li>
<li><a href="security_and_privacy.html"><code>docs/security_and_privacy.md</code></a> — threat modelling, cryptography, remote signer flows, jurisdiction policy packs, LE portal, supply-chain controls.</li>
<li><a href="developer_handbook.html"><code>docs/developer_handbook.md</code></a> — environment setup, coding standards, testing/fuzzing, simulation, dependency policy, tooling.</li>
<li><a href="apis_and_tooling.html"><code>docs/apis_and_tooling.md</code></a> — JSON-RPC, CLI, gateway HTTP &amp; DNS, explorer, probe CLI, metrics endpoints, schema references.</li>
</ul>
<p>For historical breadcrumbs the removed per-subsystem files now redirect through <a href="LEGACY_MAPPING.html"><code>docs/LEGACY_MAPPING.md</code></a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-reference-manual"><a class="header" href="#system-reference-manual">System Reference Manual</a></h1>
<p>This document consolidates the consensus, networking, storage, marketplace, governance, security, and tooling notes that previously lived across dozens of Markdown files. Every section points back to the exact modules that implement the behaviour so engineers and agents can cross‑check details against code before shipping changes.</p>
<hr />
<h2 id="1-consensus-serialization-and-sharding"><a class="header" href="#1-consensus-serialization-and-sharding">1. Consensus, Serialization, and Sharding</a></h2>
<h3 id="11-genesis-history-invariants-and-test-vectors"><a class="header" href="#11-genesis-history-invariants-and-test-vectors">1.1 Genesis history, invariants, and test vectors</a></h3>
<ul>
<li>The compile‑time genesis block is derived in <code>node/src/hash_genesis.rs</code>. Every field of <code>BlockEncoder</code> is zeroed except the fixed difficulty (<code>8</code>) and subsidy buckets. <code>calculate_genesis_hash()</code> is invoked inside a <code>const</code> context so <code>GENESIS_HASH</code> is embedded during compilation and asserted at runtime (<code>node/src/consensus/mod.rs</code>).</li>
<li>Invariants:
<ul>
<li><code>prev</code>, <code>fee_checksum</code>, and <code>state_root</code> point to <code>ZERO_HASH</code>.</li>
<li>All subsidy fields (<code>storage_sub</code>, <code>read_sub_*</code>, <code>compute_sub</code>) plus advertiser counters are zero to guarantee deterministic coinbase accounting.</li>
<li><code>read_root</code>, <code>vdf_commit</code>, <code>vdf_output</code> are 32‑byte zero arrays; <code>vdf_proof</code>, <code>tx_ids</code>, and <code>l2_*</code> arrays are empty.</li>
</ul>
</li>
<li>Test vectors: <code>node/src/hash_genesis.rs</code> exposes <code>calculate_genesis_hash_runtime()</code> and a regression test that ensures runtime and compile‑time hashes match. <code>node/tests/light_sync.rs</code> ships a minimal light‑client genesis header fixture to validate sync and PoH replay.</li>
</ul>
<h3 id="12-poh-and-vdf-parameters"><a class="header" href="#12-poh-and-vdf-parameters">1.2 PoH and VDF parameters</a></h3>
<ul>
<li>PoH ticks are generated by <code>node/src/poh.rs</code>. Each tick is a BLAKE3 hash of the previous tick; optional GPU acceleration piggybacks on the compute workloads.</li>
<li><code>timestamp_ticks</code> for mempool ordering and purge loops are recorded in nanoseconds (<code>SystemTime::now().duration_since(UNIX_EPOCH).as_nanos()</code>, see <code>node/src/lib.rs</code> around the mempool admission path). Block timestamps continue to use milliseconds for ledger compatibility.</li>
<li>VDF settings live in <code>node/src/config.rs::InflationConfig</code>. Defaults:
<ul>
<li><code>vdf_kappa = 1 &lt;&lt; 28</code> rounds (clamped via <code>set_vdf_kappa</code> when loading inflation files).</li>
<li>Hybrid PoW/PoS difficulty retune feeds the VDF by calling <code>node/src/consensus/vdf.rs</code>.</li>
</ul>
</li>
<li>Checkpoints:
<ul>
<li>Macro blocks emit every 100 micro‑blocks by default (<code>macro_interval</code> in <code>node/src/lib.rs</code>), anchoring shard roots and VDF outputs.</li>
<li>Replay harnesses (<code>tests/poh.rs</code>, <code>demo.py</code>) enforce deterministic replay by recomputing every tick and verifying <code>vdf_commit</code>, <code>vdf_output</code>, and proof bytes.</li>
</ul>
</li>
<li>Replay constraints: verifiers must recompute ticks sequentially, verify VDF proofs (currently sequential hash stand‑ins), and reject headers where <code>timestamp_ticks</code> drifts from the PoH timeline.</li>
</ul>
<h3 id="13-dkg-specifics"><a class="header" href="#13-dkg-specifics">1.3 DKG specifics</a></h3>
<ul>
<li>Implementation lives in the standalone <code>dkg/</code> crate. <code>SecretKeySet::random()</code> generates a deterministic polynomial seed per committee; shares authenticate themselves by XORing the digest with a derived per‑participant token.</li>
<li>Rotation schedule: <code>node/src/dkg.rs::run(participants, threshold)</code> is called by the consensus stack whenever validator sets rotate (macro‑block checkpoints plus governance toggles). <code>DKG_ROUND_TOTAL</code> tracks successful rounds so operators can confirm every rotation via telemetry.</li>
<li>Transcript materials:
<ul>
<li>Each participant keeps a <code>SecretKeyShare</code> (<code>id</code>, <code>seed</code>).</li>
<li>Aggregators collect <code>(participant_id, SignatureShare)</code> pairs, verify them via <code>combine_signatures</code>, and publish the resulting group signature/state inside sled.</li>
<li>Failure modes include <code>NotEnoughShares</code>, <code>InvalidShare</code>, and <code>MismatchedShares</code>; all map to RPC/CLI errors and increment telemetry counters for audit.</li>
</ul>
</li>
<li>Rotations reuse existing seeds to keep transcripts lightweight while the in‑house threshold scheme is finalized. The temporary design keeps transcripts in memory and never writes share material to disk.</li>
</ul>
<h3 id="14-hash-layout"><a class="header" href="#14-hash-layout">1.4 Hash layout</a></h3>
<p><code>node/src/hashlayout.rs</code> lists the canonical order for block fields. Highlights:</p>
<div class="table-wrapper"><table><thead><tr><th>Field order</th><th>Description</th></tr></thead><tbody>
<tr><td><code>index</code>, <code>prev</code>, <code>timestamp</code>, <code>nonce</code>, <code>difficulty</code>, <code>retune_hint</code>, <code>base_fee</code></td><td>Header core; <code>retune_hint</code> is a signed byte summarising the Kalman filter trend.</td></tr>
<tr><td>Subsidy buckets (<code>coin_c</code>, <code>coin_i</code>, <code>storage_sub</code>, <code>read_sub</code>, <code>read_sub_{viewer,host,hardware,verifier,liquidity}</code>, ad* fields, <code>compute_sub</code>, <code>proof_rebate</code>, industrial sub‑ledgers)</td><td>Every reward lane is hashed before state roots so subsidies bind to the ledger.</td></tr>
<tr><td><code>read_root</code>, <code>fee_checksum</code>, <code>state_root</code></td><td>Merkle roots; <code>fee_checksum</code> protects the per‑block fee accumulator.</td></tr>
<tr><td><code>l2_roots</code>, <code>l2_sizes</code> (variable length)</td><td>Deterministic ordering; lengths are encoded before bytes.</td></tr>
<tr><td><code>vdf_commit</code>, <code>vdf_output</code>, <code>len(vdf_proof)</code>, <code>vdf_proof</code>, <code>tx_ids</code></td><td>Final section to keep PoH/VDF data and transaction IDs adjacent.</td></tr>
</tbody></table>
</div>
<p>Never reorder or remove fields; instead add new suffix fields and bump the hash layout tests.</p>
<h3 id="15-schema-migrations-v7v10"><a class="header" href="#15-schema-migrations-v7v10">1.5 Schema migrations (v7–v10)</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Version</th><th>Scope</th><th>Pre‑migration state</th><th>Post‑migration state</th><th>Operator expectations</th></tr></thead><tbody>
<tr><td>v7</td><td>Recent timestamp journaling (<code>docs/schema_migrations/v7_recent_timestamps.md</code> ➜ <code>Blockchain::recent_timestamps</code>)</td><td>Mempool entries only tracked <code>timestamp_millis</code>; restarts lost ordering guarantees.</td><td><code>timestamp_ticks</code> persisted alongside millis so QoS lanes continue after restarts (<code>node/tests/reopen.rs</code>).</td><td>Allow the node to reopen once so it can rewrite mempool entries; no manual action beyond monitoring <code>telemetry::STARTUP_TTL_DROP_TOTAL</code>.</td></tr>
<tr><td>v8</td><td>Bridge header persistence (<code>docs/schema_migrations/v8_bridge_headers.md</code>)</td><td>Bridge proofs not materialised in sled; challenge windows could not replay after crash.</td><td><code>simple_db::names::BRIDGE</code> stores verified headers and pending withdrawals; CLI exposes `tb-cli bridge pending</td><td>disputes`.</td></tr>
<tr><td>v9</td><td>DEX escrow snapshots</td><td>Escrow state stored only in memory; C/R flows risked desync.</td><td><code>simple_db::names::DEX_STORAGE</code> now stores <code>EscrowState</code> (orders, locks, HTLC proofs).</td><td>Before upgrading, pause matching, run <code>tb-cli dex escrow export</code>; after upgrade confirm <code>dex.order_book</code> RPC matches expectations.</td></tr>
<tr><td>v10</td><td>Industrial subsidy buckets</td><td>Subsidy accounting was aggregated; per‑lane reporting missing.</td><td><code>Block::industrial_subsidies()</code> writes <code>storage_sub_it</code>, <code>read_sub_it</code>, <code>compute_sub_it</code>; governance surfaces via explorer and CLI.</td><td>Update dashboards to use the new metrics (<code>INDUSTRIAL_SUBSIDY_*</code>); re-run treasury audits to ensure ledgers reconcile.</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="2-transactions-fees-and-mempool"><a class="header" href="#2-transactions-fees-and-mempool">2. Transactions, Fees, and Mempool</a></h2>
<h3 id="21-account-abstraction-specifics"><a class="header" href="#21-account-abstraction-specifics">2.1 Account abstraction specifics</a></h3>
<ul>
<li>Handle registry (<code>node/src/identity/handle_registry.rs</code>) replaces the former <code>account_abstraction.md</code>: handles normalize via NFKC, reject <code>sys/</code> &amp; <code>admin/</code> prefixes, and require strictly increasing nonces. Records are stored under <code>simple_db::names::IDENTITY_HANDLES</code>.</li>
<li>Fee hooks (<code>node/src/fees.rs</code>, <code>node/src/fee/policy.rs</code>) enforce percentile‑based fee floors per lane. Wallets and RPC clients read <code>FeeLane</code> metadata to decide whether to dip into rebates before CT.</li>
<li>When submitting transactions:
<ul>
<li>Signature threshold vs signer set is checked before admission; failing multisig payloads are staged in <code>pending_multisig</code>.</li>
<li>Session keys (account‑level) live in sled with expiration timestamps so headless clients can register ephemeral signing keys.</li>
</ul>
</li>
</ul>
<h3 id="22-base-fee-algorithm"><a class="header" href="#22-base-fee-algorithm">2.2 Base fee algorithm</a></h3>
<p><code>node/src/fees.rs::next_base_fee</code> implements an EIP‑1559‑style update:</p>
<pre><code>Δ = prev * (gas_used - target) / target / 8
next = max(prev + Δ, 1)
</code></pre>
<ul>
<li><code>TARGET_GAS_PER_BLOCK = 1_000_000</code>.</li>
<li><code>Δ</code> is clamped by dividing by 8 (12.5 % per block). The base fee never drops below 1 (micro‑CT per byte).</li>
<li>Telemetry: <code>base_fee</code> gauge plus <code>mempool_fee_floor_*</code> histograms show nudges per block. <code>node/tests/base_fee.rs</code> exercises the path.</li>
</ul>
<h3 id="23-qos-accounting-ttl-math-and-rpc-surfaces"><a class="header" href="#23-qos-accounting-ttl-math-and-rpc-surfaces">2.3 QoS accounting, TTL math, and RPC surfaces</a></h3>
<ul>
<li>Admission + per‑sender limits (<code>node/src/mempool/admission.rs</code>): <code>AdmissionState::reserve_sender</code> tracks outstanding slots keyed by <code>(sender, lane)</code>. Defaults:
<ul>
<li><code>max_pending_per_account = 16</code> (overrideable via <code>tb-cli node mempool set-cap</code> or <code>TB_MEMPOOL_ACCOUNT_CAP</code>).</li>
<li><code>max_mempool_size_{consumer,industrial} = 1_024</code> entries each.</li>
<li><code>min_fee_per_byte_{consumer,industrial}</code> defaults to 1 µCT/byte but is immediately clamped to the rolling floor.</li>
</ul>
</li>
<li>Rolling floor: every admitted transaction records its <code>fee_per_byte</code>. The percentile window is configured by governance (<code>fee_floor_window</code>, <code>fee_floor_percentile</code>) so QoS can insist on e.g. “p75 of the last 512 admits.” When callers post <code>mempool.qos_event</code> we log overrides (<code>FEE_FLOOR_WARNING_TOTAL</code> / <code>FEE_FLOOR_OVERRIDE_TOTAL</code>).</li>
<li>TTL ordering: <code>MempoolEntry::expires_at = timestamp_millis + (tx_ttl * 1000)</code> (<code>node/src/lib.rs:806–834</code>). <code>mempool_cmp</code> sorts by:
<ol>
<li><code>tip / serialized_size</code> (descending),</li>
<li>soonest expiry,</li>
<li>transaction hash.
Any entry whose <code>now - timestamp_millis &gt; tx_ttl * 1000</code> is dropped during <code>purge_expired()</code>, contributing to <code>TTL_DROP_TOTAL</code>.</li>
</ol>
</li>
<li>Lane rotation: consumers and industrial transactions live in separate <code>DashMap</code>s but are merged before block assembly (<code>Blockchain::mine_block_with_ts</code>). Because both lanes share the same comparator they naturally interleave when fees are comparable, yet state accounting stays lane-aware via <code>FeeLane</code> and <code>fee::decompose</code>.</li>
<li>Deferred nonces: while iterating the merged list we only include a sender’s next expected nonce. Higher nonces accumulate in <code>deferred</code> until the gap closes, preventing starvation from users who pre-sign long sequences.</li>
<li>Tx errors: <code>TxAdmissionError</code> carries the numeric code returned to RPC clients. Relevant constants live near <code>node/src/lib.rs:6238</code>. The table below helps map RPC failures to on-chain causes.</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Error</th><th>Code (<code>TxAdmissionError::code</code>)</th><th>Typical RPC code</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>FeeTooLow</code></td><td>10</td><td><code>error_value("FeeTooLow")</code></td><td>Floor check failed – inspect <code>mempool.stats.fee_floor</code>.</td></tr>
<tr><td><code>MempoolFull</code></td><td>11</td><td><code>error_value("MempoolFull")</code></td><td>Both lanes at capacity; eviction refused.</td></tr>
<tr><td><code>PendingLimitReached</code></td><td>13</td><td><code>error_value("PendingLimitReached")</code></td><td>Sender already has <code>max_pending_per_account</code> entries.</td></tr>
<tr><td><code>PendingSignatures</code></td><td>17</td><td><code>status="pending_signatures"</code></td><td>Multisig payload staged in <code>pending_multisig</code>.</td></tr>
<tr><td><code>SessionExpired</code></td><td>18</td><td><code>error_value("SessionExpired")</code></td><td>Session keys (CLI <code>--session</code>) expired; renew before resubmitting.</td></tr>
<tr><td><code>UnknownSender</code>, <code>NonceGap</code>, <code>InsufficientBalance</code>, …</td><td>See <code>node/src/lib.rs</code></td><td><code>error_value("&lt;Variant&gt;")</code></td><td>Surfaced directly from the admission path.</td></tr>
</tbody></table>
</div>
<ul>
<li>QoS telemetry: <code>submit_tx</code> increments <code>TX_SUBMITTED_TOTAL</code>; successful admissions increment <code>TX_ADMITTED_TOTAL</code> and log the caller’s jurisdiction for compliance. Rejections emit <code>tx_rejected_total{reason}</code> samples so dashboards can correlate floods.</li>
</ul>
<h4 id="mempoolstats-payload"><a class="header" href="#mempoolstats-payload"><code>mempool.stats</code> payload</a></h4>
<p>RPC <code>mempool.stats</code> (<code>node/src/rpc/mod.rs:1445</code>) reads the current lane snapshot without mutating the pool. Each field is in milliseconds or micro‑CT as noted below.</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>size</code></td><td>Number of entries in the requested lane.</td></tr>
<tr><td><code>age_p50</code> / <code>age_p95</code></td><td>Milliseconds since admission (median / 95th percentile).</td></tr>
<tr><td><code>fee_p50</code> / <code>fee_p90</code></td><td>Raw <code>tip</code> percentiles (µCT). Divide by serialized size for fee per byte.</td></tr>
<tr><td><code>fee_floor</code></td><td>Current percentile floor after min‑fee clamps.</td></tr>
</tbody></table>
</div>
<p>Example:</p>
<pre><code class="language-json">{
  "size": 742,
  "age_p50": 1184,
  "age_p95": 5443,
  "fee_p50": 1200,
  "fee_p90": 3500,
  "fee_floor": 900
}
</code></pre>
<p>When consumer latency jumps above <code>comfort_threshold_p90</code> (governance parameter) the scheduler raises the base fee which in turn bumps lane floors; watch <code>mempool_fee_floor_current{lane}</code> for confirmation.</p>
<h3 id="24-transaction-scheduler-fairness-windows-and-conflict-guards"><a class="header" href="#24-transaction-scheduler-fairness-windows-and-conflict-guards">2.4 Transaction scheduler, fairness windows, and conflict guards</a></h3>
<ul>
<li><code>node/src/scheduler.rs</code> drives the service pipeline. <code>ServiceScheduler</code> owns three classes (<code>gossip</code>, <code>compute</code>, <code>storage</code>) with weights exposed as governance parameters (<code>scheduler_weight_gossip</code>, <code>scheduler_weight_compute</code>, <code>scheduler_weight_storage</code>). <code>governance::registry</code> plumbs new values directly into the atomic defaults so the change is visible to the next block without restarting.</li>
<li>Pick logic: in re‑entrant builds we maintain <code>(current_class, budget, last_idx)</code>. Each dequeue consumes one unit of the class budget; once the budget hits zero or the queue empties we rotate to the next non‑empty class. In non‑re‑entrant builds we rotate a ring buffer of <code>(class, queue_len)</code> pairs so large bursts cannot starve lower-weight lanes.</li>
<li>Batch size: callers (e.g., the gossip reactor or execution pipeline) pass <code>limit</code> into <code>drain(limit)</code> or <code>execute_ready(limit)</code>. A typical block assembly requests <code>limit = dynamic_block_limit()</code>; gossip workers usually ask for 64 messages. Because the scheduler enforces per-class weights the effective fairness window equals <code>weight</code> dequeues before the class yields to its peers.</li>
<li>Starvation guards: <code>ServiceSchedulerStats</code> exposes <code>queue_depths</code> + <code>weights</code> via diagnostics so operators can confirm configuration. If a class enters <code>reentrant_enabled</code> mode with weight zero its queues will still be scanned periodically to avoid livelock. For transaction execution, <code>TxScheduler</code> builds read/write sets (<code>TxRwSet</code>) and refuses to schedule transactions whose inputs or outputs overlap an active run (“RW conflict”). Conflicts are surfaced as <code>ScheduleError::Conflict(txid)</code> so block builders can defer the losing transaction without discarding it.</li>
<li>Governance knobs: to bias toward consumer traffic during incidents set <code>scheduler_weight_gossip=4</code>, <code>scheduler_weight_compute=1</code>, <code>scheduler_weight_storage=1</code>, then monitor <code>scheduler.stats</code> RPC to ensure queue depths drop as expected. <code>tb-cli diagnostics scheduler</code> prints the same struct locally.</li>
</ul>
<hr />
<h2 id="3-networking-overlay-and-p2p"><a class="header" href="#3-networking-overlay-and-p2p">3. Networking, Overlay, and P2P</a></h2>
<h3 id="31-wire-protocol-catalog"><a class="header" href="#31-wire-protocol-catalog">3.1 Wire protocol catalog</a></h3>
<p><code>node/src/net/message.rs</code> defines signed envelopes plus the payloads below (see Appendix D for field tables). Compatibility notes:</p>
<ul>
<li><code>Payload::Handshake(Hello)</code> negotiates <code>proto_version</code>, feature bits, transport capabilities, and QUIC certificate fingerprints (<code>p2p/handshake.rs</code>).</li>
<li><code>Hello(Vec&lt;SocketAddr&gt;)</code> is the legacy peer advertisement message; it remains for bootstrap compatibility.</li>
<li><code>Tx</code>, <code>BlobTx</code>, <code>Block</code>, <code>Chain</code> propagate consensus data. Transactions include full serialization so mempools can revalidate.</li>
<li><code>BlobChunk</code> carries erasure‑coded shards with <code>index</code>/<code>total</code> metadata; peers deduplicate by root.</li>
<li><code>Reputation(Vec&lt;ReputationUpdate&gt;)</code> syncs peer scores between overlay nodes.</li>
</ul>
<h3 id="32-quic-handshake-and-fallback-rules"><a class="header" href="#32-quic-handshake-and-fallback-rules">3.2 QUIC handshake and fallback rules</a></h3>
<ul>
<li>Hello extensions (<code>p2p/handshake.rs</code>):
<ul>
<li><code>quic_provider</code>: optional provider ID; resolved to Quinn, s2n‑quic, or the in‑house transport at runtime.</li>
<li><code>quic_capabilities</code>: vector of feature strings (<code>certificate_rotation</code>, <code>mtls</code>, etc.).</li>
<li><code>quic_cert</code> + <code>quic_fingerprint{,_previous}</code>: remote certificate payload and rolling fingerprints.</li>
</ul>
</li>
<li>Validation flow:
<ol>
<li>Ensure <code>network_id</code> and <code>proto_version</code> match (<code>SUPPORTED_VERSION = 1</code>).</li>
<li>Check required feature bits (governance toggles). Missing bits ➜ <code>HandshakeError::MissingFeatures</code>.</li>
<li>If the peer advertises QUIC, infer the provider, verify the cert (in‑house adapter for s2n), and compare fingerprints. Failures increment <code>QUIC_HANDSHAKE_FAIL_TOTAL{peer,reason}</code>.</li>
<li>Replay protection: accepted peers are recorded in <code>PEERS</code> (lazy mutex). <code>peer_provider</code>/<code>peer_capabilities</code> surfaces this cache for diagnostics.</li>
</ol>
</li>
<li>Fallbacks: if QUIC is unavailable or misconfigured, the node retains TCP transport and advertises <code>Transport::Tcp</code>.</li>
</ul>
<h3 id="33-a-heuristics-swarm-presets-and-bootstrap"><a class="header" href="#33-a-heuristics-swarm-presets-and-bootstrap">3.3 A* heuristics, swarm presets, and bootstrap</a></h3>
<ul>
<li><code>node/src/net/a_star.rs</code> caches ASN latency floors with an LRU of configurable capacity. Heuristic = floor latency + <code>μ * (1 - uptime)</code>, where <code>μ</code> defaults to <code>governance::heuristic_mu_milli / 1000</code>.</li>
<li>Swarm/bootstrap:
<ul>
<li>Static peers via <code>TB_BOOTSTRAP_PEERS</code> or CLI seeds; overlay peer stores live in <code>p2p_overlay</code>.</li>
<li>Gossip mesh uses <code>TB_PARTITION_TAG</code> to segregate drill traffic; range-boost peers are configured via <code>TB_MESH_STATIC_PEERS</code>.</li>
</ul>
</li>
<li>Topologies:
<ul>
<li>Core swarm: fanout picks peers with lowest heuristic, bounded by shard affinity.</li>
<li>Range Boost: <code>node/src/range_boost/mod.rs</code> maintains TLS/Unix/TCP peer latency caches, resynchronises forwarder threads, and enqueues bundles with exponential backoff.</li>
</ul>
</li>
</ul>
<h3 id="34-partition-detection-runbook"><a class="header" href="#34-partition-detection-runbook">3.4 Partition detection runbook</a></h3>
<ul>
<li><code>PartitionWatch</code> (<code>node/src/net/partition_watch.rs</code>) tracks unreachable peers; default threshold = 8.</li>
<li>When <code>mark_unreachable</code> pushes the count past the threshold, <code>PARTITION_EVENTS_TOTAL</code> increments and <code>active=true</code>. Operators should:
<ol>
<li>Query <code>tb-cli net partition</code> (via diagnostics) to list isolated peer IDs.</li>
<li>Inspect <code>partition_watch.current_marker()</code> to correlate with dashboards/incident logs.</li>
<li>Run the recovery recipe: flush peer bans, reseed overlay via CLI, confirm <code>is_partitioned()</code> flips back to false.</li>
</ol>
</li>
</ul>
<h3 id="35-overlay-store-and-peer-persistence"><a class="header" href="#35-overlay-store-and-peer-persistence">3.5 Overlay store and peer persistence</a></h3>
<ul>
<li>Backends: <code>node/src/config.rs</code> exposes <code>overlay.backend = inhouse | stub</code>. The in-house backend (<code>crates/p2p_overlay::InhouseOverlay</code>) persists peers under <code>TB_OVERLAY_DB_PATH</code> (defaults to <code>~/.the_block/overlay</code>). Records are pretty‑printed JSON containing the base58 peer ID, socket address, and <code>last_seen</code> epoch seconds. The stub backend (memory only) is useful for deterministic tests.</li>
<li>Discovery + RPC selection: <code>node/src/net/discovery.rs</code> re‑exports the trait so CLI/RPC consumers can issue <code>net.overlay_status</code> and <code>tb-cli net overlay-status --format json</code>. Switching backend at runtime is safe; <code>net::overlay_service()</code> holds the chosen implementation in a read/write lock, and <code>governance::registry</code> wires up policy changes.</li>
<li>Migration: <code>scripts/migrate_overlay_store.rs</code> converts legacy bincode sled trees into the new JSON format. Invoke it before flipping <code>overlay.backend</code> on existing nodes so peer histories are retained.</li>
<li>Peer metrics store: <code>node/src/net/peer_metrics_store.rs</code> writes compressed records into <code>sled</code> (<code>peer_metrics</code> tree). Values are encoded via <code>peer_metrics_binary::{encode, decode}</code> (compact bincode) and keyed by <code>&lt;peer-pk&gt;&lt;timestamp&gt;</code> which enables retention pruning. <code>peer.metrics_export</code> CLI/RPC commands also read from this tree.</li>
<li>Ban store: <code>node/src/net/ban_store.rs</code> keeps bans in <code>SimpleDb::open_named(names::NET_BANS, TB_BAN_DB)</code>. Entries are hex peer IDs with an 8‑byte timestamp payload; <code>ban_store::purge_expired()</code> enforces TTL and updates <code>BANNED_PEERS_TOTAL</code>.</li>
<li>Ancillary stores: <code>TB_PEER_DB_PATH</code> and <code>TB_QUIC_PEER_DB_PATH</code> house the bincode peer reputation cache (<code>peer.rs</code>), <code>TB_PEER_METRICS_PATH</code> controls on-disk export, and <code>TB_NET_CERT_STORE_PATH</code> stores mutual-TLS cert chains. Back them up together when migrating a node.</li>
</ul>
<hr />
<h2 id="4-localnet-range-boost-and-headlessai-tooling"><a class="header" href="#4-localnet-range-boost-and-headlessai-tooling">4. LocalNet, Range Boost, and Headless/AI Tooling</a></h2>
<h3 id="41-range-boost-queue-semantics"><a class="header" href="#41-range-boost-queue-semantics">4.1 Range-boost queue semantics</a></h3>
<ul>
<li>Queue is a <code>VecDeque&lt;QueueEntry&gt;</code> guarded by a mutex; every enqueue records <code>enqueued_at</code>.</li>
<li>Configurable fault modes (<code>FaultMode::{ForceDisabled,ForceNoPeers,ForceEncode,ForceIo}</code>) inject chaos for drills.</li>
<li>Courier/forwarder loop:
<ul>
<li>Sleep durations: idle = 200 ms, retry = 250 ms, disabled = 1 s.</li>
<li>Forward failures requeue the bundle at the front to preserve FIFO semantics.</li>
<li>Metrics: <code>RANGE_BOOST_QUEUE_DEPTH</code>, <code>RANGE_BOOST_QUEUE_OLDEST_SECONDS</code>, <code>RANGE_BOOST_FORWARDER_FAIL_TOTAL</code>, <code>MESH_PEER_LATENCY_MS</code>.</li>
</ul>
</li>
</ul>
<h3 id="42-localnet-proximity-proofs"><a class="header" href="#42-localnet-proximity-proofs">4.2 LocalNet proximity proofs</a></h3>
<p><code>node/src/localnet/proximity.rs</code> + <code>config/localnet_devices.toml</code> define class‑specific RSSI/RTT envelopes:</p>
<div class="table-wrapper"><table><thead><tr><th>Device class</th><th>Minimum RSSI</th><th>Maximum RTT</th></tr></thead><tbody>
<tr><td>Phone</td><td>−75 dBm</td><td>150 ms</td></tr>
<tr><td>Laptop</td><td>−80 dBm</td><td>200 ms</td></tr>
<tr><td>Router</td><td>−85 dBm</td><td>250 ms</td></tr>
</tbody></table>
</div>
<p><code>LocalNet::validate_proximity</code> returns <code>false</code> for out‑of‑range readings; callers include RPC methods (<code>localnet.submit_receipt</code>) and diagnostics tools. Receipts (<code>AssistReceipt</code>) embed provider, region, device class, RSSI, RTT, and Ed25519 signatures hashed via BLAKE3 for anchoring.</p>
<h3 id="43-headless-and-ai-diagnostics"><a class="header" href="#43-headless-and-ai-diagnostics">4.3 Headless and AI diagnostics</a></h3>
<ul>
<li>CLI (<code>cli/src/ai.rs</code>):
<ul>
<li><code>tb-cli ai diagnose --snapshot metrics.json</code> loads a JSON blob, evaluates heuristics (currently latency thresholds), and prints remediation tips.</li>
<li><code>Metrics</code> struct contains <code>avg_latency_ms</code>; extend it as headless tooling matures.</li>
</ul>
</li>
<li>Governance parameter <code>ai_diagnostics_enabled</code> (see §10) gates ANN‑based alerts in node + aggregator.</li>
<li>Headless flows share the same JSON codec helpers as RPC so suggestions are deterministic and auditable.</li>
</ul>
<hr />
<h2 id="5-storage-state-simpledb-and-persistence"><a class="header" href="#5-storage-state-simpledb-and-persistence">5. Storage, State, SimpleDb, and Persistence</a></h2>
<h3 id="51-pipeline-stages-and-knobs"><a class="header" href="#51-pipeline-stages-and-knobs">5.1 Pipeline stages and knobs</a></h3>
<p><code>node/src/storage/pipeline.rs</code> stages:</p>
<div class="table-wrapper"><table><thead><tr><th>Stage</th><th>Knobs / defaults</th><th>Effect</th></tr></thead><tbody>
<tr><td>Chunk sizing</td><td><code>settings::chunk_defaults()</code> (default 1 MiB with ladder <code>[256 KiB, 512 KiB, 1 MiB, 2 MiB, 4 MiB]</code>).</td><td><code>clamp_to_ladder</code> adjusts chunk size to hit 3 s target writes.</td></tr>
<tr><td>Compression</td><td><code>settings::compression_level()</code> and algorithm selection; fallback/per‑algorithm telemetry via <code>record_coding_result</code>.</td><td>Chooses compressor (e.g., zstd, RLE fallback) per manifest.</td></tr>
<tr><td>Encryption</td><td><code>settings::encryptor()</code> or per‑manifest override.</td><td>Wraps chunk bytes before erasure coding.</td></tr>
<tr><td>Erasure coding</td><td><code>settings::erasure_counts()</code> plus <code>ErasureParams</code>.</td><td>Default Reed–Solomon with fountain overlay (see §5.2).</td></tr>
<tr><td>Placement</td><td><code>storage::placement::{NodeCatalog, NodeHandle}</code>.</td><td>Selects providers respecting rent escrow, redundancy, RTT telemetry.</td></tr>
<tr><td>Repair</td><td><code>storage::repair</code> monitors health, reissues pulls, updates readiness metrics.</td><td></td></tr>
</tbody></table>
</div>
<h3 id="52-erasure-coding-parameters"><a class="header" href="#52-erasure-coding-parameters">5.2 Erasure coding parameters</a></h3>
<ul>
<li>Default (governance overrideable): RS data/parity counts from <code>settings::erasure_counts()</code>; overlay adds 3 fountain shards over the first 2 RS shards.</li>
<li>Failure tolerance: RS handles up to <code>parity_shards</code> losses; overlay can reconstruct early losses (<code>try_fill_from_overlay</code>) even when the first two shards vanish.</li>
<li>Encoding/decoding record success/failure under <code>erasure_encode</code> and <code>erasure_reconstruct</code> metrics; any fallback/enforced coder is logged as an emergency event if <code>TB_CODING_FALLBACK_EMERGENCY_*</code> envs are set.</li>
</ul>
<h3 id="53-storage-market-incentives"><a class="header" href="#53-storage-market-incentives">5.3 Storage-market incentives</a></h3>
<ul>
<li><code>storage_market/src/lib.rs</code> models each contract as <code>ContractRecord { contract: StorageContract, replicas: Vec&lt;ReplicaIncentive&gt; }</code>.</li>
<li><code>ReplicaIncentive</code> fields: <code>allocated_shares</code>, <code>price_per_block</code>, <code>deposit_ct</code>, success/failure counters, last proof block, last outcome.</li>
<li>SLAs recorded via <code>record_proof_outcome</code>:
<ul>
<li>Success ➜ increment <code>proof_successes</code>, no slashing.</li>
<li>Failure ➜ deduct <code>min(price_per_block, deposit_ct)</code> and increment <code>proof_failures</code>.</li>
</ul>
</li>
<li>CLI/RPC:
<ul>
<li><code>storage_market.provider_balances</code>, <code>storage_market.audit_log</code>, and explorer views show deposits, accrued CT, and slashes.</li>
</ul>
</li>
</ul>
<h3 id="54-simpledb-keyspaces"><a class="header" href="#54-simpledb-keyspaces">5.4 SimpleDb keyspaces</a></h3>
<p>SimpleDb uses named column families (CFs) declared in <code>node/src/simple_db/mod.rs::names</code>. See Appendix C for the complete cf→subsystem map and prefix conventions.</p>
<h3 id="55-wal-snapshots-and-pruning"><a class="header" href="#55-wal-snapshots-and-pruning">5.5 WAL, snapshots, and pruning</a></h3>
<ul>
<li>WAL: in-house engine writes JSON lines per CF at <code>&lt;cf_path&gt;/wal.log</code> (<code>crates/storage_engine/src/inhouse_engine.rs</code>). After each memtable flush the WAL is truncated to keep replay bounded. Operators can force a flush via <code>SimpleDb::flush_wal()</code>.</li>
<li>Ledger snapshots:
<ul>
<li><code>node/src/blockchain/snapshot.rs</code> writes account snapshots plus diffs under <code>&lt;data_dir&gt;/snapshots/&lt;height&gt;.bin</code>.</li>
<li>CLI helper (<code>node/src/bin/snapshot.rs</code>) supports <code>snapshot create|apply|list</code>.</li>
<li>State snapshots (<code>state/src/snapshot.rs</code>) capture trie contents, keep the last <code>keep</code> images, and log engine migrations.</li>
</ul>
</li>
<li>Pruning: <code>SnapshotManager::prune()</code> sorts snapshots by mtime, retains <code>keep</code>, and deletes the rest. Set <code>TB_SNAPSHOT_KEEP</code> to change the retention window.</li>
</ul>
<h3 id="56-manifest--storereceipt-layout"><a class="header" href="#56-manifest--storereceipt-layout">5.6 Manifest + <code>StoreReceipt</code> layout</a></h3>
<ul>
<li>Binary schema lives in <code>node/src/storage/manifest_binary.rs</code>. Each object encodes:
<ul>
<li>Core fields: <code>version</code>, <code>total_len</code>, <code>chunk_len</code>, redundancy (<code>Redundancy::ReedSolomon { data, parity }</code> or <code>None</code>), encrypted content key, and BLAKE3 digest.</li>
<li>Chunk metadata: plaintext/compressed/cipher lengths plus <code>ChunkRef { id, nodes, provider_chunks[] }</code> so repair can fall back to either manifest-level providers or per-chunk overrides.</li>
<li>Provider hints: <code>provider_chunks</code> map providers to the chunk indices they host and carry per-provider encryption keys when overrides are enabled.</li>
</ul>
</li>
<li>Receipts (<code>storage::types::StoreReceipt</code>) mirror the manifest hash, chunk count, redundancy enum, and lane label (<code>consumer</code> or <code>industrial</code>). They are serialized via the same helper and embedded into coinbase accounting so explorers can correlate manifests with payouts.</li>
</ul>
<h3 id="57-rent-escrow-releases-and-proofs"><a class="header" href="#57-rent-escrow-releases-and-proofs">5.7 Rent escrow, releases, and proofs</a></h3>
<ul>
<li><code>RentEscrow</code> (<code>node/src/storage/fs.rs</code>) persists deposits in <code>SimpleDb::open_named(names::STORAGE_FS, TB_STORAGE_PIPELINE_DIR)</code>. API summary:
<ul>
<li><code>lock(id, depositor, amount, expiry)</code> writes a record and increments <code>RENT_ESCROW_LOCKED_CT_TOTAL</code>.</li>
<li><code>release(id)</code> removes the record, refunds 90 % to the depositor, burns 10 % (tracked via <code>RENT_ESCROW_REFUNDED_CT_TOTAL</code>/<code>RENT_ESCROW_BURNED_CT_TOTAL</code>), and returns <code>(account, refund, burn)</code>.</li>
<li><code>purge_expired(now)</code> sweeps stale locks and emits identical telemetry. Expiry checks happen before every rent escrow lookup so operators only need to call it when clocks skew.</li>
</ul>
</li>
<li>Receipts tie escrows to manifests: <code>storage::pipeline</code> pre-funds rent via escrow IDs, and the settlement engine drops a receipt only when both manifest and escrow landed on disk. Explorer endpoints display the rent escrow balance alongside the <code>StoreReceipt</code>.</li>
</ul>
<h3 id="58-provider-profile-schema-and-telemetry"><a class="header" href="#58-provider-profile-schema-and-telemetry">5.8 Provider profile schema and telemetry</a></h3>
<ul>
<li>Profiles (<code>node/src/storage/pipeline.rs::ProviderProfile</code>) record the EWMA throughput, RTT, and loss history per provider together with rolling success rates and maintenance flags. They ensure we down-rank providers whose recent uploads failed.</li>
<li>Binary encoding lives in <code>node/src/storage/pipeline/binary.rs</code>: 13 fixed fields captured as <code>f64</code>/<code>u32</code>/<code>u64</code> triplets. The profile snapshot (<code>ProviderProfileSnapshot { provider, profile, quota_bytes }</code>) drives explorer dashboards and CLI dumps.</li>
<li>Governance knobs indirectly influence profiles: chunk ladder defaults, erasure parameters, and rent escrow requirements all change the amount of traffic a provider sees, which in turn updates the EWMA. <code>storage.provider_profiles</code> RPC returns the serialized profile objects for post-processing.</li>
</ul>
<h3 id="59-storage-importer-and-audit-cli"><a class="header" href="#59-storage-importer-and-audit-cli">5.9 Storage importer and audit CLI</a></h3>
<ul>
<li>Legacy manifests still exist under <code>storage_market/legacy/</code>. <code>storage_market/src/importer.rs</code> provides an ergonomic wrapper around that filesystem for audits and replay:
<ul>
<li><code>tb-cli storage importer audit --dir &lt;market&gt;</code> scans the pending/migrated manifest directories, samples up to 32 entries (<code>AUDIT_SAMPLE_LIMIT</code>), and reports duplicates/missing keys.</li>
<li><code>tb-cli storage importer rerun --overwrite</code> replays a manifest into the sled <code>market/contracts</code> tree. Modes: <code>InsertMissing</code> (default) or <code>OverwriteExisting</code>. Use <code>--dry-run</code> to see the counts first.</li>
<li><code>tb-cli storage importer verify --scope {contracts,all}</code> compares manifest checksums (deterministic hash over <code>(&lt;cf&gt;, key, value)</code> pairs) with the live database.</li>
</ul>
</li>
<li>All importer commands honor <code>--allow-absent</code> so CI can skip legacy steps when the manifests were already migrated. Reports can be written to disk (<code>--out report.json</code>) and are encoded with <code>foundation_serialization::json</code> for reproducibility.</li>
</ul>
<hr />
<h2 id="6-compute-marketplace"><a class="header" href="#6-compute-marketplace">6. Compute Marketplace</a></h2>
<h3 id="61-courier-retries-and-resume-semantics"><a class="header" href="#61-courier-retries-and-resume-semantics">6.1 Courier retries and resume semantics</a></h3>
<ul>
<li><code>CourierStore::flush</code> (sync) and <code>flush_async</code> attempt up to five deliveries per receipt. Backoff starts at 100 ms and doubles (100 → 200 → 400 → 800 → 1 600 ms). Failures increment <code>COURIER_FLUSH_FAILURE_TOTAL</code>.</li>
<li>Receipts persist in a sled tree (<code>courier</code> namespace). On restart, the flush loop replays unacknowledged receipts; <code>acknowledged=true</code> entries are left untouched for audit until the queue is compacted.</li>
<li>Capability gating: <code>send_for_capability</code> checks scheduler inventory before enqueuing work, preventing overload on providers that lack the requested hardware.</li>
</ul>
<h3 id="62-snark-receipts"><a class="header" href="#62-snark-receipts">6.2 SNARK receipts</a></h3>
<ul>
<li><code>node/src/compute_market/receipt.rs</code> fields: <code>(version, job_id, buyer, provider, quote_price, units, dry_run flag, issued_at, idempotency_key, FeeLane)</code>.</li>
<li><code>idempotency_key</code> is BLAKE3(<code>job_id | buyer | provider | price | units | version | lane</code>), guaranteeing deduplicated settlement entries even if retries occur.</li>
<li>SNARK receipts now embed a <code>ProofBundle</code> produced by <code>node/src/compute_market/snark.rs</code>. The helper wraps the Groth16 backend, hashes wasm bytes into circuit digests, caches compiled circuits per digest, selects CPU or GPU provers automatically, records telemetry (<code>snark_prover_latency_seconds{backend}</code>, <code>snark_prover_failure_total{backend}</code>), and emits bundles containing the circuit/output/witness commitments plus serialized proof bytes.</li>
<li>Each proof bundle includes a <code>CircuitArtifact { circuit_hash, wasm_hash, generated_at }</code>, allowing offline re-verification and matching against CLI/explorer exports without re-running compilation.</li>
<li>CLI support lives under <code>cli/src/snark.rs</code>. <code>snark compile</code> now writes attested circuit artifacts (digest + wasm hash + timestamp) so operators can cache proving parameters offline, and <code>tb-cli compute proofs --limit N</code> streams the latest <code>compute_market.sla_history</code> proofs (fingerprint, backend, commitments, artifact metadata) for audits.</li>
<li>Settlement persists every accepted proof via <code>Settlement::record_proof</code>, retaining the full vector of bundles per SLA and exposing them through <code>compute_market.sla_history</code>. <code>tb-cli explorer sync-proofs</code> writes the same <code>Vec&lt;ProofBundle&gt;</code> blobs into the explorer tables (<code>compute_sla_history</code>, <code>compute_sla_proofs</code>), and the explorer HTTP route <code>/compute/sla/history?limit=N</code> re-exports the decoded fingerprints/artifacts for dashboards and auditors.</li>
</ul>
<h3 id="63-sla-slashing-and-dashboards"><a class="header" href="#63-sla-slashing-and-dashboards">6.3 SLA slashing and dashboards</a></h3>
<ul>
<li>Settlement engine (<code>node/src/compute_market/settlement.rs</code>) maintains:
<ul>
<li><code>SettleMode</code> (<code>DryRun</code>, <code>Armed</code>, <code>Real</code>) toggled via governance or CLI.</li>
<li><code>SlaRecord { job_id, provider, buyer, provider_bond, consumer_bond, deadline, scheduled_at, proofs: Vec&lt;ProofBundle&gt; }</code>.</li>
<li><code>SlaResolution { outcome: Completed | Cancelled { reason } | Violated { reason }, burned_ct, refunded_ct, proofs }</code>.</li>
</ul>
</li>
<li>Automation:
<ul>
<li>Violations call <code>SlaOutcome::Violated { reason, automated }</code>, burn the lesser of the bond and configured slash amount (<code>COMPUTE_SLA_AUTOMATED_SLASH_TOTAL</code>, <code>SLASHING_BURN_CT_TOTAL</code>).</li>
<li>Dashboards plot <code>COMPUTE_SLA_PENDING_TOTAL</code>, <code>COMPUTE_SLA_VIOLATIONS_TOTAL</code>, and <code>COMPUTE_SLA_NEXT_DEADLINE_TS</code>.</li>
</ul>
</li>
<li>Operator workflow:
<ol>
<li>Arm settlement via <code>compute_arm_real</code> RPC, wait for the <code>activate_at</code> block height, then advance to <code>SettleMode::Real</code>.</li>
<li>Watch aggregator dashboards for SLA thresholds; use <code>tb-cli compute settlement</code> to inspect queued records.</li>
<li>Use the courier appendix (below) when diagnosing stuck carry-to-earn flows and pull <code>compute_market.sla_history</code> when auditing proof material alongside SLA outcomes.</li>
</ol>
</li>
</ul>
<h3 id="64-lane-aware-matcher-semantics"><a class="header" href="#64-lane-aware-matcher-semantics">6.4 Lane-aware matcher semantics</a></h3>
<ul>
<li><code>node/src/compute_market/matcher.rs</code> maintains per-lane order books. Each <code>LaneState</code> carries <code>LaneMetadata { fairness_window, max_queue_depth }</code>. Defaults come from env (<code>TB_COMPUTE_FAIRNESS_MS</code>, <code>TB_COMPUTE_LANE_CAP</code>); fairness defaults to 5 ms, capacity to 1 024 entries.</li>
<li><code>match_batch(batch)</code> snapshots the set of lanes, then loops:
<ol>
<li>Iterate lanes in order, skipping empty queues.</li>
<li>While <code>matched.len() &lt; batch</code>, peek <code>front()</code> bid/ask. If the bid price covers the ask price the pair is popped and recorded; otherwise break.</li>
<li>If the lane already matched once during this pass and <code>Instant::now() &gt; fairness_deadline</code>, we break early so other lanes get time.</li>
<li>Set <code>state.last_match_at</code> and clear starvation warnings whenever progress occurs.</li>
</ol>
</li>
<li>Batch size defaults to <code>TB_COMPUTE_MATCH_BATCH</code> (32). After every full pass we stop if no progress was made; otherwise we loop until <code>matched.len() == batch</code>.</li>
<li>Starvation guard: <code>collect_starvation(Duration::from_secs(TB_COMPUTE_STARVATION_SECS))</code> inspects queue heads and records <code>LaneWarning { lane, oldest_job, waited_for, updated_at }</code> whenever a job waits longer than the threshold (default 30 s). RPC <code>compute_market.stats</code> exposes these warnings so operators can alert on stuck jobs.</li>
</ul>
<h3 id="65-price-board-backlog-and-units"><a class="header" href="#65-price-board-backlog-and-units">6.5 Price board, backlog, and units</a></h3>
<ul>
<li><code>node/src/compute_market/price_board.rs</code> keeps a sliding window of <code>(price, weighted_price)</code> entries for each lane. <code>record_price</code> appends entries, <code>bands(lane)</code> returns <code>(min, weighted_median, max)</code>, and <code>spot_price_per_unit</code> falls back to raw medians when weighted samples are unavailable.</li>
<li><code>backlog_utilization()</code> summarises outstanding demand (<code>industrial_backlog</code>) and realised throughput (<code>industrial_utilization</code>). Both figures appear in <code>compute_market.stats</code> and the CLI.</li>
<li>Workload normalisation lives in <code>node/src/compute_market/workload.rs</code> (<code>ComputeUnits</code>, <code>compute_units(data)</code>, <code>calibrate_gpu</code>). Every workload is expressed in units per second so bids/asks remain comparable regardless of raw byte size.</li>
</ul>
<h3 id="66-scheduler-metrics-rpc"><a class="header" href="#66-scheduler-metrics-rpc">6.6 Scheduler metrics RPC</a></h3>
<ul>
<li><code>scheduler.metrics</code> (<code>node/src/rpc/compute_market.rs</code>) simply forwards the JSON from <code>scheduler::metrics()</code> which contains <code>reputation</code> (provider score map) and <code>utilization</code> (capability → units consumed). Use <code>tb-cli compute scheduler metrics --json</code> for scripting.</li>
<li><code>scheduler.stats</code> returns a richer struct (<code>SchedulerStats</code>): outcome counters (<code>success</code>, <code>capability_mismatch</code>, <code>reputation_failure</code>), queue depths per priority (<code>queued_high/normal/low</code>), pending jobs with effective priority, and preemption counters.</li>
<li>Together with <code>compute_market.stats</code> these RPCs explain why a job failed to match (e.g., reputation failure vs lack of capability) without scraping node logs.</li>
</ul>
<h3 id="67-energy-market-providers-credits-receipts"><a class="header" href="#67-energy-market-providers-credits-receipts">6.7 Energy market (providers, credits, receipts)</a></h3>
<ul>
<li><strong>Core structs</strong> (<code>crates/energy-market/src/lib.rs</code>):
<ul>
<li><code>EnergyProvider { provider_id, owner, location, capacity_kwh, available_kwh, price_per_kwh, reputation_score, meter_address, total_delivered_kwh, staked_balance, last_fulfillment_latency_ms, last_meter_value, last_meter_timestamp }</code>.</li>
<li><code>MeterReading { provider_id, meter_address, total_kwh, timestamp, signature }</code> with a <code>hash()</code> helper (BLAKE3 over provider/meter/kWh/timestamp/signature len + bytes) that becomes the credit key.</li>
<li><code>EnergyCredit { amount_kwh, provider, timestamp, meter_reading_hash }</code>, <code>EnergyReceipt { buyer, seller, kwh_delivered, price_paid, block_settled, treasury_fee, meter_reading_hash, slash_applied }</code>, and <code>EnergyMarketConfig { min_stake, treasury_fee_bps, ewma_alpha, jurisdiction_fee_bps, oracle_timeout_blocks, slashing_rate_bps }</code>.</li>
<li><code>EnergyMarket</code> maintains provider + meter maps, pending credits, settled receipts, EWMA totals, and <code>next_provider_id</code>. Errors bubble through <code>EnergyMarketError</code> (provider exists, meter claimed, insufficient stake/capacity/credit, stale reading, invalid meter value, expired credit) and map 1:1 to RPC error strings.</li>
</ul>
</li>
<li><strong>Persistence &amp; governance</strong> (<code>node/src/energy.rs</code>):
<ul>
<li>Wraps the market in <code>SimpleDb::open_named(names::ENERGY_MARKET, TB_ENERGY_MARKET_DIR.unwrap_or("energy_market"))</code>. Every mutation serializes the full market via <code>EnergyMarket::to_bytes()</code> and writes it using SimpleDb’s fsync+rename discipline.</li>
<li><code>GovernanceEnergyParams { min_stake, oracle_timeout_blocks, slashing_rate_bps }</code> lives in a <code>Lazy&lt;Mutex&lt;_&gt;&gt;</code>. <code>set_governance_params</code> updates the struct, applies it to the runtime config (<code>apply_params_to_market</code>), and persists the snapshot. Treasury fees/slashes flow into <code>NODE_GOV_STORE.record_treasury_accrual</code>.</li>
<li><code>check_energy_market_health()</code> warns when <code>pending_credit_count()</code> exceeds 25 and logs settlement heartbeats so dashboards can alert without scraping additional RPCs.</li>
</ul>
</li>
<li><strong>RPC + CLI</strong> (<code>node/src/rpc/energy.rs</code>, <code>cli/src/energy.rs</code>):
<ul>
<li>RPC methods: <code>energy.register_provider</code>, <code>energy.market_state</code>, <code>energy.submit_reading</code>, <code>energy.settle</code>. Helpers enforce payload shape (<code>require_string</code>, <code>require_u64</code>, <code>decode_hash</code>, <code>decode_signature</code>) and emit canonical JSON for providers/credits/receipts.</li>
<li>CLI mirrors the RPC schema: <code>tb-cli energy register &lt;capacity&gt; &lt;price&gt; --meter-address … --jurisdiction … --stake … --owner …</code>, <code>tb-cli energy market [--provider-id … --verbose]</code>, <code>tb-cli energy submit-reading --reading-json '…'</code>, <code>tb-cli energy settle &lt;provider&gt; &lt;kwh&gt; --meter-hash … --buyer …</code>.</li>
<li>All tooling (oracle adapters, explorer, dashboards) reuse the schema documented in <code>docs/apis_and_tooling.md#energy-rpc-payloads-auth-and-error-contracts</code> so meter hashes/receipts stay byte-identical everywhere.</li>
</ul>
</li>
<li><strong>Oracle adapter + mock service</strong> (<code>crates/oracle-adapter</code>, <code>services/mock-energy-oracle</code>):
<ul>
<li>Adapter defines <code>SignatureVerifier</code>; currently <code>NoopSignatureVerifier</code> (always true) stands in until Ed25519 verification + vectors land. <code>MeterReadingPayload</code> implements <code>MeterReading</code> and exposes <code>signing_bytes()</code> so verifiers can sign/verify consistently.</li>
<li>Mock service (in-house <code>httpd</code> router) listens on <code>MOCK_ENERGY_ORACLE_ADDR</code> (default <code>127.0.0.1:8080</code>), exposes <code>/meter/:id/reading</code> (increments totals by 250 kWh, updates timestamp, returns payload) and <code>/meter/:id/submit</code> (accepts posted readings). Used by <code>scripts/deploy-worldos-testnet.sh</code>.</li>
</ul>
</li>
<li><strong>Telemetry</strong>:
<ul>
<li>Gauges: <code>energy_providers_count</code>, <code>energy_avg_price</code>. Counters: <code>energy_kwh_traded_total</code>, <code>energy_settlements_total{provider}</code>. Histograms: <code>energy_provider_fulfillment_ms</code>, <code>oracle_reading_latency_seconds</code>.</li>
<li>Logs: <code>node::energy::check_energy_market_health</code> warns on backlog spikes and settlement stalls. Metrics aggregator wiring (<code>/wrappers</code>, <code>/telemetry/summary</code>) plus Grafana panels (provider counts, pending credits, slash totals, settlement rates) are tracked in <code>docs/architecture.md#energy-governance-and-rpc-next-tasks</code>.</li>
</ul>
</li>
<li><strong>Governance linkage</strong>:
<ul>
<li><code>ParamKey::{EnergyMinStake, EnergyOracleTimeoutBlocks, EnergySlashingRateBps}</code> live in <code>governance/src/{lib.rs,codec.rs,params.rs}</code>. Runtime hooks clamp values before writing to <code>Params</code> and call <code>node::energy::set_governance_params</code>.</li>
<li><code>node/tests/gov_param_wiring.rs</code> covers these round-trips; update it plus explorer/CLI timelines when adding new energy parameters (batch vs real-time settlement toggles, dependency graph validation, etc.).</li>
</ul>
</li>
<li><strong>Snapshot/restore</strong>:
<ul>
<li>Use <code>TB_ENERGY_MARKET_DIR</code> to relocate the sled DB, snapshot it alongside other SimpleDb stores (§5.5), and restore during drills. <code>docs/testnet/ENERGY_QUICKSTART.md</code> and <code>scripts/deploy-worldos-testnet.sh</code> document the canonical register ➜ submit ➜ settle ➜ dispute workflow until dedicated dispute RPCs land.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="7-dex-trust-lines-and-htlcs"><a class="header" href="#7-dex-trust-lines-and-htlcs">7. DEX, Trust Lines, and HTLCs</a></h2>
<h3 id="71-amm-pool-math"><a class="header" href="#71-amm-pool-math">7.1 AMM pool math</a></h3>
<ul>
<li>Constant-product pools live in <code>dex/src/amm.rs</code>.</li>
<li>Share minting:
<ul>
<li>First LP: <code>share = sqrt(ct * it)</code>.</li>
<li>Subsequent LPs: <code>share = min(total_shares * ct / reserve_ct, total_shares * it / reserve_it)</code>.</li>
</ul>
</li>
<li>Swaps preserve <code>k = reserve_ct * reserve_it</code>; slippage is purely the constant-product curve (no extra fee yet). Governance can wrap this module to add fees later without redesigning the math.</li>
</ul>
<h3 id="72-trust-line-routing"><a class="header" href="#72-trust-line-routing">7.2 Trust-line routing</a></h3>
<ul>
<li>Ledger (<code>node/src/dex/trust_lines.rs</code>) keeps a <code>HashMap&lt;(from,to), TrustLine { balance, limit, authorized }&gt;</code> pair.</li>
<li>Path finding:
<ul>
<li>BFS yields any viable path.</li>
<li><code>find_best_path</code> uses Dijkstra for the shortest hop path, then <code>max_slack_path</code> to maximise residual capacity (slack = <code>limit - |balance| - amount</code>).</li>
<li>Fallback route excludes edges chosen by the slack path to provide a disjoint backup.</li>
</ul>
</li>
<li><code>settle_path</code> applies debits/credits across hops atomically; failures roll back all adjustments.</li>
</ul>
<h3 id="73-htlc-proofs-and-replay-constraints"><a class="header" href="#73-htlc-proofs-and-replay-constraints">7.3 HTLC proofs and replay constraints</a></h3>
<ul>
<li><code>dex/src/htlc_router.rs</code> keeps pending intents (<code>VecDeque&lt;HtlcIntent { chain, amount, hash, timeout }</code>).</li>
<li>Submit flow:
<ol>
<li>Caller posts an intent; router matches against existing intents with identical hash+amount.</li>
<li>Upon match, the router returns both intents plus deterministic scripts <code>htlc:&lt;hash_hex&gt;:&lt;timeout&gt;</code> for each chain. Scripts embed the hashed secret to avoid mismatched preimages.</li>
</ol>
</li>
<li>Replay protection: matched pairs are removed from the queue; repeated submissions with the same hash+amount/time window are rejected. Timeouts must be ordered (chain A &lt; chain B) per best practices.</li>
</ul>
<h3 id="74-escrow-state-machine-and-explorer-payloads"><a class="header" href="#74-escrow-state-machine-and-explorer-payloads">7.4 Escrow state machine and explorer payloads</a></h3>
<ul>
<li><code>dex/src/escrow.rs</code> implements a Merkle-based escrow ledger:
<ul>
<li><code>EscrowEntry { from, to, total, released, payments, root, algo }</code> tracks aggregate state and supports both BLAKE3 and SHA3 hash commitments.</li>
<li>Every partial release appends to <code>payments</code>, recomputes the Merkle root, and returns <code>PaymentProof { leaf, path, algo }</code> so wallets can prove redemption without revealing unrelated payments.</li>
<li>When <code>released == total</code> the entry is deleted and <code>total_locked</code> drops accordingly (<code>dex_escrow_locked</code> gauge).</li>
</ul>
</li>
<li>Persistence and RPC:
<ul>
<li>Escrow tables live in sled (<code>dex::storage::DexStore</code>) and are exported via <code>node/src/dex/storage_binary.rs</code> (bincode helpers for order books, escrow snapshots, proofs).</li>
<li>RPC <code>dex_escrow_status</code>/<code>dex_escrow_release</code>/<code>dex_escrow_proof</code> expose the state machine; CLI mirrors them via <code>tb-cli dex escrow status|release|proof</code>.</li>
</ul>
</li>
<li>Explorer + metrics:
<ul>
<li><code>GET /dex/order_book</code> and <code>GET /dex/trust_lines</code> return the indexed order book and trust-line graph, respectively.</li>
<li>Metrics <code>dex_escrow_locked</code> and <code>dex_escrow_pending</code> (see <code>node/src/telemetry.rs</code>) feed the default dashboards so operators can spot runaway collateral.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="8-bridges-and-security"><a class="header" href="#8-bridges-and-security">8. Bridges and Security</a></h2>
<h3 id="81-header-layout-and-proof-encoding"><a class="header" href="#81-header-layout-and-proof-encoding">8.1 Header layout and proof encoding</a></h3>
<ul>
<li>External chains share a canonical header schema (<code>bridges/src/header.rs::PowHeader</code>):
<ul>
<li><code>chain_id</code> (string), <code>height</code> (u64), <code>merkle_root</code> (target tree), <code>signature</code> (BLAKE3 hash of prior fields), <code>nonce</code>, and difficulty <code>target</code>.</li>
<li><code>hash_header</code> concatenates those bytes and feeds BLAKE3; <code>verify_pow</code> compares the 64-bit prefix against <code>target</code>.</li>
</ul>
</li>
<li>Light-client payloads (<code>bridges/src/light_client.rs</code>):
<ul>
<li><code>Header { chain_id, height, merkle_root, signature }</code> plus <code>Proof { leaf, path[] }</code>.</li>
<li><code>header_hash</code> is <code>BLAKE3(chain_id || height || merkle_root)</code> and must match <code>signature</code>.</li>
<li>Merkle paths are stored as <code>Vec&lt;[u8;32]&gt;</code>; verification always hashes <code>(acc || sibling)</code> in insertion order, so callers must supply the path exactly as emitted by the origin chain.</li>
</ul>
</li>
<li>Within the node:
<ul>
<li>Headers are persisted via <code>simple_db::names::BRIDGE_HEADERS</code> and replayed during startup.</li>
<li>RPC <code>bridge.verify_deposit</code> decodes <code>ExternalSettlementProof</code>, validates the Merkle path, then records a <code>DepositReceipt</code>.</li>
<li>Proof encoding lives in <code>bridge_types::settlement_proof_digest</code>; receipts carry the digest so explorers can hyperlink deposits ↔ withdrawals.</li>
</ul>
</li>
</ul>
<h3 id="82-challenge-math-duty-windows-and-errors"><a class="header" href="#82-challenge-math-duty-windows-and-errors">8.2 Challenge math, duty windows, and errors</a></h3>
<ul>
<li><code>PendingWithdrawal</code> (<code>node/src/bridge/mod.rs:389</code>) tracks <code>commitment</code>, <code>asset</code>, <code>user</code>, <code>amount</code>, <code>initiated_at</code>, <code>deadline = initiated_at + channel.config.challenge_period_secs</code>, and <code>challenged</code> flags. Default challenge window is 30 s, overrideable per channel.</li>
<li>Duty timing:
<ol>
<li><code>bridge.request_withdrawal</code> locks the relayer bond (<code>bridge_min_bond</code>) and schedules a duty (<code>DutyRecord</code>) with deadline <code>initiated_at + bridge_duty_window_secs</code>.</li>
<li>Challengers call <code>bridge.challenge_withdrawal { commitment, challenger }</code>. If accepted, <code>BridgeError::AlreadyChallenged</code> prevents duplicates and <code>bridge_challenge_slash</code> refunds challengers after burning the bond.</li>
<li>If no challenge arrives before <code>deadline</code>, <code>bridge.finalize_withdrawal</code> releases funds and credits <code>bridge_duty_reward</code>.</li>
</ol>
</li>
<li>Error surface (<code>BridgeError</code>):
<ul>
<li><code>InvalidProof</code>, <code>SettlementProofHashMismatch</code>, <code>SettlementProofChainMismatch</code> return <code>-32071</code> style RPC codes.</li>
<li><code>ChallengeWindowOpen</code> / <code>AlreadyChallenged</code> bubble to clients so they know whether to retry later.</li>
<li>Missing rows (<code>UnknownChannel</code>, <code>WithdrawalMissing</code>) map to <code>-32072</code>.</li>
</ul>
</li>
<li>Diagnostics: <code>bridge.pending_withdrawals</code>, <code>bridge.active_challenges</code>, <code>bridge.duty_log</code>, and explorer dashboards show live commitments, deadlines, and slashing events. Telemetry families <code>BRIDGE_CHALLENGE_TOTAL</code>, <code>BRIDGE_SETTLEMENT_RESULTS_TOTAL{result,reason}</code>, and <code>BRIDGE_DISPUTE_OUTCOMES_TOTAL{kind,outcome}</code> back the Grafana bridge row.</li>
</ul>
<h3 id="83-risk-register-and-audit-playbooks"><a class="header" href="#83-risk-register-and-audit-playbooks">8.3 Risk register and audit playbooks</a></h3>
<ul>
<li><code>docs/security_and_privacy.md</code> consolidates the previous risk register:
<ul>
<li>Consensus: monitor QUIC stalls, DKG leaks.</li>
<li>Networking: peer DB corruption, overlay exhaustion.</li>
<li>Storage/Compute: erasure threshold violations, SLA slashing anomalies.</li>
<li>Governance: treasury drains, badge forgery attempts.</li>
</ul>
</li>
<li>Audit tooling:
<ul>
<li>Aggregator <code>/audit</code> endpoint stores incident logs.</li>
<li><code>scripts/settlement_audit.rs</code>, <code>tools/settlement_audit</code>, and <code>tb-cli gov treasury audit</code> compare ledger projections vs settlement state.</li>
<li>Risk reviews must cite relevant sections here before patching code.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="9-gateway-dns-and-read-receipts"><a class="header" href="#9-gateway-dns-and-read-receipts">9. Gateway, DNS, and Read Receipts</a></h2>
<h3 id="91-dns-auctions-and-cli-flows"><a class="header" href="#91-dns-auctions-and-cli-flows">9.1 DNS auctions and CLI flows</a></h3>
<ul>
<li>
<p><code>node/src/gateway/dns.rs</code> maintains auctions (<code>DomainAuctionRecord</code>) and stakes (<code>StakeEscrowRecord</code>) under <code>SimpleDb::open_named(names::GATEWAY_DNS, path)</code>.</p>
</li>
<li>
<p>Auction lifecycle:</p>
<ol>
<li>Seller lists via <code>tb-cli gateway domain list &lt;domain&gt; &lt;min_bid&gt; --seller &lt;acct&gt; ...</code>.</li>
<li>Bidders lock stake references (<code>register_stake</code>) and submit bids (<code>tb-cli gateway domain bid ... --stake-ref ref1</code>). Bids must exceed both <code>min_bid_ct</code> and previous <code>highest_bid</code>.</li>
<li>Seller (or governance) completes the sale (<code>tb-cli gateway domain complete</code>) once <code>end_ts</code> passes; ledger events (<code>LedgerEventRecord</code>) debit bidders, credit seller/royalty/treasury, and refund unused stake.</li>
<li>If no bids, <code>cancel</code> reopens the domain.</li>
</ol>
</li>
<li>
<p>Escrow bookkeeping handles stake reuse, lock/unlock flows, and ledger references for audit reports.</p>
</li>
<li>
<p>DNS TXT schema: publications must conform to <code>docs/spec/dns_record.schema.json</code> (<code>{domain, txt, pubkey, sig}</code>). Example TXT record for <code>club.block</code>:</p>
<pre><code>club.block TXT "tb-domain=club.block;pubkey=0c2e...;sig=5fb9..."
</code></pre>
<p><code>pubkey</code>/<code>sig</code> are lowercase hex (Ed25519). <code>dns_lookup</code> fetches TXT via the configured resolver (default <code>runtime::net::lookup_txt</code>), caches verdicts for <code>VERIFY_TTL = 3600s</code>, and persists them under <code>TB_DNS_DB_PATH</code> (<code>dns_db</code> by default).</p>
</li>
<li>
<p>Verification and auditing:</p>
<ul>
<li>RPC <code>gateway.policy</code> returns the active TXT policy parsed via <code>gateway::dns::gateway_policy</code>.</li>
<li><code>gateway.dns_lookup</code> exposes the cached verdict (<code>verified</code>, <code>pending</code>, <code>failed</code>) so explorers can show trust badges.</li>
<li>CLI <code>tb-cli net dns verify &lt;domain&gt;</code> exercises the same path and prints cache hits/misses.</li>
</ul>
</li>
</ul>
<h3 id="92-read-receipts-batching-and-audit-workflow"><a class="header" href="#92-read-receipts-batching-and-audit-workflow">9.2 Read receipts, batching, and audit workflow</a></h3>
<ul>
<li>Client acknowledgements (<code>node/src/read_receipt.rs::ReadAck</code>) contain manifest ID, path hash, bytes served, client hash, domain/provider metadata, optional ad readiness proofs, and Ed25519 signatures. <code>ReadAck::verify()</code> checks both signature and privacy proof.</li>
<li>Gateway receipts (<code>node/src/gateway/read_receipt.rs</code>):
<ul>
<li>Stored per hour (<code>current_epoch(ts) = ts / 3600</code>). Files are binary CBOR or legacy CBOR for compatibility.</li>
<li><code>batch(epoch)</code> loads all receipts, computes a BLAKE3 Merkle root, writes <code>&lt;epoch&gt;.root</code>, combines with execution receipts (<code>exec::batch</code>), and submits the final anchor to the settlement engine.</li>
<li><code>reads_since(epoch, domain)</code> returns <code>(count, last_ts)</code> for CLI/RPC reporting.</li>
</ul>
</li>
<li>CLI runbook:
<ol>
<li><code>tb-cli gateway reads-since --domain example.block --epoch $(date -u +%s)/3600</code> (custom script) polls the RPC <code>gateway.reads_since</code>.</li>
<li><code>tb-cli gateway mobile-cache flush</code> before maintenance to force anchors.</li>
<li>Inspect anchors via <code>compute_market.recent_roots</code> to ensure READ_SUB_CT credits landed.</li>
</ol>
</li>
</ul>
<h3 id="93-mobile-gateway-cache"><a class="header" href="#93-mobile-gateway-cache">9.3 Mobile gateway cache</a></h3>
<ul>
<li>Config (<code>MobileCacheConfig::from_env</code>):
<ul>
<li>TTL: <code>TB_MOBILE_CACHE_TTL_SECS</code> (default 300 s).</li>
<li>Sweep interval: <code>TB_MOBILE_CACHE_SWEEP_SECS</code> (default 30 s).</li>
<li>Entry cap: <code>TB_MOBILE_CACHE_MAX_ENTRIES</code> (default 512) and payload cap (<code>64 KiB</code> default).</li>
<li>Queue cap: <code>TB_MOBILE_CACHE_MAX_QUEUE</code> (default 256) for offline submissions.</li>
<li>Encryption key: <code>TB_MOBILE_CACHE_KEY_HEX</code> (falls back to <code>TB_NODE_KEY_HEX</code>).</li>
</ul>
</li>
<li>The cache uses ChaCha20‑Poly1305; entries persist <code>stored_at</code>, <code>expires_at</code>, and ciphertext bytes. TTL enforcement sweeps expired records, increments <code>MOBILE_CACHE_STALE_TOTAL</code>, and updates <code>MOBILE_CACHE_QUEUE_BYTES</code>.</li>
<li>CLI:
<ul>
<li><code>tb-cli gateway mobile-cache status --url http://node:26658 --pretty</code>.</li>
<li><code>tb-cli gateway mobile-cache flush</code> to drop encrypted responses and offline queue state.</li>
</ul>
</li>
</ul>
<h3 id="94-light-clients-state-streams-and-explorer-payloads"><a class="header" href="#94-light-clients-state-streams-and-explorer-payloads">9.4 Light clients, state streams, and explorer payloads</a></h3>
<ul>
<li>
<p>Streaming protocol (<code>node/src/rpc/state_stream.rs</code>):</p>
<ul>
<li>Upgrades the JSON-RPC connection to a WebSocket and repeatedly sends <code>StateChunk { seq, tip_height, accounts[], root, compressed }</code>.</li>
<li>Accounts are serialized as sorted sequences (<code>AccountChunk { address, balance, account_seq, proof }</code>); proofs are the Merkle siblings from <code>state::MerkleTrie::prove</code>.</li>
<li>Default cadence: 1 chunk per second; set by the loop delay inside <code>run_stream</code>. Errors close the socket; clients should reconnect and resume from the latest <code>seq</code>.</li>
</ul>
</li>
<li>
<p>Light-client crate (<code>crates/light-client</code>):</p>
<ul>
<li><code>SyncOptions</code> gate by Wi-Fi/charging/battery and compute <code>GatingReason</code>. Device probes feed <code>DeviceStatusSnapshot</code> which feeds CLI via <code>tb-cli light-client device status --json</code>.</li>
<li>Headers (<code>light_client::Header</code>) mirror the canonical layout (height, prev hash, Merkle roots, VDF data, optional validator key/sig). <code>verify_and_append</code> enforces PoW and checkpoint rules.</li>
<li><code>state_stream</code> consumers persist deltas under <code>~/.the_block/light_client.toml</code> and reuse <code>fetch_signed</code> to load jurisdiction-specific overrides.</li>
</ul>
</li>
<li>
<p>Rebate + explorer endpoints:</p>
<ul>
<li>
<p>RPC <code>light_client.rebate_status</code>/<code>light_client.rebate_history</code> query the on-disk proof tracker (<code>node/src/light_client/proof_tracker.rs</code>). Results power explorer endpoints:</p>
<pre><code class="language-json">GET /light_client/top_relayers -&gt; [
  {"id":"aa12…", "pending":3, "total_proofs":42, "total_claimed":100, "last_claim_height":18432}
]
GET /light_client/rebate_history?limit=2 -&gt; {
  "receipts":[
    {"height":17700,"amount":15,"relayers":[{"id":"aa12…","amount":10},{"id":"bb34…","amount":5}]}
  ],
  "next":null
}
</code></pre>
</li>
</ul>
</li>
<li>
<p>CLI quick start:</p>
<ol>
<li><code>tb-cli light-client rebate-status --url http://node:26658</code> prints local balances.</li>
<li><code>tb-cli light-client rebate-history --limit 5 --json</code> paginates receipts (set <code>--relayer &lt;hex&gt;</code> to filter).</li>
<li><code>tb-cli light-client device status --json</code> confirms gating vs overrides before enabling background sync.</li>
</ol>
</li>
<li>
<p>Failure modes:</p>
<ul>
<li>Missing proofs ➜ <code>state_stream</code> closes the socket and logs <code>StateStreamError::InvalidProof</code>.</li>
<li>Device gating ➜ <code>SyncOptions::gating_reason</code> returns <code>WifiUnavailable</code>, <code>RequiresCharging</code>, or <code>BatteryTooLow</code>, bubbled to CLI and RPC so users know why sync paused.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="10-governance-economics-and-treasury"><a class="header" href="#10-governance-economics-and-treasury">10. Governance, Economics, and Treasury</a></h2>
<h3 id="101-emission-curves-and-multipliers"><a class="header" href="#101-emission-curves-and-multipliers">10.1 Emission curves and multipliers</a></h3>
<ul>
<li>Logistic base reward <code>R₀(N) = R_max / (1 + e^{ξ (N - N*)})</code> (see <code>node/src/consensus/leader.rs</code>). Default <code>R_max</code> and slope parameters live in <code>governance::Params</code>.</li>
<li>Subsidy multipliers per lane follow the “one dial” formula described in the economic model; governance can clamp or trigger <code>kill_switch_subsidy_reduction</code>.</li>
<li>Explorer and CLI display live multipliers via shared serialization (<code>foundation_serialization</code>).</li>
</ul>
<h3 id="102-fee-rebates"><a class="header" href="#102-fee-rebates">10.2 Fee rebates</a></h3>
<ul>
<li>Rebates are ledger entries keyed to sender accounts. <code>node/src/fees/mod.rs</code> charges consumer traffic by applying rebates first; <code>FeeLane::Industrial</code> bypasses rebates and bills direct CT.</li>
<li>RPC: <code>peer.rebate_status</code>, <code>peer.rebate_claim</code>. CLI: <code>tb-cli fees status</code>, <code>tb-cli fees claim</code>.</li>
<li>Ordering: when submitting a transaction <code>submit_tx</code> calculates <code>total_consumer = amount_consumer + fee</code>, subtracts available rebate credits, and only then debits CT.</li>
</ul>
<h3 id="103-governance-parameters"><a class="header" href="#103-governance-parameters">10.3 Governance parameters</a></h3>
<ul>
<li><code>governance/src/params.rs</code> holds the canonical list and defaults. Appendix E summarises the most important knobs: snapshot cadence, fee floors, subsidy scalars, badge thresholds, scheduler weights, runtime/storage policy selectors, bridge incentive constants, etc.</li>
<li>Parameters can be toggled via governance proposals or RPC <code>gov_params</code>. Explorer <code>/governance/dependency_policy</code> exposes history.</li>
</ul>
<h3 id="104-release-rollback-and-ui-flows"><a class="header" href="#104-release-rollback-and-ui-flows">10.4 Release, rollback, and UI flows</a></h3>
<ul>
<li>CLI:
<ul>
<li><code>tb-cli gov release propose --hash &lt;build_hash&gt; --artifact &lt;url&gt;</code> registers new releases; provenance signatures must match <code>config/release_signers.txt</code> or env overrides.</li>
<li><code>tb-cli gov release approve --proposal &lt;id&gt;</code> collects signatures/quorum.</li>
<li><code>tb-cli gov release rollback --proposal &lt;id&gt;</code> triggers rollback windows; explorer <code>GET /releases</code> surfaces history.</li>
</ul>
</li>
<li>UI flows mirror CLI commands via explorer endpoints; log indexer ensures wallet + explorer display identical release states.</li>
<li>Provenance controls (<code>node/src/provenance.rs</code>):
<ul>
<li>
<p>Signer list: load from <code>TB_RELEASE_SIGNERS</code>, <code>TB_RELEASE_SIGNERS_FILE</code>, or <code>config/release_signers.txt</code>. <code>tb-cli gov release signers</code> prints the active keys.</p>
</li>
<li>
<p>Attestation payloads look like:</p>
<pre><code class="language-json">{
  "build_hash": "f8c3c5a…",
  "artifact": "https://artifacts.theblock.dev/the-block-f8c3.tar.zst",
  "signatures": [
    {"signer":"41b2…","signature":"0f9d…"},
    {"signer":"9ad0…","signature":"ab21…"}
  ]
}
</code></pre>
<p>Each signature is computed over <code>release:{build_hash}</code> using Ed25519 keys published in the signer list.</p>
</li>
<li>
<p><code>verify_release_signature</code> ensures at least one configured signer matches; <code>verify_artifact_signature</code> checks the downloaded artifact hash against the attestation. Failures increment <code>BUILD_PROVENANCE_INVALID_TOTAL</code> and the release proposal is marked quarantined until remediated.</p>
</li>
</ul>
</li>
</ul>
<h3 id="105-treasury-invariants"><a class="header" href="#105-treasury-invariants">10.5 Treasury invariants</a></h3>
<ul>
<li><code>governance/src/treasury.rs</code> enforces:
<ul>
<li>Non‑negative balances (<code>ct</code>, <code>industrial</code>) tracked in sled.</li>
<li>Streaming caps and kill switches (<code>kill_switch_subsidy_reduction</code>, <code>treasury_percent_ct</code>).</li>
<li>Audit log size ≤ 256 entries; older entries roll off but anchor hashes persist.</li>
</ul>
</li>
<li>Executor snapshots (CLI <code>tb-cli gov treasury executor</code>) must show monotonic <code>last_tick_at</code>. Settlement anchors are hashed and appended to the audit log for replayability.</li>
</ul>
<h3 id="106-service-badge-lifecycle"><a class="header" href="#106-service-badge-lifecycle">10.6 Service badge lifecycle</a></h3>
<ul>
<li>Tracker (<code>node/src/service_badge.rs</code>):
<ul>
<li>Maintains uptime counters, latency samples, and expiry metadata. Badges mint once <code>total_epochs &gt;= BADGE_MIN_EPOCHS</code> (default 90) and uptime ≥ <code>BADGE_ISSUE_UPTIME</code> (99 %). Revocation triggers when uptime falls below <code>BADGE_REVOKE_UPTIME</code> (95 %) or <code>expiry</code> passes (<code>BADGE_TTL_SECS</code>, default 30 days).</li>
<li>Minting calls <code>register_physical_presence(provider)</code>, stores a <code>token</code>, increments <code>BADGE_ISSUED_TOTAL</code>, and records timestamps (<code>BADGE_LAST_CHANGE_SECONDS</code>). Burning calls <code>revoke_provider_badge</code> and updates <code>BADGE_ACTIVE</code>.</li>
</ul>
</li>
<li>CLI (<code>cli/src/service_badge.rs</code>):
<ul>
<li><code>tb-cli service-badge issue|revoke</code> wrap the RPC methods for automation.</li>
<li><code>tb-cli service-badge verify &lt;token&gt;</code> confirms authenticity (good for explorer tooltips).</li>
<li><code>tb-cli service-badge venue register|rotate|status &lt;name&gt;</code> manages venue presence tokens tracked in <code>VENUE_TOKENS</code>.</li>
</ul>
</li>
<li>Explorer + telemetry:
<ul>
<li>Explorer surfaces badges under <code>/auxiliary/services</code> alongside venue crowd snapshots (<code>VENUE_COUNTS</code>).</li>
<li>Metrics <code>BADGE_ACTIVE</code>, <code>BADGE_LAST_CHANGE_SECONDS</code>, and <code>service_badge_token_age_seconds</code> back the default dashboards so operators can verify SLA compliance before badges expire.</li>
</ul>
</li>
</ul>
<hr />
<h2 id="11-security-privacy-and-provenance"><a class="header" href="#11-security-privacy-and-provenance">11. Security, Privacy, and Provenance</a></h2>
<h3 id="111-crypto-migration-steps"><a class="header" href="#111-crypto-migration-steps">11.1 Crypto migration steps</a></h3>
<ul>
<li>PQ upgrades compile by enabling the <code>pq-crypto</code> feature; <code>node/src/commit_reveal.rs</code> switches commits to Dilithium, while <code>dkg/</code> and <code>wallet/remote_signer</code> can operate in both classic and PQ modes.</li>
<li>Migration playbook:
<ol>
<li>Build both PQ and classic binaries; verify <code>crypto_suite</code> self‑tests.</li>
<li>Rotate commit‑reveal keys using CLI (<code>tb-cli gov commit-reveal rotate</code> when available) while telemetry <code>CRYPTO_MIGRATION_*</code> metrics stay zero.</li>
<li>Roll out PQ wallets by updating <code>TB_WALLET_SCHEME=dalek|dilithium</code>; the wallet crate auto‑detects remote signer capability.</li>
</ol>
</li>
</ul>
<h3 id="112-remote-signer-security"><a class="header" href="#112-remote-signer-security">11.2 Remote signer security</a></h3>
<ul>
<li><code>crates/wallet/src/remote_signer.rs</code> enforces:
<ul>
<li>Mutual TLS with rotating certificates (<code>RemoteSigner::connect_multi</code>).</li>
<li>Threshold enforcement for multisig (errors: <code>insufficient_signers</code>, <code>invalid_signature</code>, <code>timeout</code>).</li>
<li>Telemetry: <code>remote_signer_request_total</code>, <code>remote_signer_success_total</code>, <code>remote_signer_latency_seconds</code>, <code>remote_signer_key_rotation_total</code>, <code>remote_signer_error_total</code>.</li>
</ul>
</li>
<li>Threat model mitigations: CLI refuses mixing seeds and remote signers, remote signer addresses must be provided explicitly, and governance audits endpoints via aggregator dashboards.</li>
</ul>
<h3 id="113-kyc-and-jurisdiction-authoring"><a class="header" href="#113-kyc-and-jurisdiction-authoring">11.3 KYC and jurisdiction authoring</a></h3>
<ul>
<li><code>crates/jurisdiction</code> parses signed policy packs: <code>PolicyPack { region, consent_required, features, parent }</code>, <code>SignedPack { pack, signature }</code>.</li>
<li>Authoring flow:
<ol>
<li>Draft JSON (<code>examples/jurisdiction/*.json</code>), embed inheritance via <code>parent</code>.</li>
<li>Sign with Ed25519; store signature as byte array or base64.</li>
<li>Publish via RPC/CLI (<code>jurisdiction.set</code>), referencing governance proposals for region changes.</li>
</ol>
</li>
<li>Verification: nodes fetch packs via HTTP (<code>TB_JURISDICTION_FEED</code>), verify signatures against configured keys, and expose diffs through <code>jurisdiction.policy_diff</code>.</li>
</ul>
<h3 id="114-privacy-compliance-mappings"><a class="header" href="#114-privacy-compliance-mappings">11.4 Privacy compliance mappings</a></h3>
<ul>
<li><code>docs/security_and_privacy.md</code> now owns retention guidance:
<ul>
<li>Read receipts store signatures only, hashed client IDs, and optional ad proofs. TTL per region is defined in jurisdiction packs.</li>
<li>Law-enforcement portal logs metadata (request, action, result) and can redact evidence while keeping hash commitments.</li>
<li>Mobile cache encrypts everything at rest; eviction policy ensures no stale responses leak when TTL expires.</li>
</ul>
</li>
</ul>
<h3 id="115-release-provenance-and-supply-chain"><a class="header" href="#115-release-provenance-and-supply-chain">11.5 Release provenance and supply chain</a></h3>
<ul>
<li>Provenance enforcement (<code>node/src/provenance.rs</code>) loads signer lists from <code>TB_RELEASE_SIGNERS</code> or <code>config/release_signers.txt</code>, verifies <code>release:{hash}</code> signatures, and checks <code>env!("BUILD_BIN_HASH")</code> against the current executable.</li>
<li>Artifacts: <code>provenance.json</code> &amp; <code>checksums.txt</code> (CI generated) plus <code>cargo vendor</code> snapshots. Tagging is blocked unless the dependency registry audit passes.</li>
<li>CLI <code>contract-cli system dependencies</code> + aggregator <code>/wrappers</code> expose runtime/transport/storage/coding backend info for compliance review.</li>
</ul>
<hr />
<h2 id="12-apis-explorer-and-tooling"><a class="header" href="#12-apis-explorer-and-tooling">12. APIs, Explorer, and Tooling</a></h2>
<h3 id="121-rpc-method-index"><a class="header" href="#121-rpc-method-index">12.1 RPC method index</a></h3>
<p>See Appendix A for the full method → namespace → params → errors table. Highlights:</p>
<ul>
<li>Governance: <code>gov_list</code>, <code>gov_params</code>, <code>gov_propose</code>, <code>gov_vote</code>, <code>gov.release_signers</code>, <code>gov.treasury.*</code>.</li>
<li>Bridge: <code>bridge.relayer_status</code>, <code>bridge.request_withdrawal</code>, <code>bridge.challenge_withdrawal</code>, etc., all returning structured JSON with error codes mapped from <code>BridgeError</code>.</li>
<li>Compute: <code>compute_market.stats</code>, <code>compute.job_status</code>, <code>compute.job_cancel</code>.</li>
<li>Gateway: <code>dns.*</code>, <code>gateway.mobile_cache_*</code>, <code>gateway.reads_since</code>.</li>
<li>Diagnostics: <code>net.peer_stats*</code>, <code>anomaly.label</code>, <code>metrics</code> (OTLP snapshots), <code>mempool.stats</code>.</li>
</ul>
<h3 id="122-explorer-endpoints-and-schemas"><a class="header" href="#122-explorer-endpoints-and-schemas">12.2 Explorer endpoints and schemas</a></h3>
<p><code>explorer/src/lib.rs::router</code> exposes REST endpoints:</p>
<div class="table-wrapper"><table><thead><tr><th>Endpoint</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>GET /blocks/:hash</code>, <code>/blocks/:hash/summary</code>, <code>/blocks/:hash/payouts</code>, <code>/blocks/:hash/proof</code></td><td>Block inspection + proof generation.</td></tr>
<tr><td><code>GET /txs/:hash</code></td><td>Transaction details.</td></tr>
<tr><td><code>GET /gov/proposals/:id</code>, <code>/governance/treasury/*</code>, <code>/governance/dependency_policy</code></td><td>Governance history, treasury status, dependency overrides.</td></tr>
<tr><td><code>GET /releases</code></td><td>Release history with attestation metadata.</td></tr>
<tr><td><code>GET /storage/providers</code>, <code>/storage/manifests</code></td><td>Storage pipeline views.</td></tr>
<tr><td><code>GET /dex/order_book</code>, <code>/dex/trust_lines</code></td><td>DEX/per‑path data.</td></tr>
<tr><td><code>GET /compute/jobs</code></td><td>Active compute orders and receipts.</td></tr>
<tr><td><code>GET /light_client/top_relayers</code>, <code>/light_client/rebate_history</code></td><td>Light-client stream telemetry.</td></tr>
<tr><td><code>GET /receipts/provider/:id</code>, <code>/receipts/domain/:id</code></td><td>Read-receipt history for audit.</td></tr>
<tr><td><code>GET /ad/policy/snapshots*</code>, <code>/ad/readiness/status</code></td><td>Ad policy and readiness snapshots.</td></tr>
</tbody></table>
</div>
<p>Explorer uses <code>foundation_sqlite</code> and shares codec crates, so HTTP responses line up with RPC schemas.</p>
<h3 id="123-first-party-rpc-blockers-edge-cases--migration-tips"><a class="header" href="#123-first-party-rpc-blockers-edge-cases--migration-tips">12.3 First-party RPC blockers (edge cases &amp; migration tips)</a></h3>
<ul>
<li>Long‑running methods (<code>storage_upload</code>, <code>compute.job_cancel</code>) stream progress; legacy clients may block. Use the HTTP client in <code>crates/httpd</code> or <code>cli</code> wrappers to inherit timeouts/backoffs.</li>
<li>Governance RPC errors map to <code>-320xx</code> codes (see <code>AuctionError::code</code>, bridge errors, treasury errors). Legacy JSON-RPC 1.0 clients that assume <code>code=-32603</code> must be updated.</li>
<li><code>metrics</code> endpoint emits Prometheus text; when migrating from external telemetry stacks ensure scrapers handle embedded wrapper metrics and TLS (mutual TLS by default).</li>
<li>Large responses (e.g., <code>storage.manifests</code>, <code>dex.order_book</code>) require pagination; explorer endpoints and CLI support filtering to reduce payloads.</li>
</ul>
<h3 id="124-schema-inventory"><a class="header" href="#124-schema-inventory">12.4 Schema inventory</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Schema</th><th>Location</th><th>When to update</th></tr></thead><tbody>
<tr><td>Fee submission schema</td><td><code>docs/spec/fee_v2.schema.json</code></td><td>Any time RPC fee payloads gain/rename fields.</td></tr>
<tr><td>DNS record schema</td><td><code>docs/spec/dns_record.schema.json</code></td><td>When adding DNS TXT attributes or <code>.block</code> metadata.</td></tr>
</tbody></table>
</div>
<p>Keep the schemas in sync with RPC structs and rerun <code>mdbook build docs</code> before merging.</p>
<hr />
<h2 id="13-telemetry-monitoring-and-probe-clis"><a class="header" href="#13-telemetry-monitoring-and-probe-clis">13. Telemetry, Monitoring, and Probe CLIs</a></h2>
<h3 id="131-metric-inventory-and-alerting"><a class="header" href="#131-metric-inventory-and-alerting">13.1 Metric inventory and alerting</a></h3>
<p>See Appendix B for detailed metric descriptions. Key highlights:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Meaning</th><th>Labels</th><th>Units</th><th>Suggested alert</th></tr></thead><tbody>
<tr><td><code>base_fee</code></td><td>Current per-byte base fee</td><td>n/a</td><td>micro‑CT</td><td>Alert if <code>&gt;10x</code> baseline for &gt;5 min (congestion).</td></tr>
<tr><td><code>mempool_fee_floor_{consumer,industrial}</code></td><td>Rolling fee floors</td><td>lane</td><td>micro‑CT</td><td>Alert if consumer floor moves &gt;50 % in &lt;60 s (possible spam).</td></tr>
<tr><td><code>MEMPOOL_EVICTIONS_TOTAL</code></td><td>Evicted tx count</td><td>lane</td><td>count</td><td>Alert if derivative spikes unexpectedly.</td></tr>
<tr><td><code>PARTITION_EVENTS_TOTAL</code></td><td>Partition incidents</td><td>n/a</td><td>count</td><td>Alert on increments; cross‑link to runbook.</td></tr>
<tr><td><code>QUIC_HANDSHAKE_FAIL_TOTAL</code></td><td>Failed QUIC handshakes</td><td>peer,reason</td><td>count</td><td>Alert on surge for specific provider.</td></tr>
<tr><td><code>RANGE_BOOST_QUEUE_DEPTH</code></td><td>Pending mesh bundles</td><td>n/a</td><td>count</td><td>Alert if &gt; queue cap (default 256).</td></tr>
<tr><td><code>MOBILE_CACHE_*</code></td><td>Cache hits/misses/sweeps</td><td>action</td><td>count</td><td>Alert on sustained misses and queue overflows.</td></tr>
<tr><td><code>COMPUTE_SLA_VIOLATIONS_TOTAL</code></td><td>SLA breaches</td><td>reason</td><td>count</td><td>Alert &gt;0 while in <code>SettleMode::Real</code>.</td></tr>
<tr><td><code>BRIDGE_CHALLENGE_TOTAL</code></td><td>Bridge challenges</td><td>asset</td><td>count</td><td>Alert if &gt;0 without operator action.</td></tr>
<tr><td><code>TREASURY_BALANCE_CT</code></td><td>Treasury CT balance</td><td>n/a</td><td>CT</td><td>Alert if drops &gt;X% per day outside scheduled disbursements.</td></tr>
<tr><td><code>WRAPPER_DEPENDENCY_POLICY_VIOLATION</code></td><td>Dependency drift</td><td>wrapper</td><td>count</td><td>Alert immediately; governance override required.</td></tr>
</tbody></table>
</div>
<h3 id="132-probe-cli-cookbook"><a class="header" href="#132-probe-cli-cookbook">13.2 Probe CLI cookbook</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Command</th><th>Purpose</th><th>Expected output</th><th>Typical use</th></tr></thead><tbody>
<tr><td><code>probe ping-rpc --url http://node:3050 --timeout 5 --prom</code></td><td>Measures RPC latency and emits Prometheus sample <code>probe_rpc_ping_seconds</code>.</td><td>JSON summary + <code>probe_rpc_ping_seconds{}</code> sample.</td><td>Blackbox RPC monitoring.</td></tr>
<tr><td><code>probe gossip-check --addr 1.2.3.4:4040 --expect-peers 16</code></td><td>Ensures gossip port reachable and peer count ≥ threshold.</td><td>Exit 0 when <code>peer_count &gt;= expect</code>.</td><td>Overlay health checks.</td></tr>
<tr><td><code>probe mine-one --miner my-node</code></td><td>Mines a single block via RPC template submit.</td><td>Block hash or error.</td><td>Regression testing for PoW/PoS pipeline.</td></tr>
<tr><td><code>probe tip --rpc http://node:3050</code></td><td>Prints current height/hash; optional Prom output.</td><td><code>{"height":123,...}</code>.</td><td>Alert when local tip stalls vs peers.</td></tr>
</tbody></table>
</div>
<p>Pair probes with dashboards (see <code>monitoring/README.md</code>) and aggregator targets for unified observability.</p>
<hr />
<h2 id="14-developer-handbook-highlights"><a class="header" href="#14-developer-handbook-highlights">14. Developer Handbook Highlights</a></h2>
<ul>
<li><strong>Concurrency &amp; Logging</strong>: Use <code>concurrency::{MutexExt, DashMap}</code> to avoid poisoned locks; log via <code>diagnostics::tracing</code> with structured fields (<code>component</code>, <code>peer</code>, <code>lane</code>, etc.). Never log PII; privacy filters exist for the LE portal only.</li>
<li><strong>Debugging</strong>: <code>tb-cli diagnostics {mempool,gossip,mesh,tls}</code> surfaces cached stats. Enable <code>RUST_LOG=trace</code> plus diagnostics subscriber when reproducing issues. Probe CLI (above) complements runtime diagnostics.</li>
<li><strong>Performance &amp; Benchmarks</strong>: Bench harnesses under <code>benches/</code> and Grafana dashboards compare results against thresholds in <code>config/benchmarks/*.thresholds</code>. Export histograms via <code>TB_BENCH_PROM_PATH</code>.</li>
<li><strong>Formal Methods &amp; Simulation</strong>: <code>formal/</code> crate plus <code>docs/formal.md</code> house model checks. <code>sim/</code> contains scenario harnesses (dependency faults, DKG latency, bridge threats); run them before altering consensus or governance logic.</li>
<li><strong>WASM/VM Debug</strong>: <code>cli/src/wasm.rs</code>, <code>node/src/vm</code>, <code>node/src/vm/debugger.rs</code> provide contract deployment, tracing, and debugging. Workflow: <code>tb-cli wasm build</code> → <code>tb-cli contract deploy</code> → <code>tb-cli contract call</code> → <code>tb-cli vm trace --tx &lt;hash&gt;</code>.</li>
</ul>
<hr />
<h2 id="appendices"><a class="header" href="#appendices">Appendices</a></h2>
<h3 id="appendix-a--rpc-method-index"><a class="header" href="#appendix-a--rpc-method-index">Appendix A · RPC Method Index</a></h3>
<p><em>All methods speak JSON-RPC 2.0 over HTTP(S) via the in-house <code>httpd</code> router. Every request must carry <code>{"jsonrpc":"2.0","id":&lt;id&gt;,"method":...,"params":...}</code>; responses mirror the standard result/error envelope. Objects below list field names, types, and whether they are optional (<code>?</code>). Unless noted otherwise, numeric fee values are expressed in micro-CT and heights/epochs use unsigned 64-bit integers.</em></p>
<h4 id="ad_market-nodesrcrpcad_marketrs"><a class="header" href="#ad_market-nodesrcrpcad_marketrs">ad_market.* (<code>node/src/rpc/ad_market.rs</code>)</a></h4>
<p><strong>Objects</strong></p>
<ul>
<li><code>CampaignSummary</code>: <code>{id, advertiser_account, remaining_budget_usd_micros, reserved_budget_usd_micros, creatives[]}</code>.</li>
<li><code>DistributionPolicy</code>: <code>{viewer_percent, host_percent, hardware_percent, verifier_percent}</code>.</li>
<li><code>CohortPriceSnapshot</code>: <code>{domain, provider?, badges[], price_per_mib_usd_micros, target_utilization_ppm, observed_utilization_ppm}</code>.</li>
<li><code>BudgetSnapshot</code>: top-level fields <code>generated_at_micros</code>, <code>config</code> (see <code>BudgetBrokerConfig</code>), <code>campaigns[]</code> (each with <code>id</code>, <code>kappa</code>, <code>epoch_spend_total_usd_micros</code>, <code>remaining_usd_micros</code>, <code>cohorts[]</code>).</li>
<li><code>ReadinessSnapshot</code>: <code>{ready, window_secs, unique_viewers, host_count, provider_count, thresholds:{min_unique_viewers,min_host_count,min_provider_count}, utilization{cohort-&gt;{domain,viewer_ppm,host_ppm}}, distribution?}</code>.</li>
<li><code>PolicySnapshot</code>: stored in <code>node/ad_policy_snapshot.rs</code>; serialized as <code>{epoch, weights[], governance_hash}</code>.</li>
<li><code>CampaignRegistration</code>: canonical ad campaign object (ID, advertiser account, creatives, metadata map, budgets) as accepted by <code>ad_market::campaign_from_value</code>.</li>
<li><code>ConversionRecord</code>: <code>{campaign_id, creative_id, advertiser_account, assignment:{fold,u8,in_holdout,bool,propensity,f64}, value_usd_micros?, occurred_at_micros?}</code>. Requires <code>Authorization: Advertiser &lt;account&gt;:&lt;token&gt;</code> header; token hash stored under <code>metadata["conversion_token_hash"]</code>.</li>
</ul>
<p><strong>Common errors</strong>: <code>-32602</code> invalid params, <code>-32603</code> internal/disabled, <code>-32000</code> duplicate campaign, <code>-32001</code> unknown campaign, <code>-32002</code> unknown creative, <code>-32030..-32033</code> advertiser auth failures.</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>ad_market.inventory</code></td><td>none</td><td><code>{status:"ok",distribution:DistributionPolicy,oracle:{ct_price_usd_micros,it_price_usd_micros},campaigns:[CampaignSummary],cohort_prices:[CohortPriceSnapshot]}</code></td><td><code>-32603</code> when the market handle is not configured</td></tr>
<tr><td><code>ad_market.list_campaigns</code></td><td>none</td><td><code>{status:"ok",campaigns:[CampaignSummary]}</code></td><td><code>-32603</code> when disabled</td></tr>
<tr><td><code>ad_market.distribution</code></td><td>none</td><td><code>{status:"ok",distribution:DistributionPolicy}</code></td><td><code>-32603</code></td></tr>
<tr><td><code>ad_market.budget</code></td><td>none</td><td><code>{status:"ok",config:BudgetBrokerConfig,campaigns:[...],delta:{...}}</code> mirroring <code>budget_snapshot_to_value</code></td><td><code>-32603</code></td></tr>
<tr><td><code>ad_market.broker_state</code></td><td>none</td><td>Same payload as <code>ad_market.budget</code> (includes pacing deltas and analytics)</td><td><code>-32603</code></td></tr>
<tr><td><code>ad_market.readiness</code></td><td>none</td><td><code>ReadinessSnapshot</code> plus optional <code>distribution</code> (only when coupled with live market data)</td><td><code>-32603</code> when readiness handle unset</td></tr>
<tr><td><code>ad_market.policy_snapshot</code></td><td><code>{epoch}</code></td><td>Snapshot JSON from <code>ad_policy_snapshot::load_snapshot</code> or <code>{status:"not_found"}</code></td><td><code>-32602</code> missing epoch</td></tr>
<tr><td><code>ad_market.policy_snapshots</code></td><td><code>{start_epoch?,end_epoch?}</code> (<code>end_epoch</code> defaults to <code>chain_height/120</code>)</td><td><code>{snapshots:[PolicySnapshot]}</code></td><td>none (empty list when no snapshots)</td></tr>
<tr><td><code>ad_market.register_campaign</code></td><td><code>CampaignRegistration</code></td><td><code>{status:"ok"}</code> on success</td><td><code>-32603</code> when market disabled/persistence error, <code>-32602</code> invalid payload, <code>-32000</code> duplicate campaign</td></tr>
<tr><td><code>ad_market.record_conversion</code></td><td><code>ConversionRecord</code> body + advertiser auth header</td><td><code>{status:"ok",conversion_summary:{authenticated_total,rejected_total,error_counts,last_error?,last_authenticated_at?}}</code></td><td><code>-32603</code> disabled/internal, <code>-32602</code> invalid payload, <code>-32001</code> unknown campaign, <code>-32002</code> unknown creative, <code>-32030</code> auth missing, <code>-32031</code> advertiser mismatch, <code>-32032</code> token missing, <code>-32033</code> invalid token</td></tr>
</tbody></table>
</div>
<h4 id="analytics-anomaly-labeling-and-settlement-probes"><a class="header" href="#analytics-anomaly-labeling-and-settlement-probes">Analytics, anomaly labeling, and settlement probes</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>analytics</code> (<code>node/src/rpc/analytics.rs</code>)</td><td><code>{domain?}</code> filters aggregator buckets</td><td><code>AnalyticsStats { domain, bytes, requests, distribution }</code> as defined in <code>crate::rpc::analytics</code></td><td><code>-32603</code> when telemetry feature disabled</td></tr>
<tr><td><code>anomaly.label</code></td><td><code>{label}</code> string used for ML feedback</td><td><code>{status:"ok"}</code></td><td>none</td></tr>
<tr><td><code>settlement_status</code></td><td><code>{provider?}</code></td><td>`{mode:"dryrun"</td><td>"real"</td></tr>
<tr><td><code>settlement.audit</code></td><td>none</td><td>Full settlement audit JSON (<code>compute_market::settlement_audit</code>) with balances, SLA queue, outstanding receipts</td><td>propagates <code>MarketError</code> codes (<code>-32040</code> range)</td></tr>
</tbody></table>
</div>
<h4 id="account-queries-and-ledger-helpers"><a class="header" href="#account-queries-and-ledger-helpers">Account queries and ledger helpers</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>balance</code></td><td><code>{address}</code></td><td><code>{consumer,industrial}</code> balances pulled from <code>accounts</code> map (0 when missing)</td><td>none</td></tr>
<tr><td><code>ledger.shard_of</code></td><td><code>{address}</code></td><td><code>{shard}</code> numeric shard ID as computed by <code>ledger::shard_of</code></td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="bridge-nodesrcrpcbridgers"><a class="header" href="#bridge-nodesrcrpcbridgers">Bridge (<code>node/src/rpc/bridge.rs</code>)</a></h4>
<p><strong>Objects</strong></p>
<ul>
<li><code>RelayerStatusRequest</code>: <code>{relayer,String, asset?}</code>.</li>
<li><code>BondRelayerRequest</code>: <code>{relayer,String, amount,u64}</code>.</li>
<li><code>ClaimRewardsRequest</code>: <code>{relayer,String, amount,u64, approval_key}</code>.</li>
<li><code>VerifyDepositRequest</code>: <code>{asset?, relayer?, user?, amount?, header:PowHeader, proof:Proof, relayer_proofs:RelayerProof[]}</code>.</li>
<li><code>RequestWithdrawalRequest</code>: <code>{asset?, relayer?, user?, amount?, relayer_proofs[]}</code>.</li>
<li><code>ChallengeWithdrawalRequest</code>: <code>{asset?, commitment, challenger}</code>.</li>
<li><code>FinalizeWithdrawalRequest</code>: <code>{asset?, commitment}</code>.</li>
<li><code>SubmitSettlementRequest</code>: <code>{asset?, settlement:ExternalSettlementProof}</code>.</li>
<li><code>PendingWithdrawalsRequest</code>: <code>{asset?, relayer?, limit?, cursor?}</code> → paginated list of <code>PendingWithdrawalInfo</code>.</li>
<li><code>ActiveChallengesRequest</code>: <code>{asset?, limit?, cursor?}</code>.</li>
<li><code>RelayerQuorumRequest</code>: <code>{asset?, epoch?}</code> returns <code>RelayerQuorumInfo</code>.</li>
<li><code>RewardClaimsRequest</code> / <code>RewardAccrualsRequest</code>: <code>{asset?, relayer?, limit?, cursor?}</code>.</li>
<li><code>SettlementLogRequest</code>: <code>{asset?, limit?, cursor?}</code> returning historical <code>SettlementRecord</code> entries.</li>
<li><code>RelayerAccountingRequest</code>: <code>{asset?, relayer}</code> summarising stakes, duties, and payouts.</li>
<li><code>DutyLogRequest</code>: <code>{asset?, relayer?, duty_kind?, cursor?, limit?}</code>.</li>
<li><code>DepositHistoryRequest</code>: <code>{asset?, relayer?, user?, cursor?, limit?}</code>.</li>
<li><code>SlashLogRequest</code>: <code>{asset?, relayer?, cursor?, limit?}</code>.</li>
<li><code>AssetsRequest</code>: <code>{}</code> → returns <code>BridgeAssetSnapshot</code> entries.</li>
<li><code>ConfigureAssetRequest</code>: <code>{asset, channel_config:ChannelConfig}</code> for governance.</li>
</ul>
<p><strong>Common errors</strong> map bridge errors to RPC codes (<code>-32002</code> invalid proof, <code>-32007</code> duplicate withdrawal, <code>-32010</code> challenge window open, etc.) plus <code>-32602</code> invalid params and <code>-32000</code> busy DB lock.</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>bridge.relayer_status</code></td><td><code>RelayerStatusRequest</code></td><td><code>RelayerInfo {relayer,bond_ct,pending_rewards_ct,duty_state}</code></td><td>bridge error codes</td></tr>
<tr><td><code>bridge.bond_relayer</code></td><td><code>BondRelayerRequest</code></td><td><code>{status:"ok"}</code></td><td><code>-32014</code> insufficient bond, storage errors</td></tr>
<tr><td><code>bridge.claim_rewards</code></td><td><code>ClaimRewardsRequest</code></td><td><code>{claimed_ct, pending_ct}</code></td><td><code>-32015/-32017</code> reward validation errors</td></tr>
<tr><td><code>bridge.verify_deposit</code></td><td><code>VerifyDepositRequest</code></td><td><code>DepositReceipt</code> (+ emission details) when proof checks succeed</td><td>proof errors <code>-32002</code>, replay <code>-32006</code>, storage <code>-32013</code></td></tr>
<tr><td><code>bridge.request_withdrawal</code></td><td><code>RequestWithdrawalRequest</code></td><td><code>PendingWithdrawalInfo</code> plus commitment hash</td><td>duplicates <code>-32007</code>, insufficient bond <code>-32014</code></td></tr>
<tr><td><code>bridge.challenge_withdrawal</code></td><td><code>ChallengeWithdrawalRequest</code></td><td><code>{status:"ok", challenge:ChallengeRecord}</code></td><td><code>-32008</code> missing withdrawal, <code>-32009</code> already challenged</td></tr>
<tr><td><code>bridge.finalize_withdrawal</code></td><td><code>FinalizeWithdrawalRequest</code></td><td><code>{status:"ok"}</code> upon releasing funds</td><td><code>-32010</code> challenge window open</td></tr>
<tr><td><code>bridge.submit_settlement</code></td><td><code>SubmitSettlementRequest</code></td><td><code>{status:"ok"}</code> after settlement proof accepted</td><td>settlement proof errors <code>-32018..-32023</code></td></tr>
<tr><td><code>bridge.pending_withdrawals</code></td><td><code>PendingWithdrawalsRequest</code></td><td><code>{items:[PendingWithdrawalInfo], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.active_challenges</code></td><td><code>ActiveChallengesRequest</code></td><td><code>{items:[ChallengeRecord], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.relayer_quorum</code></td><td><code>RelayerQuorumRequest</code></td><td><code>RelayerQuorumInfo {epoch, members[], threshold}</code></td><td>none</td></tr>
<tr><td><code>bridge.reward_claims</code></td><td><code>RewardClaimsRequest</code></td><td><code>{items:[RewardClaimRecord], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.reward_accruals</code></td><td><code>RewardAccrualsRequest</code></td><td><code>{items:[RewardAccrualRecord], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.settlement_log</code></td><td><code>SettlementLogRequest</code></td><td><code>{items:[SettlementRecord], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.dispute_audit</code></td><td><code>DisputeAuditRequest</code></td><td><code>{items:[DisputeAuditRecord], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.relayer_accounting</code></td><td><code>RelayerAccountingRequest</code></td><td><code>{relayer, stake_ct, duty_totals, slash_totals}</code></td><td>none</td></tr>
<tr><td><code>bridge.duty_log</code></td><td><code>DutyLogRequest</code></td><td><code>{items:[DutyRecord], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.deposit_history</code></td><td><code>DepositHistoryRequest</code></td><td><code>{items:[DepositReceipt], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.slash_log</code></td><td><code>SlashLogRequest</code></td><td><code>{items:[SlashRecord], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>bridge.assets</code></td><td><code>AssetsRequest</code></td><td><code>{assets:[BridgeAssetSnapshot]}</code></td><td>none</td></tr>
<tr><td><code>bridge.configure_asset</code></td><td><code>ConfigureAssetRequest</code></td><td><code>{status:"ok"}</code> after writing channel config</td><td>governance rejects via bridge error codes</td></tr>
</tbody></table>
</div>
<h4 id="domain-registry-and-gateway-surfaces-nodesrcgatewaydnsrs-nodesrcservice_badgers-nodesrcgatewaymobile_cachers"><a class="header" href="#domain-registry-and-gateway-surfaces-nodesrcgatewaydnsrs-nodesrcservice_badgers-nodesrcgatewaymobile_cachers">Domain registry and gateway surfaces (<code>node/src/gateway/dns.rs</code>, <code>node/src/service_badge.rs</code>, <code>node/src/gateway/mobile_cache.rs</code>)</a></h4>
<p>DNS methods share the JSON schema under <code>docs/spec/dns_record.schema.json</code>. Auctions/stakes use sled-backed objects <code>{domain, owner, reserve_ct, bidders[], expires_at}</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>dns.publish_record</code></td><td><code>DnsRecord</code> JSON (TXT + proof metadata)</td><td><code>{status:"ok", record_hash}</code></td><td>schema validation errors surfaced via <code>gateway::dns::Error</code> codes (forwarded via <code>rpc_error</code>)</td></tr>
<tr><td><code>dns.list_for_sale</code></td><td><code>{domain,reserve_ct, ttl_blocks}</code></td><td><code>{status:"ok"}</code> once sale is staged</td><td>same as above</td></tr>
<tr><td><code>dns.place_bid</code></td><td><code>{domain,bid_ct,bidder}</code></td><td><code>{status:"ok", leading_bid}</code></td><td>bidding window/price errors via dns error codes</td></tr>
<tr><td><code>dns.complete_sale</code></td><td><code>{domain}</code></td><td><code>{status:"ok", new_owner}</code></td><td><code>not_found</code>, <code>auction_open</code> style dns errors</td></tr>
<tr><td><code>dns.cancel_sale</code></td><td><code>{domain}</code></td><td><code>{status:"ok"}</code></td><td>dns errors</td></tr>
<tr><td><code>dns.register_stake</code></td><td><code>{domain,stake_ct}</code></td><td><code>{status:"ok", stake_id}</code></td><td>dns errors</td></tr>
<tr><td><code>dns.withdraw_stake</code></td><td><code>{stake_id}</code></td><td><code>{status:"ok"}</code></td><td>dns errors</td></tr>
<tr><td><code>dns.stake_status</code></td><td><code>{stake_id?}</code></td><td><code>{stakes:[{domain, owner, stake_ct, expires_at}]}</code> or single entry when ID supplied</td><td>dns errors</td></tr>
<tr><td><code>dns.auctions</code></td><td><code>{domain?}</code> filter</td><td><code>{auctions:[{domain,status,reserve_ct,bids,closes_at}]}</code></td><td>dns errors</td></tr>
<tr><td><code>gateway.policy</code></td><td><code>{}</code> or filters (<code>region?</code>)</td><td>Current gateway policy JSON (rate limits, read-ack requirements, privacy toggles)</td><td>none</td></tr>
<tr><td><code>gateway.reads_since</code></td><td><code>{since_ms}</code></td><td><code>{count, buckets:[{ts_ms,count}]}</code> built from read-ack batches</td><td>none</td></tr>
<tr><td><code>gateway.venue_status</code></td><td><code>{venue_id}</code></td><td><code>{status:"ok", crowd_size, last_seen}</code> from <code>service_badge::venue_status_detail</code></td><td><code>-32602</code> missing venue_id</td></tr>
<tr><td><code>gateway.venue_register</code></td><td><code>{venue_id}</code></td><td><code>{status:"ok", venue_id, token, expires_at}</code></td><td><code>-32602</code> missing ID</td></tr>
<tr><td><code>gateway.venue_rotate</code></td><td><code>{venue_id}</code></td><td><code>{status:"ok", token, expires_at}</code> with freshly issued badge</td><td><code>-32602</code> missing ID</td></tr>
<tr><td><code>gateway.dns_lookup</code></td><td><code>{domain}</code></td><td>DNS proof bundle as returned by <code>gateway::dns::dns_lookup</code> (includes TXT contents and audit trail)</td><td>dns errors</td></tr>
<tr><td><code>gateway.mobile_cache_status</code></td><td>none</td><td>Snapshot from <code>gateway::mobile_cache::status_snapshot()</code> (entry counts, hits/misses)</td><td>none</td></tr>
<tr><td><code>gateway.mobile_cache_flush</code></td><td>none</td><td><code>{status:"ok", flushed_entries}</code></td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="compute-control-plane-nodesrcrpccompute_marketrs-nodesrccompute_marketmatcherrs"><a class="header" href="#compute-control-plane-nodesrcrpccompute_marketrs-nodesrccompute_marketmatcherrs">Compute control plane (<code>node/src/rpc/compute_market.rs</code>, <code>node/src/compute_market/matcher.rs</code>)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>compute.job_requirements</code></td><td><code>{job_id}</code></td><td>Requirements for a queued job (<code>{slices, profile, deadline_ms, fairness_lane}</code>)</td><td><code>MarketError</code> codes (propagated)</td></tr>
<tr><td><code>compute.job_status</code></td><td><code>{job_id}</code></td><td><code>{status, submitted_at, matched_at?, provider?, receipts[]}</code></td><td>market errors</td></tr>
<tr><td><code>compute.job_cancel</code></td><td><code>{job_id, reason?, requester}</code> (WebSocket upgrade supported)</td><td><code>{status:"ok"}</code> and cancels active batch</td><td><code>MarketError::Canceled</code> style codes</td></tr>
<tr><td><code>compute.provider_hardware</code></td><td><code>{provider}</code></td><td>Provider hardware manifest (CPU, GPU, mem, lanes) from registry</td><td><code>MarketError::UnknownProvider</code></td></tr>
<tr><td><code>compute.reputation_get</code></td><td><code>{provider}</code></td><td><code>{score, last_updated, evidence[]}</code></td><td>same as above</td></tr>
<tr><td><code>compute_arm_real</code></td><td><code>{}</code></td><td><code>{status:"ok"}</code> after toggling settlement to <code>SettleMode::Real</code></td><td>restricted; returns <code>-32601</code> when not compiled</td></tr>
<tr><td><code>compute_cancel_arm</code></td><td>none</td><td><code>{status:"ok"}</code> switching back to dry-run</td><td>restricted</td></tr>
<tr><td><code>compute_back_to_dry_run</code></td><td>none</td><td><code>{status:"ok"}</code> (alias)</td><td>restricted</td></tr>
</tbody></table>
</div><div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>compute_market.stats</code></td><td>`{accelerator?:"fpga"</td><td>"tpu"}` filter</td><td>Aggregated market stats <code>{lanes:[{lane, queued, matched, avg_fee}], receipts_pending}</code></td></tr>
<tr><td><code>compute_market.scheduler_stats</code></td><td>none</td><td>Raw scheduler view (per-lane queues, fairness windows)</td><td>none</td></tr>
<tr><td><code>compute_market.scheduler_metrics</code></td><td>none</td><td><code>scheduler::Metrics</code> structure (latency, starvation counters, lane deadlines)</td><td>none</td></tr>
<tr><td><code>compute_market.recent_roots</code></td><td><code>{n?}</code></td><td>Latest <code>n</code> micro-shard roots used for receipts</td><td>none</td></tr>
<tr><td><code>compute_market.provider_balances</code></td><td>none</td><td><code>{providers:[{id, pending_ct, settled_ct, sla_bond_ct}]}</code></td><td>none</td></tr>
<tr><td><code>compute_market.audit</code></td><td>none</td><td>Extended audit log incl. fairness window snapshots, outstanding SLA records</td><td><code>MarketError</code> codes</td></tr>
</tbody></table>
</div>
<h4 id="configuration-and-consensus"><a class="header" href="#configuration-and-consensus">Configuration and consensus</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>config.reload</code></td><td><code>{compaction_secs?, sample_rate_ppm?}</code></td><td><code>{status:"ok", sample_rate_ppm, compaction_secs}</code> and updates telemetry config</td><td>none (values sanitized)</td></tr>
<tr><td><code>consensus.difficulty</code></td><td><code>{window?}</code></td><td>Returns difficulty window stats (EMA short/med/long, next target)</td><td>none</td></tr>
<tr><td><code>consensus.pos.register</code></td><td><code>{validator, stake_ct, proof}</code></td><td><code>{status:"ok"}</code> once validator registered</td><td>PoS errors (<code>-32060</code> range)</td></tr>
<tr><td><code>consensus.pos.bond</code></td><td><code>{validator, stake_delta}</code></td><td><code>{status:"ok"}</code></td><td>same</td></tr>
<tr><td><code>consensus.pos.unbond</code></td><td><code>{validator, amount}</code></td><td><code>{status:"ok"}</code> after queueing unbond</td><td>same</td></tr>
<tr><td><code>consensus.pos.slash</code></td><td><code>{validator, reason, proof}</code></td><td><code>{status:"ok"}</code> after slash recorded</td><td>same</td></tr>
<tr><td><code>pow.get_template</code></td><td><code>{miner}</code> optional metadata</td><td><code>{block_template}</code> with header, transactions, and PoH tick info</td><td><code>-32070</code> when mining disabled</td></tr>
<tr><td><code>pow.submit</code></td><td><code>{block}</code> serialized candidate</td><td><code>{status:"ok", accepted:bool}</code></td><td><code>-32071</code> invalid PoW</td></tr>
</tbody></table>
</div>
<h4 id="identity-jurisdiction-kyc-and-legal-portals"><a class="header" href="#identity-jurisdiction-kyc-and-legal-portals">Identity, jurisdiction, KYC, and legal portals</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>register_handle</code></td><td><code>{address, handle}</code></td><td><code>{status:"ok"}</code> once handle stored</td><td><code>-32602</code> invalid strings, <code>-32080</code> handle taken</td></tr>
<tr><td><code>resolve_handle</code></td><td><code>{handle}</code></td><td><code>{address}</code> or <code>{status:"unknown"}</code></td><td>none</td></tr>
<tr><td><code>identity.anchor</code></td><td><code>{address, did_document}</code> uses DID schema</td><td><code>{status:"ok", anchor_hash}</code></td><td><code>kyc::Error</code> mapped to <code>-32081</code> family</td></tr>
<tr><td><code>identity.resolve</code></td><td><code>{address}</code></td><td>Latest DID document + timeline</td><td>none</td></tr>
<tr><td><code>jurisdiction.status</code></td><td><code>{}</code></td><td>Active jurisdiction pack metadata (hashes, version, enabled modules)</td><td>none</td></tr>
<tr><td><code>jurisdiction.set</code></td><td><code>{pack_json, signature}</code></td><td><code>{status:"ok"}</code> once stored in sled-backed <code>GovStore</code></td><td>signature errors -&gt; <code>-32090</code></td></tr>
<tr><td><code>jurisdiction.policy_diff</code></td><td><code>{from, to}</code> pack hashes</td><td><code>{diff:{added[], removed[], changed[]}}</code></td><td>none</td></tr>
<tr><td><code>kyc.verify</code></td><td><code>{provider, payload}</code> forwarded to configured KYC plugin</td><td><code>{status, provider_receipt}</code></td><td>provider-specific errors</td></tr>
<tr><td><code>record_le_request</code></td><td><code>{request:LawEnforcementRequest}</code> per <code>node/src/le_portal.rs</code></td><td><code>{status:"ok", request_id}</code></td><td><code>-32603</code> when LE portal disabled</td></tr>
<tr><td><code>le.list_requests</code></td><td><code>{limit?, cursor?}</code></td><td><code>{items:[{id, agency, type, opened_at, status}], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>le.record_action</code></td><td><code>{request_id, action, actor, notes?}</code></td><td><code>{status:"ok"}</code></td><td><code>-32095</code> missing request</td></tr>
<tr><td><code>le.upload_evidence</code></td><td><code>{request_id, filename, content_base64}</code></td><td><code>{status:"ok"}</code> plus evidence hash</td><td><code>-32095</code></td></tr>
<tr><td><code>warrant_canary</code></td><td><code>{status, signed_at, signature}</code></td><td><code>{status:"ok"}</code> after atomic append to log</td><td>signature check failures -&gt; <code>-32096</code></td></tr>
</tbody></table>
</div>
<h4 id="light-clients-nodesrcrpclightrs-nodesrcrpclight_clientrs-nodesrcrpcstate_streamrs"><a class="header" href="#light-clients-nodesrcrpclightrs-nodesrcrpclight_clientrs-nodesrcrpcstate_streamrs">Light clients (<code>node/src/rpc/light.rs</code>, <code>node/src/rpc/light_client.rs</code>, <code>node/src/rpc/state_stream.rs</code>)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>light.headers</code></td><td><code>{from_height, count}</code></td><td><code>{headers:[{height,header_fields...}]}</code> chunked for device syncing</td><td>none</td></tr>
<tr><td><code>light.latest_header</code></td><td>none</td><td><code>{height, header}</code> for best tip</td><td>none</td></tr>
<tr><td><code>light_client.rebate_status</code></td><td><code>{device_id}</code></td><td><code>{pending_ct, claimed_ct, last_claim_ts, proofs[]}</code></td><td><code>-32030</code> unauthorized when device not registered</td></tr>
<tr><td><code>light_client.rebate_history</code></td><td><code>{device_id, limit?, cursor?}</code></td><td><code>{items:[{epoch,ct,receipt_hash}], next_cursor?}</code></td><td>same as above</td></tr>
<tr><td><code>state_stream.subscribe</code></td><td>WebSocket upgrade, no body</td><td>Streams <code>{height, root, accounts_changed, proofs}</code> batches</td><td>HTTP 429 when subscriber cap reached</td></tr>
</tbody></table>
</div>
<h4 id="localnet-and-range-boost-helpers"><a class="header" href="#localnet-and-range-boost-helpers">LocalNet and range-boost helpers</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>localnet.submit_receipt</code></td><td><code>{receipt}</code> hex-encoded <code>AssistReceipt</code></td><td><code>{status:"ok"}</code> if new; <code>{status:"ignored"}</code> when duplicate</td><td><code>-32602</code> invalid hex or receipt, <code>-32002</code> invalid proximity</td></tr>
<tr><td><code>mempool.stats</code></td><td>`{lane:"consumer"</td><td>"industrial"}`</td><td><code>{queue_depth, avg_fee, ttl_ms, evictions, qos:{admission_slots, lane}, fee_floor}</code> from <code>Blockchain::mempool_stats</code></td></tr>
<tr><td><code>mempool.qos_event</code></td><td><code>{lane, window_ms}</code> recorded for QoS debugging</td><td><code>{status:"ok"}</code></td><td>none</td></tr>
<tr><td><code>mesh.peers</code></td><td>none</td><td>Current range-boost mesh peers (<code>{id, addr, last_seen_ms}</code>)</td><td>none</td></tr>
<tr><td><code>microshard.roots.last</code></td><td><code>{n?}</code> (default 1)</td><td>Last <code>n</code> micro-shard roots with timestamps</td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="metrics-telemetry-and-observability"><a class="header" href="#metrics-telemetry-and-observability">Metrics, telemetry, and observability</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>metrics</code></td><td>none</td><td>Prometheus text exposition from <code>runtime::telemetry::TextEncoder</code></td><td>none</td></tr>
<tr><td><code>telemetry.configure</code></td><td><code>{sample_rate_ppm?, compaction_secs?}</code> (telemetry builds only)</td><td><code>{status:"ok", sample_rate_ppm, compaction_secs}</code> after adjusting runtime knobs</td><td><code>-32603</code> when telemetry compiled out</td></tr>
</tbody></table>
</div>
<h4 id="networking-and-peer-management-nodesrcrpcnetrs-plus-inlined-helpers"><a class="header" href="#networking-and-peer-management-nodesrcrpcnetrs-plus-inlined-helpers">Networking and peer management (<code>node/src/rpc/net.rs</code> plus inlined helpers)</a></h4>
<p>Most methods share the JSON structures defined in <code>node/src/net/peer.rs</code>: <code>PeerMetrics</code>, <code>PeerReputation</code>, QUIC cert summaries, etc.</p>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>net.overlay_status</code></td><td>none</td><td>`{backend:"inhouse"</td><td>"stub", path, last_persist_ms, peers:{active,persisted}}`</td></tr>
<tr><td><code>net.peer_stats</code></td><td><code>{peer_id}</code> (base58)</td><td>Full <code>PeerMetrics</code> struct converted to JSON</td><td><code>-32602</code> invalid peer ID</td></tr>
<tr><td><code>net.peer_stats_all</code></td><td>none</td><td><code>{peers:{id:PeerMetrics}}</code></td><td>none</td></tr>
<tr><td><code>net.peer_stats_reset</code></td><td><code>{peer_id}</code></td><td><code>{status:"ok"}</code> after zeroing counters</td><td>validation errors</td></tr>
<tr><td><code>net.peer_stats_export</code></td><td><code>{path}</code></td><td><code>{status:"ok", path}</code> after writing JSON export</td><td>IO errors -&gt; <code>-32010</code> style messages</td></tr>
<tr><td><code>net.peer_stats_export_all</code></td><td><code>{path}</code></td><td>same as above but includes all peers</td><td>IO errors</td></tr>
<tr><td><code>net.peer_stats_persist</code></td><td>none</td><td><code>{status:"ok"}</code> after writing sled snapshot of peer metrics</td><td>none</td></tr>
<tr><td><code>net.peer_throttle</code></td><td><code>{peer_id, backoff_secs}</code></td><td><code>{status:"ok"}</code> after updating throttle map</td><td>invalid IDs -&gt; <code>-32602</code></td></tr>
<tr><td><code>net.backpressure_clear</code></td><td><code>{peer_id}</code></td><td><code>{status:"ok"}</code> clearing throttle/backpressure state</td><td>invalid IDs</td></tr>
<tr><td><code>net.reputation_sync</code></td><td><code>{source?}</code></td><td><code>{status:"ok", synced}</code> after forcing reputation gossip</td><td>none</td></tr>
<tr><td><code>net.rotate_cert</code> / <code>net.key_rotate</code></td><td><code>{}</code></td><td><code>{status:"ok", fingerprint}</code> after rotating TLS/peer certs</td><td><code>-32020</code> when rotation disabled</td></tr>
<tr><td><code>net.handshake_failures</code></td><td>none</td><td><code>{failures:{reason:count}}</code> aggregated counters</td><td>none</td></tr>
<tr><td><code>net.quic_stats</code></td><td>none</td><td>QUIC stats snapshot (<code>{established,total_handshakes,rtt_ms}</code>)</td><td>none</td></tr>
<tr><td><code>net.quic_certs</code></td><td>none</td><td><code>{current:{fingerprint, expires_at}, previous?}</code></td><td>none</td></tr>
<tr><td><code>net.quic_certs_refresh</code></td><td>none</td><td><code>{status:"ok"}</code> after forcing re-read from disk</td><td>none</td></tr>
<tr><td><code>net.config_reload</code></td><td><code>{}</code></td><td><code>{status:"ok"}</code> after reloading peer config from disk</td><td>IO errors</td></tr>
<tr><td><code>net.reputation_show</code></td><td><code>{peer_id}</code></td><td><code>{score,last_update}</code></td><td>invalid IDs</td></tr>
<tr><td><code>net.gossip_status</code></td><td>none</td><td><code>{fanout, duplicates_dropped, partition_tags[]}</code></td><td>none</td></tr>
<tr><td><code>net.dns_verify</code></td><td><code>{domain}</code></td><td><code>{status:"ok", proof}</code> after verifying <code>.block</code> record</td><td>dns errors</td></tr>
</tbody></table>
</div>
<h4 id="node-rpc-toggles"><a class="header" href="#node-rpc-toggles">Node RPC toggles</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>node.get_ack_privacy</code></td><td>none</td><td>`{mode:"enforce"</td><td>"permissive"</td></tr>
<tr><td><code>node.set_ack_privacy</code></td><td><code>{mode}</code></td><td><code>{status:"ok"}</code> after updating <code>ReadAckPrivacyMode</code> (enforced via governance guardrails)</td><td><code>-32602</code> invalid mode</td></tr>
</tbody></table>
</div>
<h4 id="peer-rebate-rent-escrow-scheduler-and-storage-helpers"><a class="header" href="#peer-rebate-rent-escrow-scheduler-and-storage-helpers">Peer rebate, rent escrow, scheduler and storage helpers</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>peer.rebate_status</code></td><td><code>{peer_id}</code></td><td><code>{pending_ct, claimed_ct, receipts[]}</code></td><td>invalid ID</td></tr>
<tr><td><code>peer.rebate_claim</code></td><td><code>{peer_id}</code></td><td><code>{status:"ok", claimed_ct}</code> once receipts consumed</td><td>invalid ID</td></tr>
<tr><td><code>rent.escrow.balance</code></td><td><code>{provider}</code></td><td><code>{balance_ct, locked_until}</code> from <code>RentEscrow</code> sled</td><td>none</td></tr>
<tr><td><code>scheduler.stats</code></td><td>none</td><td>Scheduler debug payload <code>{lanes:[{lane, queued, fairness_deadline_ms, slice_quota}], starvation_counters}</code></td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="governance-treasury-and-service-badges"><a class="header" href="#governance-treasury-and-service-badges">Governance, treasury, and service badges</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>gov_propose</code> / <code>submit_proposal</code></td><td><code>{proposer, key, new_value, min, max, deps?, epoch, vote_deadline?}</code></td><td><code>{proposal_id}</code></td><td><code>-32040</code> invalid proposal, <code>-32041</code> when deps invalid</td></tr>
<tr><td><code>gov_vote</code> / <code>vote_proposal</code></td><td>`{voter, proposal_id, choice:"yes"</td><td>"no"</td><td>"abstain", epoch}`</td></tr>
<tr><td><code>gov_list</code></td><td>none</td><td><code>{proposals:[... active DAG ...]}</code></td><td>none</td></tr>
<tr><td><code>gov_params</code></td><td><code>{epoch?}</code></td><td><code>{params}</code> snapshot from <code>GOV_PARAMS</code></td><td>none</td></tr>
<tr><td><code>gov_rollback_last</code> / <code>gov_rollback</code></td><td><code>{epoch}</code> / <code>{proposal_id, epoch}</code></td><td><code>{status:"ok"}</code> after reverting governance store</td><td><code>-32043</code> rollback guardrails</td></tr>
<tr><td><code>gov.release_signers</code></td><td>none</td><td><code>{signers:[{name,pubkey}]} </code></td><td>none</td></tr>
<tr><td><code>gov.treasury.balance</code></td><td>none</td><td><code>{ct, industrial_ct, usd_estimate}</code> from treasury sled</td><td>none</td></tr>
<tr><td><code>gov.treasury.balance_history</code></td><td><code>{start?, end?}</code></td><td><code>{entries:[{epoch,ct}], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>gov.treasury.disbursements</code></td><td><code>{limit?, cursor?}</code></td><td><code>{items:[{proposal_id, amount_ct, recipient, executed_at}], next_cursor?}</code></td><td>none</td></tr>
<tr><td><code>service_badge_issue</code></td><td><code>{subject, badge}</code> (requires signer badge header)</td><td><code>{status:"ok"}</code> and records issuance</td><td><code>-32602</code> invalid payload, <code>-32050</code> when badge invalid</td></tr>
<tr><td><code>service_badge_revoke</code></td><td><code>{subject, badge}</code></td><td><code>{status:"ok"}</code></td><td>same</td></tr>
<tr><td><code>service_badge_verify</code></td><td><code>{token}</code></td><td><code>{valid:bool, subject?, expires_at?}</code></td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="settlement-toggles-startstop-mining-and-tpu-aids"><a class="header" href="#settlement-toggles-startstop-mining-and-tpu-aids">Settlement toggles, start/stop mining, and TPU aids</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>set_snapshot_interval</code></td><td><code>{interval}</code> blocks</td><td><code>{status:"ok"}</code> updating snapshot cadence (<code>SimpleDb::snapshot_interval</code>)</td><td><code>-32602</code> invalid interval, <code>-32050</code> when below minimum</td></tr>
<tr><td><code>start_mining</code> / <code>stop_mining</code></td><td>none</td><td><code>{status:"ok"}</code> toggling local POW miner thread</td><td><code>-32601</code> when miner compiled out</td></tr>
<tr><td><code>set_difficulty</code></td><td><code>{value}</code> (debug)</td><td><code>{status:"ok"}</code> force-sets PoW difficulty (tests only)</td><td>debug-only</td></tr>
<tr><td><code>tpu</code></td><td><code>{action}</code> (legacy hook)</td><td><code>{status:"ok"}</code> stub used by integration tests</td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="economic-tooling-dexhtlc-industrial-stats-and-price-board"><a class="header" href="#economic-tooling-dexhtlc-industrial-stats-and-price-board">Economic tooling, DEX/HTLC, industrial stats, and price board</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>industrial</code></td><td>none</td><td><code>{backlog, utilization_ppm, multiplier}</code> from subsidy governor</td><td>none</td></tr>
<tr><td><code>price_board_get</code></td><td><code>{lane}</code></td><td><code>{lane, base_fee, tip_histogram}</code> used by wallet coaching</td><td>none</td></tr>
<tr><td><code>dex_escrow_status</code></td><td><code>{escrow_id}</code></td><td><code>{status, deposits, releases}</code> from <code>dex::escrow</code> sled</td><td><code>-32060</code> unknown escrow</td></tr>
<tr><td><code>dex_escrow_release</code></td><td><code>{escrow_id, proof}</code></td><td><code>{status:"ok"}</code></td><td>same as above</td></tr>
<tr><td><code>dex_escrow_proof</code></td><td><code>{escrow_id}</code></td><td>Proof bundle for explorer/CLI</td><td>same</td></tr>
<tr><td><code>htlc_status</code></td><td><code>{swap_id}</code></td><td><code>{status, expires_at, tx_refs}</code></td><td>unknown swap -&gt; <code>-32061</code></td></tr>
<tr><td><code>htlc_refund</code></td><td><code>{swap_id}</code></td><td><code>{status:"ok"}</code> after enforcing refund</td><td>same</td></tr>
<tr><td><code>analytics</code> already covered; <code>metrics</code> above.</td><td></td><td></td><td></td></tr>
</tbody></table>
</div>
<h4 id="vm-and-contract-debugging-nodesrcrpcvmrs"><a class="header" href="#vm-and-contract-debugging-nodesrcrpcvmrs">VM and contract debugging (<code>node/src/rpc/vm.rs</code>)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>vm.estimate_gas</code></td><td><code>{bytecode, calldata, state_root?}</code></td><td><code>{gas_used, trace?}</code> for WASM execution</td><td><code>-32070</code> invalid contract</td></tr>
<tr><td><code>vm.exec_trace</code></td><td><code>{tx_hash}</code></td><td><code>ExecTrace {steps[], state_changes}</code></td><td>same</td></tr>
<tr><td><code>vm.storage_read</code></td><td><code>{contract, key}</code></td><td><code>{value}</code></td><td>none</td></tr>
<tr><td><code>vm.storage_write</code></td><td><code>{contract, key, value}</code></td><td><code>{status:"ok"}</code> (debug only)</td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="miscellaneous-roots-whoami-metrics-handles"><a class="header" href="#miscellaneous-roots-whoami-metrics-handles">Miscellaneous roots (whoami, metrics, handles)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>whoami</code></td><td>none</td><td><code>{agent, version, features}</code></td><td>none</td></tr>
<tr><td><code>analytics</code> / <code>metrics</code> already covered.</td><td></td><td></td><td></td></tr>
</tbody></table>
</div>
<h4 id="transactions-and-fee-markets"><a class="header" href="#transactions-and-fee-markets">Transactions and fee markets</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>submit_tx</code></td><td><code>{tx}</code> hex-encoded <code>SignedTransaction</code> plus optional <code>{max_fee, tip}</code> overrides</td><td><code>{status:"ok"}</code> when enqueued; <code>{status:"error", error}</code> when mempool rejects</td><td><code>-32602</code> malformed hex/transaction, propagation errors as strings</td></tr>
<tr><td><code>price_board_get</code></td><td><code>{lane}</code></td><td><code>{lane, base_fee, tip_histogram:[{percentile, tip_micro_ct}]}</code> mirroring <code>node/src/fees.rs</code> board</td><td>none</td></tr>
<tr><td><code>industrial</code></td><td>none</td><td><code>{backlog_bytes, utilization_ppm, multiplier}</code> from subsidy governor gauges (see § CT subsidy)</td><td>none</td></tr>
<tr><td><code>inflation.params</code></td><td>none</td><td><code>{emission_curve:{start_height,end_height,rate}, decay_factor, treasury_split}</code> read from <code>docs/economics_and_governance.md</code> constants</td><td>none</td></tr>
</tbody></table>
</div>
<h4 id="staking--pos-helpers-nodesrcrpcposrs"><a class="header" href="#staking--pos-helpers-nodesrcrpcposrs">Staking / PoS helpers (<code>node/src/rpc/pos.rs</code>)</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>stake.role</code></td><td><code>{id, role?}</code> (role defaults to <code>validator</code>)</td><td><code>{id, role, stake}</code> from in-memory PoS state</td><td><code>-32602</code> missing ID</td></tr>
</tbody></table>
</div>
<h4 id="storage-pipeline-convenience-rpcs"><a class="header" href="#storage-pipeline-convenience-rpcs">Storage pipeline convenience RPCs</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Request</th><th>Response</th><th>Error codes</th></tr></thead><tbody>
<tr><td><code>storage_upload</code></td><td><code>{object_id, provider_id, original_bytes, shares, price_per_block, start_block, retention_blocks}</code></td><td>Delegates to <code>storage::upload</code>, returning manifest receipt (<code>{status:"ok", contract_id}</code>)</td><td>storage errors bubble up via <code>rpc_error(-32603,...)</code></td></tr>
<tr><td><code>storage_challenge</code></td><td><code>{object_id, provider_id?, chunk_idx, proof, current_block}</code></td><td><code>{status:"ok", challenge_id}</code> if proof accepted</td><td><code>-32602</code> invalid inputs; storage errors</td></tr>
<tr><td><code>storage.manifests</code></td><td><code>{limit?}</code></td><td><code>{manifests:[{object_id, provider_id, shares, retention_blocks, status}]}</code> limited to latest <code>limit</code> entries</td><td>none</td></tr>
<tr><td><code>storage_provider_profiles</code></td><td>none</td><td><code>{providers:[{id, capacity_bytes, maintenance, uptime_ppm}]}</code></td><td>none</td></tr>
<tr><td><code>storage_provider_set_maintenance</code></td><td><code>{provider, maintenance:bool}</code></td><td><code>{status:"ok"}</code> after toggling provider availability</td><td><code>-32030</code> unknown provider</td></tr>
<tr><td><code>storage_incentives</code></td><td>none</td><td><code>{rent_escrow:{credits_ct, debits_ct}, rebate_rates:{lane, ppm}}</code></td><td>none</td></tr>
</tbody></table>
</div>
<h3 id="appendix-b--metric-index"><a class="header" href="#appendix-b--metric-index">Appendix B · Metric Index</a></h3>
<p>The table below is generated directly from <code>node/src/telemetry.rs</code>. Run <code>python tools/extract_metrics.py --format markdown</code> whenever the telemetry file changes to keep the appendix in sync. For deployment guidance see <code>docs/operations.md#metrics-aggregator-ops</code> and <code>monitoring/README.md</code>.</p>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Type</th><th>Labels</th><th>Units</th><th>Description</th></tr></thead><tbody>
<tr><td><code>active_miners</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>effective active miners</td></tr>
<tr><td><code>ad_budget_snapshot_generated_at_micros</code></td><td>IntGauge</td><td>–</td><td>microseconds</td><td>Timestamp of the most recent budget snapshot in microseconds</td></tr>
<tr><td><code>aggregator_ingest_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total peer metric ingests</td></tr>
<tr><td><code>amm_swap_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total AMM swaps executed</td></tr>
<tr><td><code>anomaly_alarm_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total number of anomaly alarms raised</td></tr>
<tr><td><code>anomaly_labels_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Anomaly labels submitted for model feedback</td></tr>
<tr><td><code>badge_active</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Whether a service badge is active (1/0)</td></tr>
<tr><td><code>badge_issued_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Service badges issued</td></tr>
<tr><td><code>badge_last_change_seconds</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>Unix timestamp of the last badge mint/burn</td></tr>
<tr><td><code>badge_revoked_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Service badges revoked</td></tr>
<tr><td><code>balance_overflow_reject_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Transactions rejected due to balance overflow</td></tr>
<tr><td><code>banned_peers_total</code></td><td>IntGauge</td><td>–</td><td>count</td><td>Total peers currently banned</td></tr>
<tr><td><code>base_fee</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>current base fee</td></tr>
<tr><td><code>base_reward_ct</code></td><td>IntGauge</td><td>–</td><td>CT</td><td>base reward after logistic factor</td></tr>
<tr><td><code>block_apply_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Blocks that failed atomic application</td></tr>
<tr><td><code>block_mined_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total mined blocks</td></tr>
<tr><td><code>build_provenance_invalid_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Build provenance checks that failed</td></tr>
<tr><td><code>build_provenance_valid_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Build provenance checks that succeeded</td></tr>
<tr><td><code>cluster_peer_active_total</code></td><td>IntGauge</td><td>–</td><td>count</td><td>Unique peers tracked by aggregator</td></tr>
<tr><td><code>codec_payload_bytes</code></td><td>HistogramVec</td><td>codec, direction, profile, version</td><td>bytes</td><td>Serialized payload size grouped by codec profile</td></tr>
<tr><td><code>compute_job_timeout_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Jobs exceeding declared deadlines</td></tr>
<tr><td><code>compute_sla_automated_slash_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Count of SLA penalties applied automatically by the settlement engine</td></tr>
<tr><td><code>compute_sla_next_deadline_ts</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Unix timestamp of the next pending compute SLA deadline</td></tr>
<tr><td><code>compute_sla_pending_total</code></td><td>IntGauge</td><td>–</td><td>count</td><td>Number of compute jobs with active SLA tracking</td></tr>
<tr><td><code>config_reload_last_ts</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Unix timestamp of the last successful config reload</td></tr>
<tr><td><code>consumer_fee_p50</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Median consumer fee</td></tr>
<tr><td><code>consumer_fee_p90</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>p90 consumer fee</td></tr>
<tr><td><code>courier_flush_attempt_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total courier receipt flush attempts</td></tr>
<tr><td><code>courier_flush_failure_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Failed courier receipt flush attempts</td></tr>
<tr><td><code>dex_escrow_locked</code></td><td>IntGauge</td><td>–</td><td>count</td><td>Total funds locked in DEX escrow</td></tr>
<tr><td><code>dex_escrow_pending</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Number of pending DEX escrows</td></tr>
<tr><td><code>dex_liquidity_locked_total</code></td><td>IntGauge</td><td>–</td><td>count</td><td>Total liquidity currently locked in DEX escrow</td></tr>
<tr><td><code>dex_trade_volume</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total matched trade quantity across all DEX pairs</td></tr>
<tr><td><code>did_anchor_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total number of anchored DID documents</td></tr>
<tr><td><code>difficulty_clamp_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Retarget calculations clamped to bounds</td></tr>
<tr><td><code>difficulty_retarget_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Number of difficulty retarget calculations</td></tr>
<tr><td><code>difficulty_window_long</code></td><td>IntGauge</td><td>–</td><td>milliseconds</td><td>Long-term EMA of block intervals in ms</td></tr>
<tr><td><code>difficulty_window_med</code></td><td>IntGauge</td><td>–</td><td>milliseconds</td><td>Medium-term EMA of block intervals in ms</td></tr>
<tr><td><code>difficulty_window_short</code></td><td>IntGauge</td><td>–</td><td>milliseconds</td><td>Short-term EMA of block intervals in ms</td></tr>
<tr><td><code>dkg_round_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Completed DKG rounds</td></tr>
<tr><td><code>dns_verification_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total DNS verification failures</td></tr>
<tr><td><code>drop_not_found_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>drop_transaction failures for missing entries</td></tr>
<tr><td><code>dup_tx_reject_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Transactions rejected as duplicate</td></tr>
<tr><td><code>evictions_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total mempool evictions</td></tr>
<tr><td><code>fee_floor_current</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Current dynamically computed fee floor</td></tr>
<tr><td><code>fee_floor_reject_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Transactions rejected for low fee</td></tr>
<tr><td><code>fee_floor_window_changed_total</code></td><td>IntCounter</td><td>–</td><td>ratio</td><td>Total governance-triggered fee floor policy reconfigurations</td></tr>
<tr><td><code>fib_window_base_secs</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>base seconds for Fibonacci smoothing</td></tr>
<tr><td><code>gossip_convergence_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Time for all peers to agree on the network tip</td></tr>
<tr><td><code>gossip_duplicate_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Duplicate gossip messages dropped</td></tr>
<tr><td><code>gossip_fanout_gauge</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Current gossip fanout</td></tr>
<tr><td><code>gossip_latency_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Observed latency hints used for adaptive gossip fanout</td></tr>
<tr><td><code>gossip_ttl_drop_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Gossip dedup entries removed due to TTL expiry</td></tr>
<tr><td><code>gov_open_proposals</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Open governance proposals</td></tr>
<tr><td><code>gov_proposals_pending</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Governance proposals pending activation</td></tr>
<tr><td><code>gov_quorum_required</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Governance quorum</td></tr>
<tr><td><code>haar_eta_milli</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>eta parameter for burst veto x1000</td></tr>
<tr><td><code>hash_ops_total</code></td><td>IntCounter</td><td>–</td><td>ratio</td><td>Total number of hash operations measured via perf counters</td></tr>
<tr><td><code>heuristic_mu_milli</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>A* heuristic mu x1000</td></tr>
<tr><td><code>htlc_created_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>HTLC contracts created</td></tr>
<tr><td><code>htlc_refunded_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>HTLC contracts refunded</td></tr>
<tr><td><code>identity_nonce_skips_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Non-contiguous nonce submissions</td></tr>
<tr><td><code>identity_replays_blocked_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Rejected identity replay attempts</td></tr>
<tr><td><code>industrial_admitted_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Industrial lane transactions admitted</td></tr>
<tr><td><code>industrial_backlog</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Pending industrial compute slices</td></tr>
<tr><td><code>industrial_deferred_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Industrial lane submissions deferred</td></tr>
<tr><td><code>industrial_multiplier</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Current industrial subsidy multiplier</td></tr>
<tr><td><code>industrial_price_per_unit</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Latest price per compute unit</td></tr>
<tr><td><code>industrial_units_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total normalized compute units processed</td></tr>
<tr><td><code>industrial_utilization</code></td><td>IntGauge</td><td>–</td><td>percent</td><td>Industrial compute utilisation percentage</td></tr>
<tr><td><code>inter_shard_replay_evict_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total inter-shard replay cache evictions</td></tr>
<tr><td><code>invalid_selector_reject_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Transactions rejected for invalid fee selector</td></tr>
<tr><td><code>job_age_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Time a job waited in the scheduler queue</td></tr>
<tr><td><code>job_resubmitted_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Jobs resubmitted after provider failure</td></tr>
<tr><td><code>key_rotation_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Successful peer key rotations</td></tr>
<tr><td><code>kill_switch_trigger_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Times the subsidy kill switch was activated</td></tr>
<tr><td><code>light_client_stream_overhead_bytes_total</code></td><td>IntCounter</td><td>–</td><td>bytes</td><td>Bytes of overhead for light-client streaming</td></tr>
<tr><td><code>liquidity_rewards_disbursed_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Liquidity mining rewards distributed</td></tr>
<tr><td><code>lock_poison_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Lock acquisition failures due to poisoning</td></tr>
<tr><td><code>log_correlation_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Correlation lookups that returned no matching log entries</td></tr>
<tr><td><code>log_entries_indexed_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total JSON log entries processed by the offline indexer</td></tr>
<tr><td><code>log_size_bytes</code></td><td>Histogram</td><td>–</td><td>bytes</td><td>Size of serialized log events in bytes</td></tr>
<tr><td><code>match_loop_latency_seconds</code></td><td>HistogramVec</td><td>lane</td><td>seconds</td><td>Settlement loop latency</td></tr>
<tr><td><code>mempool_evictions_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total transactions evicted from the mempool</td></tr>
<tr><td><code>mempool_size</code></td><td>IntGaugeVec</td><td>lane</td><td>unitless</td><td>Current mempool size</td></tr>
<tr><td><code>mempool_size</code></td><td>GaugeVec</td><td>lane</td><td>unitless</td><td>Current mempool size</td></tr>
<tr><td><code>miner_reward_recalc_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Times the miner reward logistic factor was recalculated</td></tr>
<tr><td><code>mobile_cache_entry_bytes</code></td><td>IntGauge</td><td>–</td><td>bytes</td><td>Total bytes stored in the mobile cache</td></tr>
<tr><td><code>mobile_cache_entry_total</code></td><td>IntGauge</td><td>–</td><td>count</td><td>Active mobile cache entries</td></tr>
<tr><td><code>mobile_cache_evict_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Expired or purged mobile cache entries</td></tr>
<tr><td><code>mobile_cache_hit_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total mobile cache hits</td></tr>
<tr><td><code>mobile_cache_miss_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total mobile cache misses</td></tr>
<tr><td><code>mobile_cache_queue_bytes</code></td><td>IntGauge</td><td>–</td><td>bytes</td><td>Bytes buffered in the mobile offline queue</td></tr>
<tr><td><code>mobile_cache_queue_total</code></td><td>IntGauge</td><td>–</td><td>count</td><td>Offline transactions queued for replay</td></tr>
<tr><td><code>mobile_cache_reject_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Mobile cache insertions rejected by limits</td></tr>
<tr><td><code>mobile_cache_stale_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Mobile cache entries dropped due to TTL expiry</td></tr>
<tr><td><code>mobile_cache_sweep_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Number of mobile cache TTL sweeps</td></tr>
<tr><td><code>mobile_cache_sweep_window_seconds</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>Configured sweep interval for the mobile cache</td></tr>
<tr><td><code>mobile_tx_queue_depth</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Queued mobile transactions awaiting send</td></tr>
<tr><td><code>orphan_sweep_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Transactions dropped because the sender account is missing</td></tr>
<tr><td><code>parallel_execute_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Elapsed wall-clock time for ParallelExecutor batches</td></tr>
<tr><td><code>partition_events_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Number of detected network partitions</td></tr>
<tr><td><code>partition_recover_blocks</code></td><td>IntCounter</td><td>–</td><td>unitless</td><td>Blocks replayed during partition recovery</td></tr>
<tr><td><code>peer_metrics_active</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Number of peers currently tracked for telemetry</td></tr>
<tr><td><code>peer_metrics_dropped_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Websocket peer metrics frames dropped</td></tr>
<tr><td><code>peer_metrics_memory_bytes</code></td><td>IntGauge</td><td>–</td><td>bytes</td><td>Approximate memory used by peer metrics map</td></tr>
<tr><td><code>peer_metrics_subscribers</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Active peer metrics websocket subscribers</td></tr>
<tr><td><code>price_weight_applied_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total price entries adjusted by reputation weight</td></tr>
<tr><td><code>priority_boost_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Jobs whose priority was boosted due to aging</td></tr>
<tr><td><code>privacy_sanitization_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total sanitized payloads</td></tr>
<tr><td><code>proof_rebates_amount_total</code></td><td>IntCounter</td><td>–</td><td>CT</td><td>Total CT awarded via proof rebates</td></tr>
<tr><td><code>proof_rebates_claimed_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total proof rebate claims</td></tr>
<tr><td><code>proof_rebates_pending_total</code></td><td>IntGauge</td><td>–</td><td>CT</td><td>Pending CT rebates awaiting claim</td></tr>
<tr><td><code>quic_bytes_recv_total</code></td><td>IntCounter</td><td>–</td><td>bytes</td><td>Total bytes received over QUIC</td></tr>
<tr><td><code>quic_bytes_sent_total</code></td><td>IntCounter</td><td>–</td><td>bytes</td><td>Total bytes sent over QUIC</td></tr>
<tr><td><code>quic_conn_latency_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>QUIC connection handshake latency</td></tr>
<tr><td><code>quic_endpoint_reuse_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total QUIC endpoint reuse count</td></tr>
<tr><td><code>quic_fallback_tcp_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total times QUIC connections fell back to TCP</td></tr>
<tr><td><code>quic_retransmit_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total QUIC packet retransmissions</td></tr>
<tr><td><code>range_boost_enqueue_error_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>RangeBoost enqueue attempts dropped due to injection</td></tr>
<tr><td><code>range_boost_forwarder_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>RangeBoost forwarder failures observed</td></tr>
<tr><td><code>range_boost_queue_depth</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Current number of bundles pending in the RangeBoost queue</td></tr>
<tr><td><code>range_boost_queue_oldest_seconds</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>Age in seconds of the oldest RangeBoost queue entry</td></tr>
<tr><td><code>range_boost_toggle_latency_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Latency observed between RangeBoost enable/disable toggles</td></tr>
<tr><td><code>read_selection_proof_latency_seconds</code></td><td>HistogramVec</td><td>attestation</td><td>seconds</td><td>Selection proof verification latency by attestation</td></tr>
<tr><td><code>rebate_claims_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Peer rebate claims submitted</td></tr>
<tr><td><code>rebate_issued_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Rebate vouchers issued</td></tr>
<tr><td><code>receipt_corrupt_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Corrupted receipt entries on load</td></tr>
<tr><td><code>receipt_persist_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Receipt persistence failures</td></tr>
<tr><td><code>release_installs_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Nodes booted with governance-approved releases</td></tr>
<tr><td><code>release_quorum_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Release submissions rejected due to insufficient provenance signatures</td></tr>
<tr><td><code>remote_signer_key_rotation_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Remote signer key rotations</td></tr>
<tr><td><code>remote_signer_latency_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Remote signer latency</td></tr>
<tr><td><code>remote_signer_request_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total remote signer requests</td></tr>
<tr><td><code>remote_signer_success_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Successful remote signer responses</td></tr>
<tr><td><code>rent_escrow_burned_ct_total</code></td><td>IntCounter</td><td>–</td><td>CT</td><td>Total CT burned from rent escrow</td></tr>
<tr><td><code>rent_escrow_locked_ct_total</code></td><td>IntGauge</td><td>–</td><td>CT</td><td>Total CT locked in rent escrow</td></tr>
<tr><td><code>rent_escrow_refunded_ct_total</code></td><td>IntCounter</td><td>–</td><td>CT</td><td>Total CT refunded from rent escrow</td></tr>
<tr><td><code>reputation_gossip_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Reputation updates that failed verification or were stale</td></tr>
<tr><td><code>reputation_gossip_latency_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Propagation latency for reputation updates</td></tr>
<tr><td><code>retrieval_failure_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total failed proof-of-retrievability challenges</td></tr>
<tr><td><code>retrieval_success_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total successful proof-of-retrievability challenges</td></tr>
<tr><td><code>rpc_bans_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total RPC bans issued</td></tr>
<tr><td><code>rpc_latency_seconds</code></td><td>HistogramVec</td><td>module</td><td>seconds</td><td>Latency histogram per RPC module</td></tr>
<tr><td><code>rpc_rate_limit_attempt_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>RPC requests checked against the rate limiter</td></tr>
<tr><td><code>rpc_rate_limit_reject_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>RPC requests rejected by the rate limiter</td></tr>
<tr><td><code>runtime_pending_tasks</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Pending async tasks managed by the runtime</td></tr>
<tr><td><code>runtime_spawn_latency_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Latency observed when spawning tasks on the runtime</td></tr>
<tr><td><code>scheduler_accelerator_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Accelerator jobs that failed or were cancelled</td></tr>
<tr><td><code>scheduler_accelerator_miss_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Jobs requiring accelerators that could not be matched</td></tr>
<tr><td><code>scheduler_accelerator_util_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Jobs requiring accelerators that started successfully</td></tr>
<tr><td><code>scheduler_active_jobs</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Number of currently assigned jobs</td></tr>
<tr><td><code>scheduler_class_wait_seconds</code></td><td>HistogramVec</td><td>class</td><td>seconds</td><td>Wait time per service class before execution</td></tr>
<tr><td><code>scheduler_match_latency_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Time to perform a scheduler match</td></tr>
<tr><td><code>scheduler_priority_miss_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>High-priority jobs exceeding wait threshold</td></tr>
<tr><td><code>scheduler_provider_reputation</code></td><td>Histogram</td><td>–</td><td>unitless</td><td>Distribution of provider reputation scores</td></tr>
<tr><td><code>scheduler_thread_count</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Current compute scheduler worker threads</td></tr>
<tr><td><code>session_key_expired_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Expired session keys encountered</td></tr>
<tr><td><code>session_key_issued_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Session keys issued</td></tr>
<tr><td><code>settle_applied_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Receipts applied</td></tr>
<tr><td><code>settle_audit_mismatch_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Receipts failing settlement audit</td></tr>
<tr><td><code>shard_cache_evict_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total shard cache evictions</td></tr>
<tr><td><code>shielded_pool_size</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Number of pending shielded nullifiers</td></tr>
<tr><td><code>shielded_tx_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total shielded transactions accepted</td></tr>
<tr><td><code>sigverify_ops_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total number of signature verifications measured via perf counters</td></tr>
<tr><td><code>slashing_burn_ct_total</code></td><td>IntCounter</td><td>–</td><td>CT</td><td>Total CT burned from slashing penalties</td></tr>
<tr><td><code>snapshot_created_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total snapshots created</td></tr>
<tr><td><code>snapshot_duration_seconds</code></td><td>Histogram</td><td>–</td><td>seconds</td><td>Snapshot operation duration</td></tr>
<tr><td><code>snapshot_fail_total</code></td><td>IntCounter</td><td>–</td><td>ratio</td><td>Total snapshot operation failures</td></tr>
<tr><td><code>snapshot_interval</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Snapshot interval in blocks</td></tr>
<tr><td><code>snapshot_interval_changed</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Last requested snapshot interval</td></tr>
<tr><td><code>snapshot_restore_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Failed snapshot restores</td></tr>
<tr><td><code>snark_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Failed SNARK proof verifications</td></tr>
<tr><td><code>snark_verifications_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Successfully verified SNARK proofs</td></tr>
<tr><td><code>snark_prover_latency_seconds</code></td><td>HistogramVec</td><td>backend</td><td>seconds</td><td>SNARK prover latency segmented by backend</td></tr>
<tr><td><code>snark_prover_failure_total</code></td><td>IntCounterVec</td><td>backend</td><td>count</td><td>SNARK prover failures segmented by backend</td></tr>
<tr><td><code>startup_ttl_drop_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Expired mempool entries dropped during startup</td></tr>
<tr><td><code>state_stream_lag_alert_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Clients falling behind alert count</td></tr>
<tr><td><code>state_stream_subscribers_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total websocket state stream subscribers</td></tr>
<tr><td><code>state_sync_overhead_bytes_total</code></td><td>IntCounter</td><td>–</td><td>bytes</td><td>Bytes of overhead for state sync streaming</td></tr>
<tr><td><code>storage_chunk_size_bytes</code></td><td>Histogram</td><td>–</td><td>bytes</td><td>Size of chunks put into storage</td></tr>
<tr><td><code>storage_compaction_total</code></td><td>IntCounter</td><td>–</td><td>ratio</td><td>Number of RocksDB compaction operations</td></tr>
<tr><td><code>storage_compression_ratio</code></td><td>HistogramVec</td><td>algorithm</td><td>ratio</td><td>Compression ratios achieved per algorithm</td></tr>
<tr><td><code>storage_contract_created_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total number of storage contracts created</td></tr>
<tr><td><code>storage_disk_full_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Number of storage writes that failed due to disk exhaustion</td></tr>
<tr><td><code>storage_final_chunk_size</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Final preferred chunk size after upload</td></tr>
<tr><td><code>storage_initial_chunk_size</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Initial chunk size used for object upload</td></tr>
<tr><td><code>storage_provider_loss_rate</code></td><td>HistogramVec</td><td>provider</td><td>unitless</td><td>Observed provider loss rate</td></tr>
<tr><td><code>storage_provider_rtt_ms</code></td><td>HistogramVec</td><td>provider</td><td>milliseconds</td><td>Observed provider RTT in milliseconds</td></tr>
<tr><td><code>storage_put_chunk_seconds</code></td><td>HistogramVec</td><td>erasure, compression</td><td>seconds</td><td>Time to put a single chunk</td></tr>
<tr><td><code>storage_put_eta_seconds</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>Estimated time to upload object in seconds</td></tr>
<tr><td><code>storage_put_object_seconds</code></td><td>HistogramVec</td><td>erasure, compression</td><td>seconds</td><td>End-to-end latency for StoragePipeline::put_object</td></tr>
<tr><td><code>storage_repair_bytes_total</code></td><td>IntCounter</td><td>–</td><td>bytes</td><td>Total bytes reconstructed by repair loop</td></tr>
<tr><td><code>subsidy_auto_reduced_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Multiplier auto-reduction events due to inflation guard</td></tr>
<tr><td><code>subsidy_cpu_ms_total</code></td><td>IntCounter</td><td>–</td><td>milliseconds</td><td>Total subsidized compute time in ms</td></tr>
<tr><td><code>telemetry_alloc_bytes</code></td><td>IntGauge</td><td>–</td><td>bytes</td><td>Telemetry memory allocation in bytes</td></tr>
<tr><td><code>threshold_signature_fail_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Failed threshold signature verifications</td></tr>
<tr><td><code>token_bridge_volume_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Volume bridged via token bridge</td></tr>
<tr><td><code>tokens_created_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total number of registered tokens</td></tr>
<tr><td><code>treasury_executor_last_error_seconds</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>Timestamp of the last treasury executor error</td></tr>
<tr><td><code>treasury_executor_last_submitted_nonce</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Latest nonce submitted by the treasury executor</td></tr>
<tr><td><code>treasury_executor_last_success_seconds</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>Timestamp of the last successful treasury executor run</td></tr>
<tr><td><code>treasury_executor_last_tick_seconds</code></td><td>IntGauge</td><td>–</td><td>seconds</td><td>Timestamp of the last treasury executor tick</td></tr>
<tr><td><code>treasury_executor_lease_last_nonce</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Lease watermark nonce retained across treasury executor holders</td></tr>
<tr><td><code>treasury_executor_lease_released</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Flag indicating the active treasury executor lease has been released</td></tr>
<tr><td><code>treasury_executor_pending_matured</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Matured treasury disbursements pending execution</td></tr>
<tr><td><code>treasury_executor_staged_intents</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>Staged treasury execution intents awaiting submission</td></tr>
<tr><td><code>ttl_drop_total</code></td><td>IntCounter</td><td>–</td><td>ratio</td><td>Transactions dropped due to TTL expiration</td></tr>
<tr><td><code>tx_admitted_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total admitted transactions</td></tr>
<tr><td><code>tx_submitted_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total submitted transactions</td></tr>
<tr><td><code>util_var_threshold_milli</code></td><td>IntGauge</td><td>–</td><td>unitless</td><td>utilisation variance threshold x1000</td></tr>
<tr><td><code>vm_gas_used_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total gas consumed by VM executions</td></tr>
<tr><td><code>vm_out_of_gas_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>VM executions that ran out of gas</td></tr>
<tr><td><code>vm_trace_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total VM trace sessions</td></tr>
<tr><td><code>wal_corrupt_recovery_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>WAL entries skipped due to checksum mismatch</td></tr>
<tr><td><code>wasm_contract_executions_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total WASM contract executions</td></tr>
<tr><td><code>wasm_gas_consumed_total</code></td><td>IntCounter</td><td>–</td><td>count</td><td>Total gas used by WASM contracts</td></tr>
</tbody></table>
</div>
<h3 id="appendix-c--simpledb-column-family-and-prefix-map"><a class="header" href="#appendix-c--simpledb-column-family-and-prefix-map">Appendix C · SimpleDb Column Family and Prefix Map</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Purpose</th><th>Common prefixes/keys</th></tr></thead><tbody>
<tr><td><code>default</code></td><td>Legacy chain state (fallback).</td><td><code>chain</code>, <code>accounts</code>.</td></tr>
<tr><td><code>bridge</code></td><td>Bridge headers, pending withdrawals.</td><td><code>header:&lt;height&gt;</code>, <code>withdrawal:&lt;commitment&gt;</code>.</td></tr>
<tr><td><code>compute_settlement</code></td><td>Settlement engine state (balances, audit log).</td><td><code>ledger_ct</code>, <code>ledger_it</code>, <code>sla_queue</code>, <code>sla_history</code>.</td></tr>
<tr><td><code>dex_storage</code></td><td>Order book, trades, escrow locks, AMM pools.</td><td><code>book</code>, <code>trade:*</code>, <code>escrow</code>, <code>amm/&lt;pool_id&gt;</code>.</td></tr>
<tr><td><code>gateway_dns</code></td><td>Domain auctions, stakes, ownership.</td><td><code>auction:&lt;domain&gt;</code>, <code>stake:&lt;ref&gt;</code>, <code>ownership:&lt;domain&gt;</code>.</td></tr>
<tr><td><code>gateway_ad_readiness</code></td><td>Readiness snapshots for ad flows.</td><td><code>snapshot:&lt;epoch&gt;</code>.</td></tr>
<tr><td><code>gossip_relay</code></td><td>Peer relay state for overlay dedup.</td><td><code>peer:&lt;id&gt;</code>.</td></tr>
<tr><td><code>identity_did</code></td><td>DID registry entries.</td><td><code>did:&lt;addr&gt;</code>.</td></tr>
<tr><td><code>identity_handle_registry</code></td><td>Handle → account mappings.</td><td><code>handles/&lt;handle&gt;</code>, <code>owners/&lt;address&gt;</code>, <code>nonces/&lt;address&gt;</code>.</td></tr>
<tr><td><code>light_client_proofs</code></td><td>Light-client proof cache.</td><td><code>header:&lt;height&gt;</code>.</td></tr>
<tr><td><code>localnet_receipts</code></td><td>LocalNet assist receipts.</td><td><code>receipt:&lt;hash&gt;</code>.</td></tr>
<tr><td><code>net_peer_chunks</code></td><td>Gossip chunk tracking.</td><td><code>chunk:&lt;root&gt;:&lt;idx&gt;</code>.</td></tr>
<tr><td><code>net_bans</code></td><td>Peer ban records.</td><td><code>peer:&lt;id&gt;</code>.</td></tr>
<tr><td><code>rpc_bridge</code></td><td>RPC bridge caches.</td><td>Implementation-specific.</td></tr>
<tr><td><code>storage_fs</code></td><td>Storage pipeline manifests, receipts.</td><td><code>manifest:&lt;hash&gt;</code>.</td></tr>
<tr><td><code>storage_pipeline</code></td><td>Chunk placement metadata.</td><td><code>chunk:&lt;id&gt;</code>.</td></tr>
<tr><td><code>storage_repair</code></td><td>Repair state/counters.</td><td><code>repair:&lt;provider&gt;</code>.</td></tr>
</tbody></table>
</div>
<h3 id="appendix-d--p2p-wire-payloads"><a class="header" href="#appendix-d--p2p-wire-payloads">Appendix D · P2P Wire Payloads</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Payload</th><th>Fields (order)</th><th>Notes</th></tr></thead><tbody>
<tr><td><code>Handshake</code></td><td><code>Hello</code> struct: <code>network_id[4], proto_version, feature_bits, agent, nonce, transport, quic_addr?, quic_cert?, quic_fingerprint?, quic_fingerprint_previous[], quic_provider?, quic_capabilities[]</code>.</td><td>First message every session.</td></tr>
<tr><td><code>Hello</code></td><td><code>Vec&lt;SocketAddr&gt;</code></td><td>Legacy peer advertisement.</td></tr>
<tr><td><code>Tx</code></td><td><code>SignedTransaction</code> (foundation serialization).</td><td>Used for gossip/broadcast.</td></tr>
<tr><td><code>BlobTx</code></td><td><code>BlobTx</code> struct (commitment, payload).</td><td>For L2 blobspace.</td></tr>
<tr><td><code>Block</code></td><td><code>(ShardId, Block)</code></td><td>Shard IDs live under <code>ledger::address::ShardId</code>.</td></tr>
<tr><td><code>Chain</code></td><td><code>Vec&lt;Block&gt;</code></td><td>Fork resolution snapshots.</td></tr>
<tr><td><code>BlobChunk</code></td><td><code>root[32], index, total, data</code></td><td>Erasure-coded shard.</td></tr>
<tr><td><code>Reputation</code></td><td><code>Vec&lt;ReputationUpdate { peer, delta, reason }&gt;</code></td><td>Synchronises overlay reputation.</td></tr>
</tbody></table>
</div>
<p>All payloads are serialized via <code>foundation_serialization::binary_cursor</code> to avoid serde drift.</p>
<h3 id="appendix-e--governance-parameter-catalog-partial"><a class="header" href="#appendix-e--governance-parameter-catalog-partial">Appendix E · Governance Parameter Catalog (partial)</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Key</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>snapshot_interval_secs</code></td><td>30</td><td>Block cadence for ledger snapshots.</td></tr>
<tr><td><code>consumer_fee_comfort_p90_microunits</code></td><td>2 500</td><td>Wallet fee guidance (p90).</td></tr>
<tr><td><code>fee_floor_window</code> / <code>fee_floor_percentile</code></td><td>256 / 75</td><td>Rolling window size and percentile for fee floors.</td></tr>
<tr><td><code>industrial_admission_min_capacity</code></td><td>10</td><td>Minimum queue capacity before opening industrial lane.</td></tr>
<tr><td><code>fairshare_global_max_ppm</code></td><td>250 000</td><td>QoS cap per account in parts‑per‑million.</td></tr>
<tr><td><code>burst_refill_rate_per_s_ppm</code></td><td>500 000 (≈30 tx/min)</td><td>Token bucket refill rate.</td></tr>
<tr><td><code>beta_storage_sub_ct</code>, <code>gamma_read_sub_ct</code>, <code>kappa_cpu_sub_ct</code>, <code>lambda_bytes_out_sub_ct</code></td><td>50/20/10/5</td><td>Subsidy multipliers (basis points).</td></tr>
<tr><td><code>read_subsidy_*_percent</code></td><td>Derived defaults</td><td>Split READ_SUB_CT across viewer/host/hardware/verifier/liquidity.</td></tr>
<tr><td><code>kill_switch_subsidy_reduction</code></td><td>0</td><td>Emergency knob to damp multipliers.</td></tr>
<tr><td><code>miner_reward_logistic_target</code>, <code>logistic_slope_milli</code>, <code>miner_hysteresis</code></td><td>100 / ~230 / 10</td><td>Shape of logistic emission curve.</td></tr>
<tr><td><code>badge_expiry_secs</code>, <code>badge_issue/revoke_uptime_percent</code></td><td>30 days / 99 % / 95 %</td><td>Service badge policy.</td></tr>
<tr><td><code>jurisdiction_region</code></td><td>0</td><td>Active policy pack region (int ID).</td></tr>
<tr><td><code>ai_diagnostics_enabled</code></td><td>0</td><td>Toggles ANN diagnostics.</td></tr>
<tr><td><code>kalman_r_{short,med,long}</code></td><td>1 / 3 / 8</td><td>Kalman filter variance terms for difficulty retune.</td></tr>
<tr><td><code>scheduler_weight_{gossip,compute,storage}</code></td><td>3 / 2 / 1</td><td>Weight of fairness window per workload type.</td></tr>
<tr><td><code>runtime_backend_policy</code>, <code>transport_provider_policy</code>, <code>storage_engine_policy</code></td><td>0× defaults</td><td>Governance-enforced backend selections.</td></tr>
<tr><td><code>bridge_min_bond</code>, <code>bridge_duty_reward</code>, <code>bridge_failure_slash</code>, <code>bridge_challenge_slash</code>, <code>bridge_duty_window_secs</code></td><td>From <code>BridgeIncentiveParameters</code></td><td>Bridge incentive/penalty knobs.</td></tr>
</tbody></table>
</div>
<p>(Refer to <code>governance/src/params.rs</code> for the exhaustive list and serialization order.)</p>
<h3 id="appendix-f--dns--read-receipt-flowchart"><a class="header" href="#appendix-f--dns--read-receipt-flowchart">Appendix F · DNS &amp; Read-Receipt Flowchart</a></h3>
<ol>
<li><code>tb-cli gateway domain list</code> → RPC <code>dns.list_for_sale</code> → SimpleDb <code>auction:&lt;domain&gt;</code> record.</li>
<li><code>tb-cli gateway domain bid</code> (optionally register stake first) → RPC <code>dns.place_bid</code>.</li>
<li><code>tb-cli gateway domain complete</code> → RPC <code>dns.complete_sale</code> → ledger events (seller, royalties, treasury) + CLI confirmation.</li>
<li>Gateway serves content; clients issue reads with <code>ReadAck</code> headers (<code>X-TB-Read-Ack</code>).</li>
<li>Gateway writes receipts (<code>read/&lt;epoch&gt;/&lt;seq&gt;.bin</code>), batches hourly via RPC <code>gateway.reads_since</code> &amp; <code>gateway.batch_read_receipts</code>, anchors root through settlement.</li>
</ol>
<h3 id="appendix-g--compute-courier-and-sla-quick-reference"><a class="header" href="#appendix-g--compute-courier-and-sla-quick-reference">Appendix G · Compute Courier and SLA Quick Reference</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Component</th><th>Behaviour</th></tr></thead><tbody>
<tr><td>Courier queue</td><td>sled <code>courier</code> tree storing <code>CourierReceipt</code>. Automatic retries (5 attempts, 100 ms exponential backoff).</td></tr>
<tr><td>SLA scheduler</td><td><code>Settlement::sla</code> vector holds active deadlines; <code>SLA_HISTORY_LIMIT = 256</code>.</td></tr>
<tr><td>CLI hooks</td><td><code>tb-cli compute courier status</code>, <code>tb-cli compute settlement audit</code> (future extension) read <code>compute_market.*</code> RPC responses.</td></tr>
</tbody></table>
</div>
<h3 id="appendix-h--amm--htlc-math-reference"><a class="header" href="#appendix-h--amm--htlc-math-reference">Appendix H · AMM &amp; HTLC Math Reference</a></h3>
<ul>
<li>AMM invariants:
<ul>
<li><code>k = reserve_ct * reserve_it</code>.</li>
<li>Share minting uses geometric mean for first LP; later LPs mint shares proportional to contributions.</li>
<li>No fee term yet; add <code>fee_bps</code> multiplier before recomputing <code>k</code> if needed.</li>
</ul>
</li>
<li>HTLC scripts:
<ul>
<li>Two intents match if <code>hash</code> and <code>amount</code> match. Scripts follow <code>htlc:&lt;hash_hex&gt;:&lt;timeout&gt;</code>.</li>
<li>Ensure <code>timeout_B &gt; timeout_A + safety_margin</code> so refunds propagate in opposite order.</li>
<li>Replay safety: matched intents are removed; duplicates require a new hash.</li>
</ul>
</li>
</ul>
<hr />
<p><strong>Reminder:</strong> always cross‑check these summaries against the referenced source files before modifying behaviour. If a doc section drifts from the code, patch this file first to keep the expectations self‑consistent.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>Everything below reflects what ships in <code>main</code> today. Paths reference the exact modules so engineers can cross-check behaviour while hacking.</p>
<h2 id="ledger-and-consensus"><a class="header" href="#ledger-and-consensus">Ledger and Consensus</a></h2>
<h3 id="block-format-and-state"><a class="header" href="#block-format-and-state">Block Format and State</a></h3>
<ul>
<li><code>node/src/blockchain</code> and <code>node/src/ledger_binary.rs</code> define the canonical block/ledger codecs using <code>codec::profiles</code>. Ledger snapshots embed service-badge flags, governance params, subsidy buckets, and AI-diagnostics toggles so upgrades round-trip without drift.</li>
<li>Macro-block checkpoints (<code>node/src/macro_block.rs</code>) record per-shard state roots and finalize batches of 1-second blocks for light clients and replay harnesses.</li>
<li>Genesis material stays in <code>hash_genesis.rs</code>; the compile-time assertion in <code>node/src/consensus/mod.rs</code> panics if <code>GENESIS_HASH</code> drifts from the serialized baseline.</li>
<li>Blob chain and root assembly live in <code>node/src/blob_chain.rs</code>; roots are scheduled deterministically alongside block production.</li>
</ul>
<h3 id="serialization--codecs"><a class="header" href="#serialization--codecs">Serialization &amp; Codecs</a></h3>
<ul>
<li>Canonical codecs are implemented via the <code>foundation_serialization</code> facade and the <code>codec</code> crate. Binary layouts used by the node, CLI, explorer, and metrics aggregator round-trip under these profiles.</li>
<li>JSON schemas under <code>docs/spec/</code> (for example, <code>dns_record.schema.json</code> and <code>fee_v2.schema.json</code>) document public payloads; cross-language vectors live in tests and fuzz targets (<code>fuzz/rpc</code>, <code>explorer/tests</code>).</li>
<li>Hash layout and binary struct helpers live in <code>node/src/util/binary_struct.rs</code> and <code>node/src/util/binary_codec.rs</code>. Production crates use the serialization facade; <code>serde_json</code> and <code>bincode</code> appear only in tooling.</li>
</ul>
<h3 id="proof-of-work-and-service"><a class="header" href="#proof-of-work-and-service">Proof of Work and Service</a></h3>
<ul>
<li>The hybrid PoW/PoS engine lives under <code>node/src/consensus</code>. <code>pow.rs</code> covers hash-based leaders, <code>pos.rs</code> handles stake selection, and <code>leader.rs</code> coordinates their votes before block assembly.</li>
<li>Service-aware weighting feeds through <code>node/src/service_badge.rs</code>; badge-earned weight modifies scheduler fairness plus governance quorum checks.</li>
<li><code>node/src/exec.rs</code> binds work proofs into block production, ensuring compute/storage receipts attach directly to the coinbase ledger entries.</li>
</ul>
<h3 id="sharding"><a class="header" href="#sharding">Sharding</a></h3>
<ul>
<li>Per-shard state roots are tracked and finalized in macro blocks. Inter-shard coordination, including cross-shard dependencies and reorg handling, lives in <code>node/src/blockchain/inter_shard.rs</code> with tests in <code>node/src/blockchain/tests</code>.</li>
<li>Shard identifiers and layout are defined alongside ledger codecs; helper types are under <code>ledger::address::ShardId</code>.</li>
</ul>
<h3 id="difficulty-and-proof-of-history"><a class="header" href="#difficulty-and-proof-of-history">Difficulty and Proof of History</a></h3>
<ul>
<li><code>node/src/consensus/difficulty*.rs</code> implement Kalman retargeting with clamped deltas. VDF checkpoints feed <code>node/src/poh.rs</code> so propagation remains deterministic even under adversarial timing.</li>
<li>PoH ticks emit telemetry and are replayed by <code>tests/poh.rs</code> plus the Python harness under <code>demo.py</code>.</li>
</ul>
<h3 id="macro-blocks-and-finality"><a class="header" href="#macro-blocks-and-finality">Macro Blocks and Finality</a></h3>
<ul>
<li><code>node/src/consensus/finality.rs</code> collects validator attestations, rotates stakes, and records dispute evidence in sled (<code>state/</code>).</li>
<li>The DKG helper crate <code>dkg/</code> plus <code>node/src/dkg.rs</code> coordinates committee key refresh without exposing transcripts.</li>
</ul>
<h2 id="transaction-and-execution-pipeline"><a class="header" href="#transaction-and-execution-pipeline">Transaction and Execution Pipeline</a></h2>
<h3 id="transaction-lifecycle"><a class="header" href="#transaction-lifecycle">Transaction Lifecycle</a></h3>
<ul>
<li><code>node/src/transaction.rs</code> and <code>node/src/tx</code> encode canonical transaction envelopes shared with CLI/explorer via <code>foundation_serialization</code>. Account abstraction hooks (<code>docs/account_abstraction.md</code> equivalent) now live in <code>node/src/identity/handle_registry.rs</code> and <code>node/src/transaction/fee.rs</code>.</li>
<li>Pipeline: mempool admission → QoS lanes → scheduler → execution → receipts anchored in ledger.</li>
</ul>
<h3 id="fee-lanes-and-rebates"><a class="header" href="#fee-lanes-and-rebates">Fee Lanes and Rebates</a></h3>
<ul>
<li>Fee lanes are typed via <code>node/src/transaction::FeeLane</code> and <code>node/src/fee</code>, with rebate hooks under <code>node/src/fees</code> and <code>node/src/fee/readiness</code>. Governance controls floors through <code>governance/src/params.rs</code> and telemetry tracks enforcement (<code>gateway_fee_floor_*</code> metrics).</li>
<li>Rebates post ledger entries that auto-apply to the submitter before consuming liquid CT. Reference detail lives in <code>docs/economics_and_governance.md#fee-lanes-and-rebates</code>.</li>
</ul>
<h3 id="mempool-admission-and-eviction"><a class="header" href="#mempool-admission-and-eviction">Mempool Admission and Eviction</a></h3>
<ul>
<li>Admission and QoS live under <code>node/src/mempool/admission.rs</code>; scoring and eviction policies are in <code>node/src/mempool/scoring.rs</code>. Tests live in <code>node/src/mempool/tests</code>.</li>
<li>Fee floors and EIP‑1559‑style base fee nudges are applied per block; telemetry exposes <code>mempool_fee_floor_*</code> and target fullness gauges.</li>
</ul>
<h3 id="scheduler-and-parallel-execution"><a class="header" href="#scheduler-and-parallel-execution">Scheduler and Parallel Execution</a></h3>
<ul>
<li><code>node/src/scheduler.rs</code> coordinates lane-aware batches with fairness timeouts. Workloads feed into <code>node/src/parallel.rs</code> so CPU-heavy tasks (GPU hashing, SNARK verification) stay deterministic.</li>
<li>The compute scheduler reuses the same fairness machinery via <code>node/src/compute_market/scheduler</code> and <code>workloads.rs</code>.</li>
</ul>
<h3 id="virtual-machine-and-wasm"><a class="header" href="#virtual-machine-and-wasm">Virtual Machine and WASM</a></h3>
<ul>
<li><code>node/src/vm</code> embeds the bytecode VM, while WASM execution and debugging helpers sit in <code>node/src/vm/debugger.rs</code> plus <code>docs/developer_handbook.md#contract-and-vm-development</code>.</li>
<li>Contracts interact with both UTXO and account space; CLI helpers live in <code>cli/src/wasm.rs</code> and <code>cli/src/contract_dev.rs</code>.</li>
</ul>
<h3 id="account-abstraction-and-identity"><a class="header" href="#account-abstraction-and-identity">Account Abstraction and Identity</a></h3>
<ul>
<li>Distributed handles, DIDs, and registry logic live in <code>node/src/identity</code>. Binary codecs for handles/DIDs ensure explorers, wallets, and RPC share the same storage bytes.</li>
<li>Light clients rely on this identity layer for DID revocation proofs and remote signer flows (<code>node/src/light_client</code>).</li>
</ul>
<h2 id="networking-and-propagation"><a class="header" href="#networking-and-propagation">Networking and Propagation</a></h2>
<h3 id="p2p-handshake"><a class="header" href="#p2p-handshake">P2P Handshake</a></h3>
<ul>
<li><code>node/src/p2p/handshake.rs</code> negotiates capabilities, runtime/transport providers, and telemetry hooks. Peer identity lives in the <code>p2p_overlay</code> crate with in-house and stub adapters.</li>
<li>Capability negotiation exposes compression, service roles, and QUIC certificate fingerprints so gossip and RPC choose the right transport.</li>
</ul>
<h3 id="p2p-wire-protocol"><a class="header" href="#p2p-wire-protocol">P2P Wire Protocol</a></h3>
<ul>
<li>Message framing and compatibility shims live under <code>node/src/p2p/wire_binary.rs</code>. Versioned encodings ensure older/minor peers interoperate; tests assert round-trip and legacy compatibility.</li>
</ul>
<h3 id="quic-transport"><a class="header" href="#quic-transport">QUIC Transport</a></h3>
<ul>
<li>The transport crate (<code>crates/transport</code>) exposes provider traits with backends for Quinn and s2n (feature-gated) plus an in-house stub for tests. Providers advertise capabilities to the handshake layer.</li>
<li>TLS configuration is applied per provider during instance creation (e.g., <code>apply_quinn_tls</code>, <code>apply_s2n_tls</code>), with resets ensuring only one provider’s TLS stack is active at a time.</li>
<li>Callbacks propagate connect/disconnect/handshake statistics into telemetry for dashboards and incident analysis.</li>
</ul>
<h3 id="overlay-and-peer-persistence"><a class="header" href="#overlay-and-peer-persistence">Overlay and Peer Persistence</a></h3>
<ul>
<li>Overlay persistence relies on <code>SimpleDb</code> namespaces (<code>node/src/net/peer.rs</code>, <code>net/overlay_store</code>). Operators migrate peer DBs via <code>scripts/migrate_overlay_store.rs</code> with guidance captured in <code>docs/operations.md#overlay-stores</code>.</li>
<li>Uptime accounting flows through <code>p2p_overlay::uptime</code>; governance reward issuances reuse the same sled-backed snapshots.</li>
</ul>
<h3 id="gossip-relay"><a class="header" href="#gossip-relay">Gossip Relay</a></h3>
<ul>
<li><code>node/src/gossip/relay.rs</code> implements TTL-bound dedup, shard-aware peer sets, and latency + reputation scoring. Fanout metrics live in <code>node/src/telemetry.rs</code> (<code>GOSSIP_*</code> series) and the relay persists shard membership so partitions recover quickly.</li>
<li>Range-boost deliveries and ANN payloads register as gossip hops, keeping mesh telemetry side-by-side with QUIC counts.</li>
</ul>
<h3 id="quic-transport-1"><a class="header" href="#quic-transport-1">QUIC Transport</a></h3>
<ul>
<li>The in-house transport crate (<code>crates/transport</code>) abstracts Quinn and s2n providers. <code>node/src/net/quic.rs</code> publishes diag snapshots through RPC/CLI (<code>tb-cli net quic-stats</code>).</li>
<li>Mutual-TLS materials derive from node keys, are cached, and rotate via governance toggles. Chaos tooling lives in <code>docs/operations.md#chaos-and-fault-drills</code>.</li>
</ul>
<h3 id="localnet-and-range-boost"><a class="header" href="#localnet-and-range-boost">LocalNet and Range Boost</a></h3>
<ul>
<li>Device-to-device mesh lives in <code>node/src/localnet</code> (proximity proofs) and <code>node/src/range_boost</code> (queue, forwarder, telemetry). CLI toggles match env vars <code>TB_MESH_STATIC_PEERS</code> &amp; <code>--range-boost</code>.</li>
<li>Range boost ties into ad-market ANN snapshots: <code>node/src/ad_policy_snapshot.rs</code> persists signed JSON + <code>.sig</code> files for operator audits.</li>
</ul>
<h3 id="network-recovery-and-topologies"><a class="header" href="#network-recovery-and-topologies">Network Recovery and Topologies</a></h3>
<ul>
<li>Partition detection sits in <code>node/src/net/partition_watch.rs</code>; remediation helpers live in <code>docs/operations.md#network-recovery</code> and CLI commands under <code>cli/src/remediation.rs</code>.</li>
<li>A* routing heuristics, swarm presets, and bootstrap flow are summarized from the former <code>docs/net_a_star.md</code>, <code>docs/swarm.md</code>, <code>docs/net_bootstrap.md</code>, and <code>docs/network_topologies.md</code> into this section.</li>
</ul>
<h2 id="storage-and-state"><a class="header" href="#storage-and-state">Storage and State</a></h2>
<h3 id="storage-pipeline"><a class="header" href="#storage-pipeline">Storage Pipeline</a></h3>
<ul>
<li><code>node/src/storage/pipeline.rs</code> handles chunk sizing, erasure coding, encryption/compression selection, and provider placement. <code>coding/</code> supplies the compressor/erasure backends with runtime switches recorded in telemetry.</li>
<li>Manifest handling uses <code>manifest_binary.rs</code> and <code>pipeline/binary</code> for compatibility across CLI/SDK.</li>
</ul>
<h3 id="storage-market"><a class="header" href="#storage-market">Storage Market</a></h3>
<ul>
<li><code>storage_market/</code> unifies sled, RocksDB, and memory via the <code>storage_engine</code> crate and the new policy layer. Rent escrows, provider profiles, and governance overrides for redundancy all sit here.</li>
<li>Proof-of-retrievability, chunk repair, and simulator hooks now share the same store (see <code>node/src/storage/repair.rs</code>).</li>
</ul>
<h3 id="simpledb-and-storage-engines"><a class="header" href="#simpledb-and-storage-engines">SimpleDb and Storage Engines</a></h3>
<ul>
<li><code>node/src/simple_db</code> wraps the <code>storage_engine</code> traits; engines include in-house, RocksDB (feature-gated), and a memory engine for lightweight integration. Runtime selection is governed by <code>EngineConfig</code> and per-name overrides.</li>
<li>Snapshot rewrites atomically replace column families using fsync’d temp files.</li>
<li>The sled store remains in use for dedicated subsystems (for example, governance and explorer stores via the <code>sled/</code> crate), but it is not a SimpleDb backend.</li>
<li>See also <code>state/README.md</code> and <code>docs/operations.md#storage-snapshots-and-wal-management</code> for crash replay and compaction guidance.</li>
</ul>
<h3 id="snapshots-and-state-pruning"><a class="header" href="#snapshots-and-state-pruning">Snapshots and State Pruning</a></h3>
<ul>
<li>WAL + snapshot lifecycle is inside <code>node/src/storage/wal.rs</code>, <code>docs/operations.md#wal-and-snapshots</code>, and CLI commands <code>tb-cli snapshots ...</code>.</li>
<li>State pruning logic lives under <code>node/src/state_pruning.rs</code>; governance knobs guard pruning depth and compaction windows.</li>
</ul>
<h3 id="repair-and-simulation"><a class="header" href="#repair-and-simulation">Repair and Simulation</a></h3>
<ul>
<li><code>node/src/storage/repair</code> + <code>docs/operations.md#storage-repair</code> outline provider scoring, erasure thresholds, and CLI triggers.</li>
<li>Simulation harnesses (<code>docs/simulation_framework.md</code> content) now live here with references to <code>sim/</code> and <code>fuzz/</code> suites.</li>
</ul>
<h3 id="schema-migrations"><a class="header" href="#schema-migrations">Schema Migrations</a></h3>
<ul>
<li>On-disk schema changes are introduced behind version bumps and lossless migrations. Historical notes from <code>docs/schema_migrations/*</code> are consolidated here and inline in code where applicable.</li>
<li>Examples: bridge header persistence (v8), DEX escrow (v9), and industrial subsidies (v10). Migrations run during startup with telemetry for progress and error handling.</li>
</ul>
<h2 id="compute-marketplace"><a class="header" href="#compute-marketplace">Compute Marketplace</a></h2>
<h3 id="offers-and-matching"><a class="header" href="#offers-and-matching">Offers and Matching</a></h3>
<ul>
<li>Computation lives under <code>node/src/compute_market</code>. Offers, bids, and receipts serialize through <code>foundation_serialization</code> and are exposed over RPC (<code>node/src/rpc/compute_market.rs</code>).</li>
<li>Providers stake bonds (<code>compute_market::Offer</code>), schedule workloads, and settle receipts via <code>compute_market::settlement</code>.</li>
</ul>
<h3 id="lane-scheduler"><a class="header" href="#lane-scheduler">Lane Scheduler</a></h3>
<ul>
<li>The matcher rotates fairness windows per lane and is backed by sled state stored under <code>state/market</code>. Lane telemetrics feed <code>match_loop_latency_seconds{lane}</code>.</li>
</ul>
<h3 id="workloads-and-snark-receipts"><a class="header" href="#workloads-and-snark-receipts">Workloads and SNARK Receipts</a></h3>
<ul>
<li>Supported workloads: transcode, inference, GPU hash, SNARK. SNARK proofs now run through <code>node/src/compute_market/snark.rs</code>, which wraps the Groth16 backend, hashes wasm bytes into circuit digests, caches compiled shapes per digest, and chooses CPU/GPU provers (with telemetry exported via <code>snark_prover_latency_seconds{backend}</code> / <code>snark_prover_failure_total{backend}</code>).</li>
<li>Proof bundles carry circuit/output/witness commitments and serialized proof bytes; they are attached to SLA records in <code>compute_market::settlement</code> and surfaced over RPC via <code>compute_market.sla_history</code>.</li>
<li>Explorer ingest mirrors the same payloads: <code>tb-cli explorer sync-proofs --db explorer.db --url http://node:26658</code> streams <code>compute_market.sla_history(limit)</code> responses, persists the serialized <code>Vec&lt;ProofBundle&gt;</code> per job (<code>compute_sla_proofs</code> table), and exposes them under <code>/compute/sla/history</code> so dashboards can render fingerprints/artifacts without talking to the node.</li>
<li>Providers that advertise CUDA/ROCm GPUs (or dedicated accelerators) automatically attempt GPU proving first; failures fall back to CPU while feeding scheduler accelerator telemetry so providers can be reweighted.</li>
<li>Benchmark harnesses for the prover live under <code>node/src/compute_market/tests/prover.rs</code> so operators can compare CPU/GPU latency locally before enabling accelerators.</li>
</ul>
<h3 id="courier-and-replay"><a class="header" href="#courier-and-replay">Courier and Replay</a></h3>
<ul>
<li>Retry/courier logic (<code>node/src/compute_market/courier.rs</code>) persists inflight bundles so restarts resume outstanding work only.</li>
<li><code>docs/compute_market_courier.md</code> content moved here; CLI commands under <code>cli/src/compute.rs</code> manage the queue.</li>
</ul>
<h3 id="compute-backed-money-cbm"><a class="header" href="#compute-backed-money-cbm">Compute-backed Money (CBM)</a></h3>
<ul>
<li>CBM hooks live in <code>node/src/compute_market/cbm.rs</code>. Governance toggles lane payouts, refundable deposits, and SLA slashing (<code>compute_market::settlement::SlaOutcome</code>).</li>
</ul>
<h2 id="energy-market"><a class="header" href="#energy-market">Energy Market</a></h2>
<ul>
<li>Energy credits live in <code>crates/energy-market</code> with the node wrapper in <code>node/src/energy.rs</code>. Providers, credits, and receipts persist in sled via <code>SimpleDb::open_named(names::ENERGY_MARKET, …)</code>; set <code>TB_ENERGY_MARKET_DIR</code> to relocate the DB. The store snapshots to bytes (<code>EnergyMarket::{to_bytes,from_bytes}</code>) on every mutation and uses the same fsync+rename discipline as other <code>SimpleDb</code> consumers so restarts replay identical state.</li>
<li>RPC wiring (<code>node/src/rpc/energy.rs</code>) exposes <code>energy.register_provider</code>, <code>energy.market_state</code>, <code>energy.submit_reading</code>, and <code>energy.settle</code>. The CLI (<code>cli/src/energy.rs</code>) emits the same JSON schema and prints providers, outstanding credits (meter hashes), and settled receipts, so oracle adapters (<code>crates/oracle-adapter</code>) and explorers stay aligned. <code>docs/testnet/ENERGY_QUICKSTART.md</code> covers bootstrap, signature validation, dispute rehearsal, and how to script <code>tb-cli energy</code> calls.</li>
<li>Governance owns <code>energy_min_stake</code>, <code>energy_oracle_timeout_blocks</code>, and <code>energy_slashing_rate_bps</code>. Proposals feed those values through the shared governance crate, latch them in <code>node/src/governance/params.rs</code>, then invoke <code>node::energy::set_governance_params</code>, so runtime hooks refresh the market config plus treasury/slashing math with no recompiles.</li>
<li>Observability: <code>energy_market</code> emits gauges (<code>energy_providers_count</code>, <code>energy_avg_price</code>), counters (<code>energy_kwh_traded_total</code>, <code>energy_settlements_total{provider}</code>), histograms (<code>energy_provider_fulfillment_ms</code>, <code>oracle_reading_latency_seconds</code>), and simple health probes (<code>node::energy::check_energy_market_health</code>). Feed them into the metrics-aggregator dashboards and alert whenever pending meter credits exceed the safe envelope.</li>
</ul>
<h3 id="energy-governance-and-rpc-next-tasks"><a class="header" href="#energy-governance-and-rpc-next-tasks">Energy, Governance, and RPC Next Tasks</a></h3>
<ul>
<li><strong>Governance + Params</strong>
<ul>
<li>Add proposal payloads for energy bundles (batch vs real-time settle) with <code>ParamSpec</code> + runtime hooks.</li>
<li>Wire explorer + CLI timelines so energy param changes and activation/rollback history stay visible.</li>
<li>Expand dependency graph support in proposals (deps validation in the node mirror + conflict tests).</li>
<li>Harden param persistence snapshots and rollback audits with more regression coverage.</li>
</ul>
</li>
<li><strong>Energy + Oracle</strong>
<ul>
<li>Implement real signature verification inside <code>oracle-adapter</code> (replace <code>NoopSignatureVerifier</code>) and ship vectors.</li>
<li>Add oracle quorum/expiry policy (multi-reading attestation) with slashing telemetry and dispute RPCs.</li>
<li>Persist energy receipts to ledger anchors or dedicated sled trees with replay tests.</li>
<li>Expand CLI flows: list providers/receipts, dispute + open cases, and provider updates (price + stake top-up).</li>
</ul>
</li>
<li><strong>RPC + CLI Hardening</strong>
<ul>
<li>Add RPC auth + rate limiting specific to the <code>energy.*</code> endpoints (aligned with gateway policy).</li>
<li>Cover negative cases + structured errors for <code>energy.submit_reading</code> (bad signature, stale timestamp, wrong meter).</li>
<li>Publish JSON schema snippets for energy payloads/oracle messages plus round-trip CLI tests.</li>
</ul>
</li>
<li><strong>Telemetry + Observability</strong>
<ul>
<li>Extend Grafana dashboards: provider count, pending credits, settlement rate, slash totals.</li>
<li>Add SLOs/alerts for oracle latency, slashing spikes, settlement stalls.</li>
<li>Wire metrics-aggregator summary endpoints so <code>/wrappers</code> and <code>/telemetry/summary</code> expose energy stats.</li>
</ul>
</li>
<li><strong>Network + Transport</strong>
<ul>
<li>Run QUIC chaos drills with per-provider failover simulation + fingerprint rotation tests.</li>
<li>Add handshake capability assertions in <code>node/tests</code> for the new transport metadata paths.</li>
</ul>
</li>
<li><strong>Storage + State</strong>
<ul>
<li>Mirror <code>SimpleDb</code> snapshots for energy (<code>TB_ENERGY_MARKET_DIR</code>) with fsync+atomic swap and document restore flow.</li>
<li>Ship migration drill scripts/tests for energy schema evolution (backwards compatibility).</li>
</ul>
</li>
<li><strong>Security + Supply Chain</strong>
<ul>
<li>Enforce release provenance gates for energy/oracle crates (vendor snapshot + checksums in CI).</li>
<li>Tighten oracle adapter secret hygiene (key sourcing, redaction) + boundary fuzz tests for decoding.</li>
</ul>
</li>
<li><strong>Performance + Correctness</strong>
<ul>
<li>Throughput benchmarks for meter ingestion + settlement (per-provider histograms).</li>
<li>Fuzzers for the energy binary codec, RPC param decoding, and governance activation queue.</li>
<li>Deterministic replay in CI for energy receipt reapplication across x86_64/AArch64.</li>
</ul>
</li>
<li><strong>Docs + Explorer</strong>
<ul>
<li>Explorer views: provider table, receipts timeline, fee/slash summaries, plus SQLite schema updates.</li>
<li>Expand <code>docs/testnet/ENERGY_QUICKSTART.md</code> with dispute flows + verifier integration.</li>
</ul>
</li>
<li><strong>CI + Test Suite</strong>
<ul>
<li>Stabilize the full integration suite and gate merges on: governance-param wiring, RPC energy, handshake, rate limiters, ad-market RPC.</li>
<li>Add a “fast mainnet gate” workflow that runs: unit tests + targeted integration (governance, RPC, ledger replay, transport handshake).</li>
</ul>
</li>
</ul>
<h2 id="bridges-dex-and-settlement"><a class="header" href="#bridges-dex-and-settlement">Bridges, DEX, and Settlement</a></h2>
<h3 id="token-bridges"><a class="header" href="#token-bridges">Token Bridges</a></h3>
<ul>
<li>The <code>bridges/</code> crate handles POW header verification, relayer sets, telemetry, and dispute handling. RPC wiring lives in <code>node/src/rpc/bridge.rs</code>.</li>
<li>Verified headers persist in sled (schema migration v8) and CLI commands under <code>cli/src/bridge.rs</code> manage challenge windows.</li>
</ul>
<h3 id="dex-and-trust-lines"><a class="header" href="#dex-and-trust-lines">DEX and Trust Lines</a></h3>
<ul>
<li><code>node/src/dex</code> + <code>dex/</code> supply order books, trust-line routing, escrow constraints, and adapters (Uniswap/Osmosis). Trust-line state is sled-backed and streamed to explorers/CLI.</li>
</ul>
<h3 id="htlc-and-cross-chain"><a class="header" href="#htlc-and-cross-chain">HTLC and Cross-Chain</a></h3>
<ul>
<li>Atomic swap primitives (<code>docs/htlc_swaps.md</code> replacement) were folded into <code>node/src/dex/htlc.rs</code> with RPC + CLI helpers. Governance tracks lane quotas and telemetry under <code>DEX_*</code> metrics.</li>
</ul>
<h2 id="gateway-and-client-access"><a class="header" href="#gateway-and-client-access">Gateway and Client Access</a></h2>
<h3 id="http-gateway"><a class="header" href="#http-gateway">HTTP Gateway</a></h3>
<ul>
<li><code>node/src/gateway/http.rs</code> uses <code>crates/httpd</code> for the router, TLS, and WebSocket upgrades. Gateways serve static content, APIs, and compute relays from the embedded storage pipeline.</li>
<li>CLI + explorer insight commands surfaced from old <code>docs/gateway.md</code> now live in <code>docs/apis_and_tooling.md#gateway</code>.</li>
</ul>
<h3 id="dns-publishing"><a class="header" href="#dns-publishing">DNS Publishing</a></h3>
<ul>
<li>DNS + <code>.block</code> records are handled by <code>node/src/gateway/dns.rs</code> with schemas archived under <code>docs/spec/dns_record.schema.json</code>.</li>
</ul>
<h3 id="dns-auctions-and-staking"><a class="header" href="#dns-auctions-and-staking">DNS Auctions and Staking</a></h3>
<ul>
<li>Gateway domain auctions use stake-backed bids and escrowed CT recorded under <code>node/src/gateway/dns.rs</code> (see <code>StakeEscrowRecord</code>). RPC/CLI support deposit, withdraw, and refund flows with error codes under the same module.</li>
</ul>
<h3 id="mobile-gateway-cache"><a class="header" href="#mobile-gateway-cache">Mobile Gateway Cache</a></h3>
<ul>
<li>Mobile caches persist ChaCha20-Poly1305 encrypted blobs in sled (<code>node/src/gateway/mobile_cache.rs</code>). TTL sweeps and CLI flush commands ensure offline support without stale data.</li>
</ul>
<h3 id="light-clients"><a class="header" href="#light-clients">Light Clients</a></h3>
<ul>
<li><code>node/src/light_client</code> streams headers, DID updates, and proofs. Streaming endpoints live in <code>node/src/rpc/state_stream.rs</code> and CLI commands under <code>cli/src/light_sync.rs</code>.</li>
<li>Mobile updates plus power/bandwidth heuristics from the old <code>docs/mobile_light_client.md</code> live here and in <code>docs/apis_and_tooling.md#light-client-streaming</code>.</li>
</ul>
<h3 id="read-receipts"><a class="header" href="#read-receipts">Read Receipts</a></h3>
<ul>
<li><code>node/src/gateway/read_receipt.rs</code> records signed acknowledgements, batches them for ledger inclusion, and exposes CLI/metrics counters. Economics for <code>READ_SUB_CT</code> live in <code>docs/economics_and_governance.md</code>.</li>
</ul>
<h2 id="telemetry-and-instrumentation"><a class="header" href="#telemetry-and-instrumentation">Telemetry and Instrumentation</a></h2>
<h3 id="runtime-telemetry"><a class="header" href="#runtime-telemetry">Runtime Telemetry</a></h3>
<ul>
<li><code>node/src/telemetry.rs</code> registers every metric (TLS warnings, coding results, gossip fanout, SLA counters). CLI + aggregator share the same registry via <code>runtime::telemetry</code>.</li>
<li>Wrapper telemetry exports runtime/transport/overlay/storage/coding metadata so governance policy violations are visible.</li>
</ul>
<h3 id="metrics-aggregator"><a class="header" href="#metrics-aggregator">Metrics Aggregator</a></h3>
<ul>
<li><code>metrics-aggregator/</code> collects node metrics, correlates them, exposes TLS warning audits, bridge remediation, and governance telemetry. HTTP endpoints live in the same <code>httpd</code> router, and optional S3 uploads reuse <code>foundation_object_store</code>.</li>
</ul>
<h3 id="monitoring-stack"><a class="header" href="#monitoring-stack">Monitoring Stack</a></h3>
<ul>
<li><code>monitoring/</code> provides Grafana dashboards and Prometheus rules. JSON dashboards (e.g., <code>monitoring/compute_market_dashboard.json</code>) are kept in-tree; see <code>docs/operations.md#monitoring</code> for install steps.</li>
</ul>
<h2 id="auxiliary-services"><a class="header" href="#auxiliary-services">Auxiliary Services</a></h2>
<h3 id="service-badges"><a class="header" href="#service-badges">Service Badges</a></h3>
<ul>
<li><code>node/src/service_badge.rs</code> tracks uptime, latency, renewals, and issuance/revocation logic. Governance toggles TTL, uptime thresholds, and telemetry is emitted as <code>BADGE_*</code> counters.</li>
</ul>
<h3 id="ad-marketplace"><a class="header" href="#ad-marketplace">Ad Marketplace</a></h3>
<ul>
<li>Ad market crates (<code>ad_market</code>, <code>node/src/ad_policy_snapshot.rs</code>, <code>node/src/ad_readiness.rs</code>) manage policy snapshots, ANN proofs, conversion tokens, and mesh deliveries. CLI + RPC surfaces sit in <code>cli/src/ad_market.rs</code> and <code>node/src/rpc/ad_market.rs</code>.</li>
</ul>
<h3 id="law-enforcement-portal-and-jurisdiction-packs"><a class="header" href="#law-enforcement-portal-and-jurisdiction-packs">Law-enforcement Portal and Jurisdiction Packs</a></h3>
<ul>
<li>LE logging (<code>node/src/le_portal.rs</code>) records requests, actions, canaries, and evidence logs, with privacy redaction optional. Jurisdiction packs (<code>jurisdiction/</code>, <code>docs/security_and_privacy.md#jurisdiction-packs</code>) scope consent defaults and audit hooks.</li>
</ul>
<h3 id="range-boost-and-localnet-telemetry"><a class="header" href="#range-boost-and-localnet-telemetry">Range-Boost and LocalNet Telemetry</a></h3>
<ul>
<li>Mesh queue depth, hop latency, and fault toggles are exported via <code>node/src/range_boost</code> metrics. Operators manage peers and mesh policies through the CLI + <code>docs/operations.md#range-boost</code>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="economics-and-governance"><a class="header" href="#economics-and-governance">Economics and Governance</a></h1>
<p>Everything settles in CT. Consumer workloads, industrial compute/storage, and governance treasury actions all share the same ledger so explorers/CLI/telemetry never disagree.</p>
<h2 id="ct-supply-and-sub-ledgers"><a class="header" href="#ct-supply-and-sub-ledgers">CT Supply and Sub-Ledgers</a></h2>
<ul>
<li>Coinbases embed <code>STORAGE_SUB_CT</code>, <code>READ_SUB_CT</code>, and <code>COMPUTE_SUB_CT</code> fields (see <code>node/src/blockchain/block_binary.rs</code>). Each bucket mints CT but is accounted separately for policy analysis.</li>
<li>Industrial workload gauges (<code>industrial_backlog</code>, <code>industrial_utilization</code>) flow from storage/compute telemetry into <code>Block::industrial_subsidies()</code>.</li>
<li>Personal rebates are ledger entries only. They auto-apply to the submitter’s own write traffic before dipping into transferable CT and never circulate.</li>
</ul>
<h2 id="energy-market-economics"><a class="header" href="#energy-market-economics">Energy Market Economics</a></h2>
<ul>
<li><strong>Single-token model</strong> — Energy payouts settle in CT just like storage/compute. Credits (<code>EnergyCredit</code>) and receipts (<code>EnergyReceipt</code>) are internal ledger objects stored in <code>SimpleDb::open_named(names::ENERGY_MARKET, …)</code>; settlement burns meter credits, decrements provider capacity, and records <code>EnergyReceipt { buyer, seller, kwh_delivered, price_paid, treasury_fee, slash_applied }</code>.</li>
<li><strong>Treasury integration</strong> — <code>node::energy::settle_energy_delivery</code> forwards <code>treasury_fee + slash_applied</code> to <code>NODE_GOV_STORE.record_treasury_accrual</code>, so explorer/CLI treasury views capture energy fees without extra plumbing. Governance proposals can earmark these accruals like any other treasury inflow.</li>
<li><strong>Governance parameters</strong> — <code>energy_min_stake</code>, <code>energy_oracle_timeout_blocks</code>, and <code>energy_slashing_rate_bps</code> live in the shared <code>governance</code> crate (<code>ParamKey::EnergyMinStake</code>, etc.). Proposals use the same <code>ParamSpec</code> flow as other knobs; once activated, <code>node::energy::set_governance_params</code> updates the runtime config and snapshots the energy sled DB. Outstanding work adds new payloads (batch vs real-time settlement, dependency graph validation) tracked in <code>docs/architecture.md#energy-governance-and-rpc-next-tasks</code>.</li>
<li><strong>Oracle economics</strong> — Meter readings produce <code>EnergyCredit</code> entries keyed by the reading hash (BLAKE3 over provider, meter, readings, timestamp, signature). Credits expire after <code>energy_oracle_timeout_blocks</code>; stale readings cannot be settled and must be re-issued. <code>energy.submit_reading</code> RPC will soon enforce signature validation and multi-reading attestations, with slashing telemetry + dispute RPCs covering bad actors.</li>
<li><strong>CLI/RPC visibility</strong> — <code>tb-cli energy market --verbose</code> and <code>energy.market_state</code> expose provider capacity, price, stake, outstanding credits, and receipts so explorers can mirror the same tables. Upcoming explorer work adds energy provider tables, receipt timelines, and slash summaries (see <code>AGENTS.md</code> tasks).</li>
<li><strong>Dispute flow</strong> — Until dedicated dispute RPCs land, governance proposals (e.g., temporarily raising <code>energy_slashing_rate_bps</code> for a provider, pausing settlement) act as the economic kill switch. Once the dispute endpoints ship they will create ledger anchors referencing disputed meter hashes while preserving CT accounting invariants.</li>
</ul>
<h2 id="multipliers-and-emissions"><a class="header" href="#multipliers-and-emissions">Multipliers and Emissions</a></h2>
<ul>
<li>Per-epoch utilisation <code>U_x</code> feeds the “one dial” multiplier:
[
\text{multiplier}<em>x = \frac{\phi_x I</em>{\text{target}} S / 365}{U_x / \text{epoch_secs}}
]
Adjustments clamp to ±15 % to prevent thrash. Near-zero utilisation doubles the multiplier to keep incentives alive; governance can override via <code>kill_switch_subsidy_reduction</code>.</li>
<li>Miner base reward follows the logistic curve implemented in <code>node/src/consensus/leader.rs</code>:
[
R_0(N) = \frac{R_{\max}}{1+e^{\xi (N-N^\star)}}
]
with hysteresis (ΔN ≈ √N*) that damps flash joins/leaves.</li>
<li>Governance, ledger, CLI, explorer, and metrics aggregator all pull multiplier history through the shared <code>governance</code> crate to avoid drift.</li>
</ul>
<h2 id="fee-lanes-and-rebates-1"><a class="header" href="#fee-lanes-and-rebates-1">Fee Lanes and Rebates</a></h2>
<ul>
<li><code>node/src/fee</code> defines the lane taxonomy (consumer, industrial, priority, treasury). <code>node/src/fees</code> implements QoS eviction and rebate books shared with RPC.</li>
<li>Lane-aware mempool enforcement sits in <code>node/src/mempool</code> (see <code>docs/architecture.md#fee-lanes-and-rebates</code>). Each block nudges the base fee toward a fullness target while telemetry exposes <code>mempool_fee_floor_*</code> gauges.</li>
<li>Rebates are persisted ledger entries exposed via RPC (<code>node/src/rpc/fees.rs</code>) and CLI (<code>cli/src/fee_estimator.rs</code>).</li>
</ul>
<h2 id="service-badges-and-citizenship"><a class="header" href="#service-badges-and-citizenship">Service Badges and Citizenship</a></h2>
<ul>
<li>Operators earn service badges when uptime/latency stay within governance thresholds. <code>node/src/service_badge.rs</code> calculates eligibility; telemetry publishes <code>BADGE_ISSUED_TOTAL</code>, <code>COMPUTE_PROVIDER_UPTIME</code>, etc.</li>
<li>Badges gate governance votes (Operators + Builders houses) and feed range-boost multipliers plus ANN mesh prioritisation.</li>
</ul>
<h2 id="treasury-and-disbursements"><a class="header" href="#treasury-and-disbursements">Treasury and Disbursements</a></h2>
<ul>
<li>Treasury state resides in <code>governance/src/treasury.rs</code> with shared sled persistence. Disbursement DAG validation (quorum, timelocks, rollback windows) is enforced in the <code>governance</code> crate and mirrored by explorer + CLI.</li>
<li>Treasury events emit ledger anchors, aggregator metrics (<code>treasury_balance_*</code>), and CLI history (<code>tb-cli gov treasury</code>).</li>
</ul>
<h2 id="proposal-lifecycle"><a class="header" href="#proposal-lifecycle">Proposal Lifecycle</a></h2>
<ol>
<li>Snapshot of eligible voters occurs on proposal creation (bicameral: Operators + Builders).</li>
<li>Secret ballots + timelocks enforced by <code>governance/src/bicameral.rs</code>.</li>
<li>Parameter changes apply next epoch; upgrades require supermajority plus rollback windows.</li>
<li>Emergency catalog/app-layer overrides auto-expire and must be fully logged.</li>
</ol>
<h2 id="governance-parameters"><a class="header" href="#governance-parameters">Governance Parameters</a></h2>
<ul>
<li><code>governance/src/params.rs</code> exposes typed knobs for fee floors, multipliers, SLA slashing, telemetry sampling, mesh toggles, AI diagnostics, etc.</li>
<li>Every integration (node, CLI, explorer, metrics aggregator) uses the same crate so policy proofs line up with on-chain values.</li>
<li>Historical policy snapshots stream through RPC + CLI; explorers visualise the same baseline.</li>
</ul>
<h2 id="commitreveal-and-pq-hooks"><a class="header" href="#commitreveal-and-pq-hooks">Commit–Reveal and PQ Hooks</a></h2>
<ul>
<li><code>node/src/commit_reveal.rs</code> implements Dilithium-based commits when compiled with <code>pq-crypto</code>, otherwise BLAKE3 commitments. Used for ballots, treasury releases, and challenge proofs.</li>
<li>Governance DAG nodes store both commit and reveal payloads plus telemetry for mismatches.</li>
</ul>
<h2 id="treasury-kill-switch-and-risk-controls"><a class="header" href="#treasury-kill-switch-and-risk-controls">Treasury Kill Switch and Risk Controls</a></h2>
<ul>
<li><code>governance/src/state.rs</code> wires <code>kill_switch_subsidy_reduction</code>, <code>kill_switch_fee_floor</code>, and range-boost toggles to treasury guardians.</li>
<li>Risk mitigations from the former <code>docs/risk_register.md</code>, <code>docs/audit_handbook.md</code>, and <code>docs/system_changes.md</code> live here plus <code>docs/security_and_privacy.md</code>.</li>
</ul>
<h2 id="settlement-and-audit-guarantees"><a class="header" href="#settlement-and-audit-guarantees">Settlement and Audit Guarantees</a></h2>
<ul>
<li><code>tools/settlement_audit</code> and <code>node/tests/settlement_audit.rs</code> reconcile receipts against ledger anchors. Operators must keep <code>cargo test -p the_block --test settlement_audit --release</code> green.</li>
<li>Settlement switch semantics (industrial vs consumer routing) live in <code>node/src/compute_market/settlement</code> and <code>node/src/storage/pipeline</code>. Governance toggles them via params documented here.</li>
</ul>
<h2 id="governance-tooling"><a class="header" href="#governance-tooling">Governance Tooling</a></h2>
<ul>
<li>CLI: <code>cli/src/gov.rs</code> (proposals, DAG inspection, treasury approvals), <code>cli/src/service_badge.rs</code> (badge status), <code>cli/src/telemetry.rs</code> (wrapper metadata).</li>
<li>Explorer + log indexer share the same governance crate via <code>foundation_serialization</code> + <code>foundation_sqlite</code> wrappers.</li>
<li>Metrics aggregator publishes <code>/governance</code>, <code>/treasury</code>, <code>/wrappers</code>, and <code>/bridge</code> dashboards plus webhook outputs (<code>docs/operations.md#metrics-aggregator</code>).</li>
</ul>
<h2 id="ledger-invariants"><a class="header" href="#ledger-invariants">Ledger Invariants</a></h2>
<ul>
<li>Ledger invariants from the former <code>docs/ledger_invariants.md</code> now anchor here: no mint-to-EOA, subsidy buckets sum to the recorded total, governance history is monotonic, badge revocations are fully logged, and macro-block anchors must match the gossip replay harness.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operations"><a class="header" href="#operations">Operations</a></h1>
<p>This guide replaces the scattered <code>docs/operators/**</code>, <code>docs/monitoring*.md</code>, <code>docs/runbook.md</code>, and similar files. Use it when running production nodes, gateways, metrics stacks, or chaos drills.</p>
<h2 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h2>
<ul>
<li>Rust 1.86+, <code>cargo-nextest</code>, <code>cargo-fuzz</code> (nightly), Python 3.12.3 (virtualenv), Node 18+ for dashboards. <code>scripts/bootstrap.sh</code>/<code>.ps1</code> installs everything plus <code>patchelf</code> on Linux.</li>
<li>Storage engines via SimpleDb: in-house (default), RocksDB (feature-gated), and memory (for lightweight builds). Sled is used for dedicated subsystems (for example, governance) via the <code>sled/</code> crate. Provision SSDs and enable <code>storage_engine::KeyValue::flush_wal</code> watchers to keep WAL sizes bounded.</li>
<li>Network: QUIC-ready NICs with low jitter; TLS certificates derive directly from node keys so no external PKI is needed.</li>
</ul>
<h2 id="bootstrap-and-configuration"><a class="header" href="#bootstrap-and-configuration">Bootstrap and Configuration</a></h2>
<ol>
<li>Clone the repo, run <code>scripts/bootstrap.sh</code>, and copy <code>.env.example</code> → <code>.env</code> for node defaults.</li>
<li><code>just lint</code>, <code>just fmt</code>, <code>just test-fast</code> before coding; <code>just test-full</code> mirrors CI.</li>
<li>Node configuration flows through <code>node/.env</code>, CLI flags, or env vars prefixed with <code>TB_*</code> (see <code>node/src/config.rs</code>). <code>config/</code> holds policy baselines.</li>
<li>Use <code>scripts/bootstrap_ps1</code> on Windows/WSL; the runtime works cross-platform and telemetry pipes remain identical.</li>
</ol>
<h2 id="building-and-testing"><a class="header" href="#building-and-testing">Building and Testing</a></h2>
<ul>
<li><code>cargo build -p the_block --release</code> builds the node; <code>cargo build -p cli --bin tb-cli</code> compiles the CLI.</li>
<li><code>cargo nextest run --all-features</code> exercises the multi-crate workspace with the telemetry feature enabled.</li>
<li>Python demo: <code>python demo.py</code> wires the PyO3 module from <code>node/src/py.rs</code> for deterministic replay tests.</li>
</ul>
<h2 id="running-a-node"><a class="header" href="#running-a-node">Running a Node</a></h2>
<ul>
<li><code>tb-cli node start --config config/node.toml</code> (or <code>just node</code>) spawns the daemon with gateway, RPC, gossip, and compute/storage workers enabled.</li>
<li>Gateway hosting toggles: <code>--gateway-http</code>, <code>--range-boost</code>, <code>--mobile-cache</code>. DNS publishing requires the <code>.block</code> registry key configured via <code>TB_GATEWAY_ZONE</code> env vars.</li>
<li>Mesh + overlay migration: run <code>scripts/migrate_overlay_store.rs</code> when upgrading peer stores; reference <code>docs/architecture.md#overlay-and-peer-persistence</code> for background.</li>
</ul>
<h2 id="telemetry-wiring"><a class="header" href="#telemetry-wiring">Telemetry Wiring</a></h2>
<ul>
<li>Enable telemetry via <code>--telemetry</code> or <code>TB_ENABLE_TELEMETRY=1</code>. Metrics flow into the in-process registry (<code>node/src/telemetry.rs</code>) and are exposed on the <code>/metrics</code> HTTP endpoint.</li>
<li>TLS warning sink: export <code>TB_HTTP_TLS</code>/<code>TB_AGGREGATOR_TLS</code> or rely on bundled roots; warnings stream to the aggregator and dashboards.</li>
<li>Wrapper telemetry tracks runtime/transport/storage/coding metadata and enforces governance overrides; CLI: <code>tb-cli telemetry wrappers</code>.</li>
</ul>
<h2 id="metrics-aggregator-ops"><a class="header" href="#metrics-aggregator-ops">Metrics Aggregator Ops</a></h2>
<ul>
<li><code>metrics-aggregator</code> runs as its own binary; configure via env (<code>TB_AGGREGATOR_*</code>). It ingests node metrics, replicates them, archives snapshots (optional S3), and exposes admin endpoints for bridge remediation and TLS warning acknowledgements.</li>
<li>Bridge remediation constants (<code>BRIDGE_REMEDIATION_*</code>) now reference <code>docs/operations.md#bridge-liquidity-remediation</code>; update dashboards accordingly.</li>
<li>Set <code>TB_METRICS_ARCHIVE</code> to append raw JSON into a log for offline audit.</li>
</ul>
<h2 id="monitoring-and-dashboards"><a class="header" href="#monitoring-and-dashboards">Monitoring and Dashboards</a></h2>
<ul>
<li>Grafana/Prometheus configs live under <code>monitoring/</code>. Install with <code>npm ci --prefix monitoring &amp;&amp; make monitor</code> to render dashboards.</li>
<li>Dashboards include compute-market fairness, gossip fanout, gateway pacing, telemetry anomalies, bridge liquidity, SLA enforcement, ANN diagnostics, and badge distributions. JSON is committed (e.g., <code>monitoring/compute_market_dashboard.json</code>).</li>
<li>The dashboard README moved here; use <code>docs/apis_and_tooling.md#metrics-and-telemetry-apis</code> for endpoint paths.</li>
</ul>
<h2 id="energy-market-operations"><a class="header" href="#energy-market-operations">Energy Market Operations</a></h2>
<ul>
<li><strong>Scope</strong> — Everything is first-party: <code>crates/energy-market</code> (providers/credits/receipts + metrics), <code>node/src/energy.rs</code> (sled store + treasury hooks), <code>node/src/rpc/energy.rs</code> (JSON-RPC), <code>cli/src/energy.rs</code> (operator commands), <code>crates/oracle-adapter</code> (ingest client), and <code>services/mock-energy-oracle</code> (World OS drill). No third-party RPC stacks or DBs.</li>
<li><strong>State &amp; backups</strong> — The market uses <code>SimpleDb::open_named(names::ENERGY_MARKET, path)</code> and serializes the entire <code>EnergyMarket</code> struct (providers, credits, receipts) after every mutation. <code>path</code> defaults to <code>energy_market/</code> but can be overridden with <code>TB_ENERGY_MARKET_DIR</code>. Snapshot the directory with the same fsync+rename guarantees as other <code>SimpleDb</code> stores; keep it in your backup/DR rotation alongside consensus/governance sleds.</li>
<li><strong>Bootstrap script</strong> — <code>scripts/deploy-worldos-testnet.sh</code> builds the node with <code>--features worldos-testnet</code>, starts it with <code>--chain worldos-energy --validator</code>, launches the mock oracle (<code>cargo run --release</code> inside <code>services/mock-energy-oracle</code>), and (if <code>docker/telemetry-stack.yml</code> exists) spins up Grafana/Prometheus. This is the canonical energy drill; pair it with <code>docs/testnet/ENERGY_QUICKSTART.md</code> for CLI/RPC steps.</li>
<li><strong>RPC/CLI usage</strong> — Operators interact through <code>tb-cli energy register|market|settle|submit-reading</code>, which sends the same JSON the RPC expects. The endpoints (<code>energy.register_provider</code>, <code>energy.market_state</code>, <code>energy.submit_reading</code>, <code>energy.settle</code>) inherit mutual-TLS, <code>TB_RPC_AUTH_TOKEN</code>, and rate-limit policy from the RPC server. Always log snapshots via <code>tb-cli energy market --verbose &gt; energy_snapshot.json</code> before/after maintenance.</li>
<li><strong>Governance params</strong> — <code>energy_min_stake</code>, <code>energy_oracle_timeout_blocks</code>, and <code>energy_slashing_rate_bps</code> live in the shared governance store (<code>governance/src/params.rs</code>). Proposals update them, runtime hooks call <code>node::energy::set_governance_params</code>, and the sled DB is re-snapshotted. Track activations/rollbacks with <code>tb-cli gov param history</code> or explorer timelines. Upcoming work (batch vs real-time settlement payloads, dependency validation, rollback audits) is tracked in <code>docs/architecture.md#energy-governance-and-rpc-next-tasks</code>.</li>
<li><strong>Telemetry &amp; dashboards</strong> — Metrics include <code>energy_providers_count</code>, <code>energy_avg_price</code>, <code>energy_kwh_traded_total</code>, <code>energy_settlements_total{provider}</code>, <code>energy_provider_fulfillment_ms</code>, and <code>oracle_reading_latency_seconds</code>. Health checks emit logs when pending credits exceed safe thresholds or settlements stall. Update Grafana dashboards to show provider counts, pending credits, slash totals, oracle latency, and settlement rate; alert when latency &gt; SLO, slash spikes, or settlement throughput drops.</li>
<li><strong>Oracle hygiene</strong> — Production adapters must enforce real signature verification (forthcoming in <code>crates/oracle-adapter</code>), source keys from secure env/keystores, redact secrets from logs, and honour RPC rate limits. The mock oracle service (<code>services/mock-energy-oracle</code>, HTTP endpoints <code>/meter/:id/reading</code> and <code>/meter/:id/submit</code>) is for dev/testnet only.</li>
<li><strong>Dispute workflow</strong> — Until dedicated <code>energy.dispute</code>/<code>energy.receipts.list</code> endpoints ship, disputes run through governance: capture the suspect <code>meter_hash</code> + provider ID via <code>energy.market_state</code>, submit a <code>gov param update</code> (tighten slashing rate or pause settlement), and document rollback steps. Keep explorers/CLI in sync so operators can see activation/rollback history.</li>
<li><strong>Snapshot/restore drills</strong> — Practice quiescing the node, copying <code>TB_ENERGY_MARKET_DIR</code>, and restoring it on staging nodes. Mirror the SimpleDb snapshot/restore drills described earlier so operators can rehearse schema migrations or recovery from corruption. Integration tests for backward-compatibility live under <code>node/tests/gov_param_wiring.rs</code>; extend them when modifying the schema.</li>
</ul>
<h2 id="chaos-and-fault-drills"><a class="header" href="#chaos-and-fault-drills">Chaos and Fault Drills</a></h2>
<ul>
<li>Gossip chaos: <code>tests/net_gossip.rs</code> exercises packet loss/jitter; ensure convergence through tie-break rules and inspect <code>partition_watch</code> metrics.</li>
<li>QUIC chaos: <code>node/tests/net_quic.rs</code> captures retransmit counters and handshake distributions; aggregator <code>/chaos</code> endpoints record incidents.</li>
<li>Disk-full and repair: <code>node/tests/storage_repair.rs</code> simulates storage failures; use <code>tb-cli storage repair</code> and monitor <code>STORAGE_*</code> metrics.</li>
<li>Range-boost drills: toggle <code>TB_PARTITION_TAG</code>, adjust mesh peers, and verify recovery with <code>tb-cli mesh status</code>.</li>
</ul>
<h2 id="probe-cli-and-diagnostics"><a class="header" href="#probe-cli-and-diagnostics">Probe CLI and Diagnostics</a></h2>
<ul>
<li><code>crates/probe</code> provides synthetic health checks: <code>probe ping-rpc</code>, <code>probe gossip-check</code>, <code>probe mine-one</code>, <code>probe tip</code>. Flags: <code>--timeout</code>, <code>--expect</code>, <code>--prom</code> for Prometheus output.</li>
<li>Diagnostics harness: <code>tb-cli diagnostics range-boost</code>, <code>tb-cli diagnostics gossip</code>, <code>tb-cli diagnostics tls</code> expose cached stats for on-call triage.</li>
<li>AI diagnostics toggles live in governance params; metrics and CLI output share the same flag.</li>
</ul>
<h2 id="deployment-and-release"><a class="header" href="#deployment-and-release">Deployment and Release</a></h2>
<ul>
<li>Build provenance lives in <code>node/src/provenance.rs</code> and <code>docs/security_and_privacy.md#release-provenance</code>. Release gating requires deterministic hashes and signatures listed in <code>config/release_signers.txt</code> or env overrides.</li>
<li><code>cargo vendor</code> snapshots and <code>provenance.json</code>/<code>checksums.txt</code> block tagging unless the dependency-registry audit passes (<code>just dependency-audit</code>).</li>
<li>Upgrades: follow <code>tb-cli gov release approve</code>, ensure metrics dashboards show <code>release_attestation_*</code>, and leverage the built-in rollback windows.</li>
</ul>
<h2 id="incident-response"><a class="header" href="#incident-response">Incident Response</a></h2>
<ul>
<li>Runbook coverage: bridge liquidity remediation, DHT recovery, gateway flush, snapshot repair. Each subsection lives below for quick linking.</li>
<li><strong>Bridge Liquidity Remediation</strong> – aggregator dispatch endpoints <code>/remediation/bridge/*</code> plus dashboards (<strong>Bridge Remediation</strong> row) keep quorum on pending actions. Operators must acknowledge via CLI + aggregator ack endpoints.</li>
<li><strong>DHT / Gossip Recovery</strong> – purge peer DBs (<code>simple_db::names::OVERLAY</code>), reseed via bootstrap peers, run <code>provision_overlay_store</code> helper, monitor <code>partition_watch</code> metrics.</li>
<li><strong>Gateway Flush</strong> – use <code>tb-cli gateway mobile-cache flush</code> and <code>tb-cli read-acks export</code> before restarts.</li>
</ul>
<h2 id="storage-snapshots-and-wal-management"><a class="header" href="#storage-snapshots-and-wal-management">Storage, Snapshots, and WAL Management</a></h2>
<ul>
<li>Snapshots: <code>tb-cli snapshots create --path &lt;dir&gt;</code> writes fsync’d temp files before atomic rename. Legacy dumps stay until the new snapshot lands.</li>
<li>WAL hygiene: <code>SimpleDb::flush_wal</code> runs before snapshots; set <code>TB_SIMPLE_DB_LIMIT_BYTES</code> to guard disk usage.</li>
<li>Repair: <code>tb-cli storage repair --manifest &lt;file&gt;</code> reissues pulls, rebuilds Lagrange-coded shards, and flags under-replicated providers.</li>
</ul>
<h2 id="backup-and-restore-path-reference"><a class="header" href="#backup-and-restore-path-reference">Backup and Restore Path Reference</a></h2>
<p>The directories below map directly onto the SimpleDb column families listed in <code>docs/system_reference.md#appendix-c--simpledb-column-family-and-prefix-map</code>, so operators can tie on-disk artifacts back to the logical subsystems referenced throughout the system reference.</p>
<div class="table-wrapper"><table><thead><tr><th>Subsystem</th><th>Default path</th><th>Env/flag</th><th>Notes</th></tr></thead><tbody>
<tr><td>Overlay peer store</td><td><code>~/.the_block/overlay/overlay_peers.json</code></td><td><code>TB_OVERLAY_DB_PATH</code></td><td>JSON list of peers + last-seen timestamps (<code>p2p_overlay</code>).</td></tr>
<tr><td>Gossip/QUIC peer caches</td><td><code>~/.the_block/peer_db</code> / <code>peer_db_quic</code></td><td><code>TB_PEER_DB_PATH</code>, <code>TB_QUIC_PEER_DB_PATH</code>, <code>TB_PEER_KEY_HISTORY_PATH</code>, <code>TB_CHUNK_DB_PATH</code></td><td>Keys, reputation history, and chunk dedup stores from <code>node/src/net/peer.rs</code>.</td></tr>
<tr><td>DNS auctions</td><td><code>dns_db</code></td><td><code>TB_DNS_DB_PATH</code></td><td>SimpleDb backing auctions, stakes, and ownership.</td></tr>
<tr><td>Gateway read receipts</td><td><code>gateway_receipts</code></td><td><code>TB_GATEWAY_RECEIPTS</code></td><td>Hourly CBOR batches + Merkle roots; archive before purging.</td></tr>
<tr><td>Mobile cache</td><td><code>mobile_cache.db</code></td><td><code>TB_MOBILE_CACHE_DB</code>, <code>TB_MOBILE_CACHE_KEY_HEX</code></td><td>ChaCha20-Poly1305 encrypted sled.</td></tr>
<tr><td>Storage pipeline</td><td><code>blobstore/</code></td><td><code>TB_STORAGE_PIPELINE_DIR</code></td><td>Holds manifests, rent-escrow records, and provider overrides.</td></tr>
<tr><td>Storage market contracts</td><td><code>storage_market/</code></td><td><code>TB_STORAGE_MARKET_DIR</code></td><td>Sled tree (<code>market/contracts</code>) plus importer checkpoints.</td></tr>
<tr><td>Compute scheduler</td><td><code>~/.the_block/compute/{pending,cancel,reputation}</code></td><td><code>TB_PENDING_PATH</code>, <code>TB_CANCEL_PATH</code>, <code>TB_REPUTATION_DB_PATH</code></td><td>Pending job queue, cancellation log, and reputation DB.</td></tr>
<tr><td>Bridge sled</td><td><code>bridge_db/</code></td><td><code>TB_BRIDGE_DB_PATH</code>, <code>TB_BRIDGE_SLED_PATH</code></td><td>Persisted headers, withdrawals, and duty logs.</td></tr>
<tr><td>Light-client proofs</td><td><code>proof_tracker</code></td><td><code>TB_PROOF_TRACKER_PATH</code> (implied)</td><td>Path is displayed by <code>tb-cli light-client rebate-status</code>; back it up with explorer data.</td></tr>
<tr><td>LE portal</td><td><code>./le_portal</code></td><td>CLI <code>--base</code></td><td><code>le_requests.log</code>, <code>le_actions.log</code>, <code>le_evidence.log</code>, <code>warrant_canary.log</code>, plus <code>evidence/</code> blob files.</td></tr>
</tbody></table>
</div>
<p>Backups should snapshot these directories before upgrades. Restores require stopping the node, restoring the directory, and restarting with the same <code>TB_*</code> overrides to avoid partial migrations.</p>
<h2 id="network-recovery-and-chaos"><a class="header" href="#network-recovery-and-chaos">Network Recovery and Chaos</a></h2>
<ul>
<li>Chaos harness: <code>docs/architecture.md#telemetry-and-instrumentation</code> + <code>monitoring/grafana/...</code> capture WAN-scale drills. Use <code>tests/net_gossip.rs</code> fixtures with injected loss/latency before rolling changes.</li>
<li>Partition drills: toggle <code>TB_PARTITION_TAG</code>, observe <code>partition_watch</code> alerts, ensure quorum recovers, document remediation in telemetry dashboards.</li>
<li>QUIC chaos: <code>node/tests/net_quic.rs</code> and aggregator <code>/chaos</code> endpoints record retransmit counters and handshake distributions.</li>
</ul>
<h2 id="range-boost-and-localnet-operations"><a class="header" href="#range-boost-and-localnet-operations">Range Boost and LocalNet Operations</a></h2>
<ul>
<li>Enable with <code>--range-boost</code> or <code>TB_RANGE_BOOST=1</code>. Peers are configured via <code>TB_MESH_STATIC_PEERS</code> (comma-separated <code>host:port</code> list). Diagnostics: <code>tb-cli mesh status</code>, <code>tb-cli diagnostics range-boost</code>, metrics <code>RANGE_BOOST_*</code>.</li>
<li>Forwarder control: set <code>TB_RANGE_BOOST=0</code> (or remove <code>--range-boost</code>) then restart to pause deliveries while keeping the queue on disk. <code>node/src/range_boost/mod.rs::set_enabled(false)</code> drains the forwarder gracefully; re-enable to resume.</li>
<li>Queue handling:
<ol>
<li>Before reseeding peers, disable range boost and wait for <code>range_boost_queue_depth</code> to hit zero.</li>
<li>Adjust <code>TB_MESH_STATIC_PEERS</code> / mesh discovery, then re-enable and confirm <code>range_boost_forwarder_fail_total</code> stays flat.</li>
<li>For forced drains, delete the persisted queue directory under <code>~/.the_block/range_boost</code> (only after confirming backups) and restart.</li>
</ol>
</li>
<li>Chaos drills can simulate failures with <code>FaultMode::{ForceDisabled,ForceNoPeers,ForceEncode,ForceIo}</code> (see <code>node/src/range_boost/mod.rs</code>). Build-time toggles wire these modes into diagnostic RPCs; use them to validate monitoring before production changes.</li>
</ul>
<h2 id="simulation-and-replay"><a class="header" href="#simulation-and-replay">Simulation and Replay</a></h2>
<ul>
<li>Use <code>sim/</code>, <code>examples/</code>, and <code>tests/</code> harnesses to rehearse dependency swaps, storage migrations, and governance policy changes. Replay harnesses guarantee byte-identical results across CPU architectures.</li>
<li>Chaos + replay logs feed both the aggregator and <code>docs/developer_handbook.md#simulation-and-chaos</code> for developer workflows.</li>
</ul>
<h2 id="operator-checklist"><a class="header" href="#operator-checklist">Operator Checklist</a></h2>
<ul>
<li>Keep <code>scripts/pre-commit.sample</code> installed to enforce fmt/lint.</li>
<li>Regenerate dependency inventories whenever <code>Cargo.lock</code> changes.</li>
<li>Run settlement audits and badge/SLA telemetry before and after upgrades.</li>
<li>Document incidents in the aggregator’s <code>/audit</code> endpoints and link to the relevant sections above for forensics.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="security-and-privacy"><a class="header" href="#security-and-privacy">Security and Privacy</a></h1>
<p>Security is enforced in code, not promises. This guide consolidates the former threat-model, bridge-security, privacy, and supply-chain docs.</p>
<h2 id="threat-model"><a class="header" href="#threat-model">Threat Model</a></h2>
<ul>
<li>Adversaries include malicious operators, compromised gateways, colluding relayers, and jurisdiction-specific takedown requests.</li>
<li>Consensus hardens liveness with hybrid PoW/PoS plus macro-block checkpoints; even if gossip partitions, PoH + VDF tie the timeline together.</li>
<li>Gossip/range-boost nets track <code>partition_watch</code> metrics so partitions trigger incident playbooks before operators lose quorum.</li>
<li>Storage/compute markets slash via <code>compute_market::settlement::SlaOutcome</code> and provider loss metrics in <code>node/src/storage/pipeline.rs</code>.</li>
</ul>
<h2 id="cryptography-stack"><a class="header" href="#cryptography-stack">Cryptography Stack</a></h2>
<ul>
<li><code>crypto_suite</code> and <code>crates/crypto</code> expose BLAKE3, Ed25519, Dilithium, Kyber, etc. All consensus + wallet code compiles with <code>#![forbid(unsafe_code)]</code>.</li>
<li>Commit–reveal and DKG flows rely on PQ-ready fallbacks. <code>node/src/commit_reveal.rs</code> switches between Dilithium and BLAKE3; <code>dkg/</code> handles committee keys; <code>zkp/</code> contains SNARK verification code.</li>
<li>Mathematical proofs remain under <code>docs/maths/</code> (LaTeX + PDF) and are referenced from CI + auditors.</li>
</ul>
<h2 id="remote-signers-and-key-management"><a class="header" href="#remote-signers-and-key-management">Remote Signers and Key Management</a></h2>
<ul>
<li>Remote signer workflows live in <code>node/src/remote_signer_security.rs</code>, <code>cli/src/wallet.rs</code>, and <code>wallet/</code> crates. CLI enforces multisig, escrow-hash selection, and remote telemetry.</li>
<li>Release provenance (<code>node/src/provenance.rs</code>) verifies binary hashes against signed allow lists; attested binaries roll back automatically if hashes drift.</li>
<li>Environment variables <code>TB_RELEASE_SIGNERS</code>, <code>TB_RELEASE_SIGNERS_FILE</code> override defaults for air-gapped deployments.</li>
</ul>
<h2 id="energy-oracle-safety"><a class="header" href="#energy-oracle-safety">Energy Oracle Safety</a></h2>
<ul>
<li><strong>Key sourcing</strong> — Oracle adapters (<code>crates/oracle-adapter</code>) must draw signing keys from hardened storage (<code>TB_ORACLE_KEY_HEX</code>, hardware modules, or governance-approved secret stores). Never embed keys in code or logs. The forthcoming Ed25519 verifier replaces <code>NoopSignatureVerifier</code>; once landed, every meter reading must carry a valid signature over <code>MeterReadingPayload::signing_bytes()</code>.</li>
<li><strong>Transport &amp; auth</strong> — Oracle adapters send readings through the same HTTP/TLS stack as all other tooling (first-party <code>httpd::Client</code>). Configure mutual-TLS or RPC auth tokens (<code>TB_RPC_AUTH_TOKEN</code>) before enabling public ingestion. Rate limiting (<code>node/src/rpc/mod.rs::check_rate_limit</code>) applies to <code>energy.*</code> endpoints, so adapters should honour <code>429</code> responses and retry with jitter.</li>
<li><strong>Telemetry redaction</strong> — Meter readings flow through <code>node/src/rpc/energy.rs</code>. Logs must omit raw signatures and meter values unless <code>RUST_LOG=trace</code> is explicitly set. Oracle adapters should scrub meter IDs and signatures before logging; dashboards rely on aggregate metrics (<code>energy_kwh_traded_total</code>, <code>oracle_reading_latency_seconds</code>) instead of raw payloads.</li>
<li><strong>Dispute hooks</strong> — Until dedicated dispute RPCs ship, governance proposals (e.g., raising <code>energy_slashing_rate_bps</code>, pausing settlement) are the primary kill switch. Record suspect <code>meter_hash</code> values via <code>tb-cli energy market --verbose</code>, attach them to proposals, and document rollback steps. Once the dispute RPC/CLI pair lands they will emit ledger anchors referencing the disputed readings plus telemetry counters for slash totals.</li>
<li><strong>Mock oracle isolation</strong> — <code>services/mock-energy-oracle</code> is a dev/testnet binary only. It uses mock signatures (provider_id||kwh) and intentionally relaxed auth. Never expose it to production networks; wrap it in loopback-only listeners when exercising <code>scripts/deploy-worldos-testnet.sh</code>.</li>
<li><strong>Release &amp; supply chain</strong> — Energy/oracle crates fall under the same release-provenance gates as the rest of the workspace: <code>cargo vendor</code> snapshots, <code>provenance.json</code> hashes, signed tags, and dependency audits must pass before shipping binaries that include <code>crates/energy-market</code> or <code>crates/oracle-adapter</code>. Secrets must be injected at runtime (env or KMS), not bundled into release artifacts.</li>
</ul>
<h2 id="privacy-layers"><a class="header" href="#privacy-layers">Privacy Layers</a></h2>
<ul>
<li>Reads stay free by logging signed <code>ReadAck</code> receipts, not payloads. Operators can redact metadata via the privacy crate (<code>privacy/</code>) when the <code>privacy</code> feature is enabled.</li>
<li>Read-ack privacy modes (<code>node/src/config.rs::ReadAckPrivacyMode</code> + <code>node/src/blockchain/privacy.rs</code>):
<ul>
<li><code>Enforce</code> (default) — every receipt must include a privacy proof; failures raise <code>ReadAckError::PrivacyProofRejected</code>.</li>
<li><code>Observe</code> — proofs are checked but failures are logged (<code>read_ack_privacy_verification_failed</code>) instead of rejected so operators can collect samples without losing revenue.</li>
<li><code>Disabled</code> — privacy proof checks are skipped (only use during incident response). RPC <code>node.get_ack_privacy</code>/<code>node.set_ack_privacy</code> change modes live; CLI wrappers should restore <code>enforce</code> after drills.</li>
</ul>
</li>
<li>Law-enforcement portal (<code>node/src/le_portal.rs</code>) writes hashed case IDs and action logs; optional ChaCha20-Poly1305 evidence buckets live under <code>&lt;base&gt;/evidence/</code>.</li>
<li>Range-boost mesh encrypts payloads, tracks hop proofs, and never exposes raw content to intermediate peers.</li>
</ul>
<h2 id="kyc-jurisdiction-and-compliance"><a class="header" href="#kyc-jurisdiction-and-compliance">KYC, Jurisdiction, and Compliance</a></h2>
<ul>
<li>
<p>KYC provider flows live in <code>node/src/kyc.rs</code> plus the <code>jurisdiction/</code> crate. Policy packs encode consent defaults, languages, and feature toggles per region.</p>
</li>
<li>
<p>Pack schema (<code>crates/jurisdiction/src/lib.rs</code>):</p>
<pre><code class="language-json">{
  "region": "US",
  "consent_required": true,
  "features": ["wallet","dex"],
  "parent": "NA"
}
</code></pre>
<p>Packs may inherit from a <code>parent</code> region; <code>PolicyPack::resolve()</code> flattens the tree so downstream services operate on the effective settings.</p>
</li>
<li>
<p>Signed packs embed the pack JSON plus a 64-byte Ed25519 signature. <code>SignedPack::verify(vk)</code> enforces authenticity; feeds fetched via <code>fetch_signed(url, pk)</code> honour TLS settings (<code>TB_JURISDICTION_TLS</code>, <code>TB_HTTP_TLS</code>). CLI <code>jurisdiction.set</code> swaps packs, while <code>jurisdiction.policy_diff</code> compares two packs and highlights consent/feature changes.</p>
</li>
<li>
<p>Governance proposals log pack hashes so explorers and dashboards can prove which policy applied at any height. Forked jurisdictions publish separate feeds and set <code>jurisdiction_region</code> accordingly.</p>
</li>
<li>
<p><code>docs/jurisdiction_authoring.md</code> content is folded here: versioned packs, governance-voted updates, optional forks for conflicting jurisdictions.</p>
</li>
<li>
<p>Non-custodial core: ramps handle KYC/AML; the node never holds user secrets.</p>
</li>
</ul>
<h2 id="law-enforcement-portal-and-warrant-canary"><a class="header" href="#law-enforcement-portal-and-warrant-canary">Law-Enforcement Portal and Warrant Canary</a></h2>
<ul>
<li>API surface (<code>node/src/le_portal.rs</code>):
<ul>
<li><code>LeRequest { timestamp, agency, case_hash, jurisdiction, language }</code> and <code>LeAction { action_hash, … }</code> are serialized to JSON and appended to <code>&lt;base&gt;/le_requests.log</code> and <code>&lt;base&gt;/le_actions.log</code>. CLI commands (<code>tb-cli le request|action</code>) accept <code>--base &lt;dir&gt;</code>; default base is <code>./le_portal</code>.</li>
<li>Evidence uploads write raw bytes to <code>&lt;base&gt;/evidence/&lt;hash&gt;</code> and log <code>EvidenceRecord</code> JSON lines in <code>le_evidence.log</code>. Payloads are hashed via BLAKE3 before persistence for tamper detection.</li>
<li>Warrant canary entries append <code>&lt;timestamp&gt; &lt;hash&gt;</code> to <code>warrant_canary.log</code>. Operators publish signed statements out-of-band; if authorities compel silence the canary stops updating.</li>
</ul>
</li>
<li>Sanitisation hooks: when the optional <code>privacy</code> feature is enabled, <code>sanitize_payload</code> rejects memos outside the “local” jurisdiction before writing logs, and the audit sled mirrors every entry for later review.</li>
</ul>
<h2 id="risk-register-and-incident-logging"><a class="header" href="#risk-register-and-incident-logging">Risk Register and Incident Logging</a></h2>
<ul>
<li>Former <code>docs/risk_register.md</code> entries are now structured as:
<ul>
<li><strong>Consensus</strong> – watch for leader splits, PoH stalls, DKG transcript leaks.</li>
<li><strong>Networking</strong> – QUIC/TLS misconfigs, peer DB corruption, overlay exhaustion.</li>
<li><strong>Storage/Compute</strong> – erasure thresholds, SLA slashing, escrow exhaustion.</li>
<li><strong>Governance</strong> – treasury drains, kill-switch toggles, badge forgeries.
Log incidents via the metrics aggregator <code>/audit</code> endpoint and cross-link to this section.</li>
</ul>
</li>
</ul>
<h2 id="bridge-and-cross-chain-security"><a class="header" href="#bridge-and-cross-chain-security">Bridge and Cross-Chain Security</a></h2>
<ul>
<li><code>bridges/</code> telemetry counters (<code>bridge_*</code>) highlight proof verification failures, disputes, liquidity changes, and slash events. Aggregator dashboards keep per-asset panels.</li>
<li>Reward approval workflows require multisig attestations; CLI + explorer use the same code paths to prevent phantom unlocks.</li>
<li>HTLC proofs and trust-line routing reuse the same ledger invariants so locked liquidity can’t leak.</li>
</ul>
<h2 id="release-provenance-and-supply-chain"><a class="header" href="#release-provenance-and-supply-chain">Release Provenance and Supply Chain</a></h2>
<ul>
<li>Release provenance is enforced by <code>node/src/provenance.rs</code>, <code>config/release_signers.txt</code>, and the CI job that verifies <code>provenance.json</code> + <code>checksums.txt</code>.</li>
<li>Dependency independence: first-party wrappers (<code>foundation_*</code> crates) replace third-party TLS/HTTP/serialization stacks. <code>docs/developer_handbook.md#dependency-policy</code> covers required tooling and audits.</li>
<li>Reproducible builds: <code>docs/repro.md</code> + <code>docs/reproducible_builds.md</code> were merged here. Build IDs must match <code>env!("BUILD_BIN_HASH")</code> or binaries are rejected on startup.</li>
</ul>
<h2 id="data-retention-and-privacy-compliance"><a class="header" href="#data-retention-and-privacy-compliance">Data Retention and Privacy Compliance</a></h2>
<ul>
<li>Privacy compliance from the old docs now lives here: reads store signatures only, storage manifests encrypt content keys, telemetry scrubs PII and includes sampling controls (<code>node/src/telemetry.rs</code>).</li>
<li>Gateway caches encrypt at rest; even mobile caches derive keys from <code>TB_MOBILE_CACHE_KEY_HEX</code>/<code>TB_NODE_KEY_HEX</code> to avoid plaintext recoveries.</li>
<li>Jurisdiction packs dictate retention timers. Governance votes log pack hashes so explorers/CLI can prove which policy was active for any block.</li>
</ul>
<h2 id="auditing-and-tooling"><a class="header" href="#auditing-and-tooling">Auditing and Tooling</a></h2>
<ul>
<li>Settlement audits (<code>tools/settlement_audit</code>), dependency audits (<code>just dependency-audit</code>), and TLS warning snapshots all land in the aggregator for historical replay.</li>
<li>Probe CLI can emit Prometheus metrics for latency SLAs; dashboards include authn/authz traces for RPC + gateway endpoints.</li>
<li>Formal proofs, fuzz coverage, and chaos traces (bridge/compute/gossip) are expected before every release; see <code>docs/developer_handbook.md#formal-methods</code>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="developer-handbook"><a class="header" href="#developer-handbook">Developer Handbook</a></h1>
<p>Every change assumes main-net readiness. Treat this as the working agreement for engineers and AI agents.</p>
<h2 id="environment-setup"><a class="header" href="#environment-setup">Environment Setup</a></h2>
<ul>
<li>Run <code>scripts/bootstrap.sh</code> (Linux/macOS) or <code>scripts/bootstrap.ps1</code> (Windows/WSL). The script installs Rust 1.86+, <code>cargo-nextest</code>, <code>cargo-fuzz</code>, Node 18+, Python 3.12.3 venv, and OS packages (<code>patchelf</code>, <code>llvm-tools-preview</code>).</li>
<li>Set <code>PATH=.venv/bin:$PATH</code> to pick up the Python shim, and ensure <code>rustup show</code> lists the workspace toolchain.</li>
<li>Optional: install <code>just</code>, <code>nix</code>, or <code>direnv</code> if you rely on those flows; the repo ships configs for each.</li>
</ul>
<h2 id="workspace-layout"><a class="header" href="#workspace-layout">Workspace Layout</a></h2>
<ul>
<li><code>node/</code> – full node, gateway, RPC, compute/storage stacks, plus Python bindings.</li>
<li><code>crates/</code> – first-party libraries (<code>foundation_*</code>, <code>transport</code>, <code>httpd</code>, <code>storage_engine</code>, <code>p2p_overlay</code>, <code>wallet</code>, <code>probe</code>, etc.).</li>
<li><code>cli/</code> – user-facing CLI with governance, wallet, bridge, compute, telemetry, and remediation commands.</li>
<li><code>metrics-aggregator/</code>, <code>monitoring/</code>, <code>explorer/</code> – ops tooling.</li>
<li><code>bridges/</code>, <code>dex/</code>, <code>storage_market/</code>, <code>gateway/</code> – specialised crates referenced by the node.</li>
<li><code>docs/</code> – this handbook (mdBook). Run <code>mdbook build docs</code> before submitting docs changes.</li>
</ul>
<h2 id="toolchain-and-commands"><a class="header" href="#toolchain-and-commands">Toolchain and Commands</a></h2>
<ul>
<li><code>just lint</code> → <code>cargo clippy --workspace --all-targets --all-features</code>.</li>
<li><code>just fmt</code> → <code>cargo fmt --all</code>.</li>
<li><code>just test-fast</code> → targeted unit tests; <code>just test-full</code> → <code>cargo test --workspace --features telemetry</code>.</li>
<li><code>make monitor</code>, <code>make aggregator</code>, and <code>make cli</code> wrap common workflows.</li>
<li>Use <code>cargo nextest</code> for high-parallel test runs; CI uses the same harness.</li>
</ul>
<h2 id="coding-standards"><a class="header" href="#coding-standards">Coding Standards</a></h2>
<ul>
<li><code>#![forbid(unsafe_code)]</code> across the workspace. If you think you need <code>unsafe</code>, stop and open an issue.</li>
<li>Prefer first-party crates (<code>httpd</code>, <code>foundation_tls</code>, <code>foundation_serialization</code>, <code>foundation_sqlite</code>, <code>storage_engine</code>) over upstream dependencies.</li>
<li>Use <code>concurrency::{MutexExt, DashMap}</code> instead of raw locks to keep poisoning + metrics consistent.</li>
<li>Keep modules small and feature-gated; RPC code should stay in <code>node/src/rpc</code>, CLI code in <code>cli/src</code>, etc.</li>
</ul>
<h2 id="testing-strategy"><a class="header" href="#testing-strategy">Testing Strategy</a></h2>
<ul>
<li>Unit tests live next to code; integration tests under <code>node/tests</code>, <code>gateway/tests</code>, <code>bridges/tests</code>, etc.</li>
<li>Replay harness: <code>cargo test -p the_block --test replay</code> replays ledger snapshots across architectures.</li>
<li>Settlement audit: <code>cargo test -p the_block --test settlement_audit --release</code> must pass before merging.</li>
<li>Fuzzing: <code>scripts/fuzz_coverage.sh</code> installs LLVM tools, runs fuzz targets (e.g., <code>cargo fuzz run storage</code>), and uploads <code>.profraw</code> artifacts. Remember to set <code>LLVM_PROFILE_FILE</code>.</li>
<li>Chaos: <code>tests/net_gossip.rs</code>, <code>tests/net_quic.rs</code>, <code>node/tests/storage_repair.rs</code>, <code>node/tests/gateway_rate_limit.rs</code> simulate packet loss, disk-full, etc.</li>
</ul>
<h2 id="debugging-and-diagnostics"><a class="header" href="#debugging-and-diagnostics">Debugging and Diagnostics</a></h2>
<ul>
<li>Enable <code>RUST_LOG=trace</code> plus the diagnostics subscriber when chasing runtime issues; <code>diagnostics::tracing</code> is wired everywhere.</li>
<li><code>cli/src/debug_cli.rs</code> and <code>tb-cli diagnostics …</code> provide structured dumps for mempool, scheduler, gossip, mesh, TLS, and telemetry state.</li>
<li>Use <code>docs/operations.md#probe-cli-and-diagnostics</code> for probe commands.</li>
</ul>
<h2 id="performance-and-benchmarks"><a class="header" href="#performance-and-benchmarks">Performance and Benchmarks</a></h2>
<ul>
<li>Bench harnesses sit under <code>benches/</code>, <code>monitoring/build</code>, and <code>node/benches</code>. Publish results through the metrics exporter by setting <code>TB_BENCH_PROM_PATH</code>.</li>
<li><code>docs/benchmarks.md</code> content moved here: store thresholds in <code>config/benchmarks/&lt;name&gt;.thresholds</code>, compare to <code>monitoring/metrics.json</code>, and watch Grafana’s <strong>Benchmarks</strong> row.</li>
</ul>
<h2 id="contract-and-vm-development"><a class="header" href="#contract-and-vm-development">Contract and VM Development</a></h2>
<ul>
<li>
<p>WASM tooling: <code>cli/src/wasm.rs</code>, <code>node/src/vm</code>, <code>node/src/vm/debugger.rs</code>. Use <code>docs/architecture.md#virtual-machine-and-wasm</code> for runtime behaviour.</p>
</li>
<li>
<p><code>docs/contract_dev.md</code>, <code>docs/wasm_contracts.md</code>, and <code>docs/vm_debugging.md</code> merged here.</p>
</li>
<li>
<p>CLI flow: <code>tb-cli wasm build</code>, <code>tb-cli contract deploy</code>, <code>tb-cli contract call</code>, <code>tb-cli vm trace</code>.</p>
</li>
<li>
<p>Gas model (<code>node/src/vm/gas.rs</code>):</p>
<ul>
<li>Each opcode has a base cost (<code>cost(op)</code>), and storage/hash-heavy ops add explicit constants (<code>GAS_STORAGE_READ</code>, <code>GAS_STORAGE_WRITE</code>, <code>GAS_HASH</code>).</li>
<li><code>GasMeter</code> enforces limits and reports <code>used()</code> for fee accounting. ABI helpers (<code>node/src/vm/abi.rs</code>) encode <code>(gas_limit, gas_price)</code> as a 16-byte blob when interacting with wallets.</li>
</ul>
</li>
<li>
<p>Debugger + traces: <code>node/src/vm/debugger.rs</code> steps through opcodes, exposes <code>VmDebugger::into_trace()</code> (stack, gas, opcode, pc). CLI <code>tb-cli vm trace --tx &lt;hash&gt; --json</code> prints entries like:</p>
<pre><code class="language-json">{"pc":12,"opcode":"SSTORE","gas_before":1200,"gas_after":696,"stack":[1,2,3]}
</code></pre>
<p>Use it alongside <code>tb-cli contract disasm</code> when diagnosing mispriced contracts.</p>
</li>
</ul>
<h2 id="python--headless-tooling"><a class="header" href="#python--headless-tooling">Python + Headless Tooling</a></h2>
<ul>
<li><code>demo.py</code> exercises the <code>node/src/py.rs</code> bridge for deterministic ledger replay and educational demos.</li>
<li>Headless tooling (<code>docs/headless.md</code> content) stays in <code>cli/src/headless.rs</code> and <code>docs/apis_and_tooling.md</code>.</li>
</ul>
<h2 id="dependency-policy"><a class="header" href="#dependency-policy">Dependency Policy</a></h2>
<ul>
<li>Policies live in <code>config/dependency_policies.toml</code>. Run <code>cargo run -p dependency_registry -- --check config/dependency_policies.toml</code> (or <code>just dependency-audit</code>) to refresh <code>docs/dependency_inventory*.json</code>.</li>
<li>The pivot strategy formerly described in <code>docs/pivot_dependency_strategy.md</code> now reads: wrap critical stacks in first-party crates, record governance overrides, and track violations via telemetry + dashboards.</li>
<li>Never introduce <code>reqwest</code>, <code>serde_json</code>, <code>bincode</code>, etc. Production crates must route through the first-party facades.</li>
</ul>
<h2 id="formal-methods-and-verification"><a class="header" href="#formal-methods-and-verification">Formal Methods and Verification</a></h2>
<ul>
<li>Formal specs (<code>formal/*.fst</code>, <code>docs/formal.md</code>) integrate with CI. Run <code>make -C formal</code> or <code>cargo test -p formal</code> to re-check F* proofs before merging math-heavy changes.</li>
<li>zk-SNARK and Dilithium proofs are stored alongside code; refer to <code>docs/maths/</code> for derivations.</li>
<li>Prover benchmarking harnesses live under <code>node/src/compute_market/tests/prover.rs</code> so you can run focused comparisons between CPU and GPU provers (<code>cargo test -p the_block prover_cpu_gpu_latency_smoke</code>).</li>
<li><code>tb-cli compute proofs --limit 10</code> calls <code>compute_market.sla_history</code> and prints proof fingerprints, backend selection, and circuit artifacts so you can validate end-to-end traces without spelunking the settlement sled DB.</li>
<li><code>tb-cli explorer sync-proofs --db explorer.db --url http://localhost:26658</code> takes the same RPC output, persists <code>Vec&lt;ProofBundle&gt;</code> records inside the explorer SQLite tables, and lets you re-verify bundles (or feed <code>/compute/sla/history</code>) without granting RPC access to dashboards.</li>
<li>Simulation framework lives under <code>sim/</code>:
<ul>
<li>Scenarios are regular Rust binaries (see <code>sim/examples/basic.rs</code>, <code>sim/fee_spike.rs</code>, <code>sim/compute_market/*</code>). They accept <code>--scenario &lt;name&gt; --out &lt;dir&gt;</code> flags and emit JSON summaries (latency histograms, slashing events, etc.).</li>
<li><code>cargo run -p sim -- --scenario dependency_fault --config sim/src/dependency_fault_harness/config.toml</code> reproduces dependency-fault drills. Logs land in <code>sim/target/</code>.</li>
<li>Use the harness before altering consensus/governance logic; CI expects new scenarios for major protocol toggles.</li>
</ul>
</li>
</ul>
<h2 id="logging-and-traceability"><a class="header" href="#logging-and-traceability">Logging and Traceability</a></h2>
<ul>
<li>Logging guidelines from <code>docs/logging.md</code> live here: use structured events, avoid PII, include <code>component</code>, <code>peer</code>, <code>slot</code>, <code>lane</code>, <code>job_id</code> labels.</li>
<li>Traces feed into the metrics aggregator and optionally into external stacks via exporters (no vendor lock-in required).</li>
</ul>
<h2 id="explainability-and-ai-diagnostics"><a class="header" href="#explainability-and-ai-diagnostics">Explainability and AI Diagnostics</a></h2>
<ul>
<li><code>docs/explain.md</code> + <code>docs/ai_diagnostics.md</code> merged here. CLI commands <code>tb-cli explain tx|block|governance</code> render JSON traces; governance toggles <code>ai_diagnostics_enabled</code> to control ANN-based alerts.</li>
</ul>
<h2 id="developer-support-scripts"><a class="header" href="#developer-support-scripts">Developer Support Scripts</a></h2>
<ul>
<li><code>Justfile</code> targets include bootstrap, fmt/lint/test, docs, coverage, fuzz, and docker image builds.</li>
<li><code>scripts/</code> directory hosts installers, overlay-store migrations, settlement audits, chaos helpers, and release scripts.</li>
<li><code>scripts/deploy-worldos-testnet.sh</code> spins up the World OS energy stack (node + mock oracle + telemetry). Pair it with <code>docs/testnet/ENERGY_QUICKSTART.md</code> to exercise the <code>energy.*</code> RPCs locally.</li>
<li>Use <code>tools/</code> for specialist binaries (settlement audit, peer-store migrator, etc.).</li>
</ul>
<h2 id="energy-market-development"><a class="header" href="#energy-market-development">Energy Market Development</a></h2>
<ul>
<li><strong>Crates and modules</strong> — <code>crates/energy-market</code> owns the provider/credit/receipt data model, metrics, and serialization; <code>node/src/energy.rs</code> persists the market via <code>SimpleDb</code> (sled under <code>TB_ENERGY_MARKET_DIR</code>, default <code>energy_market/</code>), exposes health checks, and records treasury accruals. RPC handlers live in <code>node/src/rpc/energy.rs</code>, CLI glue in <code>cli/src/energy.rs</code>, oracle ingestion under <code>crates/oracle-adapter</code>, and the mock oracle service in <code>services/mock-energy-oracle</code>.</li>
<li><strong>Configuration</strong> — Set <code>TB_ENERGY_MARKET_DIR</code> to relocate the sled DB (mirrors other <code>SimpleDb</code> consumers). Governance parameters (<code>energy_min_stake</code>, <code>energy_oracle_timeout_blocks</code>, <code>energy_slashing_rate_bps</code>) live in the shared <code>governance</code> crate; the runtime hooks call <code>node::energy::set_governance_params</code> so proposal activations atomically retune stakes, expiry, and slashing without code changes.</li>
<li><strong>RPC and CLI flows</strong> — <code>tb-cli energy register|market|settle|submit-reading</code> speak the same JSON schema the RPC expects (see <code>docs/apis_and_tooling.md#energy-rpc-payloads-auth-and-error-contracts</code>). Use <code>--verbose</code> or <code>--format json</code> to dump raw payloads for automation or explorer ingestion. Example round-trip:
<pre><code class="language-bash">tb-cli energy register 10000 120 --meter-address meter_a --jurisdiction US_CA --stake 5000 --owner acct
tb-cli energy market --provider-id energy-0x00 --verbose | jq .
tb-cli energy submit-reading --reading-json @reading.json
tb-cli energy settle energy-0x00 400 --meter-hash &lt;hex&gt; --buyer acct_consumer
</code></pre>
</li>
<li><strong>Telemetry &amp; metrics</strong> — The crate emits <code>energy_providers_count</code>, <code>energy_avg_price</code>, <code>energy_kwh_traded_total</code>, <code>energy_settlements_total{provider}</code>, <code>energy_provider_fulfillment_ms</code>, and <code>oracle_reading_latency_seconds</code>. Gate pending-credit health via <code>node::energy::check_energy_market_health</code> logs; dashboards ingest the same metrics via the metrics-aggregator.</li>
<li><strong>Testing</strong> — Run <code>cargo test -p energy-market</code> for unit coverage and <code>cargo test -p node --test gov_param_wiring</code> to ensure governance parameters round-trip correctly. Use <code>scripts/deploy-worldos-testnet.sh</code> + <code>docs/testnet/ENERGY_QUICKSTART.md</code> for integration drills (node + mock oracle + telemetry). When altering serialization, add vectors under <code>crates/energy-market/tests</code> and extend the CLI tests in <code>cli/tests/</code> to keep JSON schemas stable.</li>
<li><strong>Oracle adapters</strong> — <code>crates/oracle-adapter</code> currently ships <code>NoopSignatureVerifier</code>; replacing it with the real verifier requires feeding Ed25519/Schnorr keys through env vars (<code>TB_ORACLE_SIGNING_KEY</code>, etc., to be finalised) and extending test vectors. The mock oracle service (<code>services/mock-energy-oracle</code>) exposes <code>/meter/:id/reading</code> and <code>/meter/:id/submit</code> endpoints over the in-house <code>httpd</code> router so you can simulate both fetching and submitting readings without third-party stacks.</li>
<li><strong>Next steps</strong> — Signature verification, dispute RPCs, explorer visualisations, and deterministic replay coverage are tracked in <code>docs/architecture.md#energy-governance-and-rpc-next-tasks</code> and summarised in <code>AGENTS.md</code>. Treat those bullets as blocking work items whenever you touch the energy crates.</li>
</ul>
<h2 id="contribution-flow"><a class="header" href="#contribution-flow">Contribution Flow</a></h2>
<ol>
<li>Open an issue or draft PR describing the change.</li>
<li>Create a feature branch, keep it rebased, and avoid merge commits.</li>
<li>Run <code>fmt</code>, <code>clippy</code>, <code>nextest</code>, relevant integration tests, and <code>mdbook build docs</code>.</li>
<li>Update docs (this handbook + subsystem sections) as part of the same PR.</li>
<li>Include test output + rationale in the PR description; mention any skipped suites and why.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="apis-and-tooling"><a class="header" href="#apis-and-tooling">APIs and Tooling</a></h1>
<p>Reference for every public surface: RPC, CLI, gateway, DNS, explorer, telemetry, and schemas.</p>
<h2 id="json-rpc"><a class="header" href="#json-rpc">JSON-RPC</a></h2>
<ul>
<li>Server lives in <code>node/src/rpc</code>. Namespaces: <code>consensus</code>, <code>ledger</code>, <code>storage</code>, <code>compute_market</code>, <code>ad_market</code>, <code>governance</code>, <code>peer</code>, <code>treasury</code>, <code>vm</code>, <code>logs</code>, <code>state_stream</code>, <code>analytics</code>.</li>
<li>Transport: first-party <code>httpd</code> router (HTTP/1.1, HTTP/2, WebSocket upgrades) plus mutual-TLS derived from node keys.</li>
<li>Fault handling: clients clamp <code>TB_RPC_FAULT_RATE</code>, saturate exponential backoff after 31 attempts, and expose regression coverage for bounded retries.</li>
<li>Streaming: <code>state_stream</code> (light-client snapshots), <code>compute.job_cancel</code>, and telemetry correlations.</li>
</ul>
<h2 id="cli-tb-cli"><a class="header" href="#cli-tb-cli">CLI (<code>tb-cli</code>)</a></h2>
<ul>
<li>Main entry in <code>cli/src/main.rs</code>. Subcommands include: <code>node</code>, <code>gov</code>, <code>wallet</code>, <code>bridge</code>, <code>dex</code>, <code>compute</code>, <code>storage</code>, <code>gateway</code>, <code>mesh</code>, <code>light</code>, <code>telemetry</code>, <code>probe</code>, <code>diagnostics</code>, <code>service-badge</code>, <code>remediation</code>, <code>ai</code>, <code>ann</code>, <code>identity</code>.</li>
<li><code>tb-cli energy</code> wraps the <code>energy.*</code> RPCs (<code>register</code>, <code>market</code>, <code>settle</code>, <code>submit-reading</code>). It prints friendly tables by default and supports <code>--verbose</code>/<code>--format json</code> when you need machine-readable payloads or to export providers/receipts for explorer ingestion. See <code>docs/testnet/ENERGY_QUICKSTART.md</code> for scripted walkthroughs and dispute drills.</li>
<li>Use <code>tb-cli --help</code> or <code>tb-cli &lt;cmd&gt; --help</code>. Structured output via <code>--format json</code> for automation.</li>
<li>CLI shares foundation crates (serialization, HTTP client, TLS) with the node, so responses stay type-aligned.</li>
</ul>
<h3 id="energy-rpc-payloads-auth-and-error-contracts"><a class="header" href="#energy-rpc-payloads-auth-and-error-contracts">Energy RPC payloads, auth, and error contracts</a></h3>
<ul>
<li>Endpoints live under <code>energy.*</code> and inherit the RPC server’s mutual-TLS/auth policy (<code>TB_RPC_AUTH_TOKEN</code>, allowlists) plus IP-based rate limiting defined in <code>docs/operations.md#gateway-policy</code>. Use <code>tb-cli diagnostics rpc-policy</code> to inspect the live policy before enabling public oracle submitters.</li>
<li>Endpoint map:</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Method</th><th>Description</th><th>Request Body</th></tr></thead><tbody>
<tr><td><code>energy.register_provider</code></td><td>Register capacity/jurisdiction/meter binding plus stake.</td><td><code>{ "capacity_kwh": u64, "price_per_kwh": u64, "meter_address": "string", "jurisdiction": "US_CA", "stake": u64, "owner": "account-id" }</code></td></tr>
<tr><td><code>energy.market_state</code></td><td>Fetch snapshot of providers, outstanding meter credits, and receipts; pass <code>{"provider_id":"energy-0x01"}</code> to filter.</td><td>optional object</td></tr>
<tr><td><code>energy.submit_reading</code></td><td>Submit signed meter total to mint a credit.</td><td><code>MeterReadingPayload</code> JSON (below)</td></tr>
<tr><td><code>energy.settle</code></td><td>Burn credit + capacity to settle kWh and produce <code>EnergyReceipt</code>.</td><td><code>{ "provider_id": "energy-0x01", "buyer": "acct"?, "kwh_consumed": u64, "meter_hash": "0x..." }</code></td></tr>
</tbody></table>
</div>
<ul>
<li><code>energy.market_state</code> response structure:</li>
</ul>
<pre><code class="language-json">{
  "status": "ok",
  "providers": [ { "provider_id": "energy-0x00", "capacity_kwh": 10_000, "...": "..." } ],
  "credits": [ { "provider": "energy-0x00", "meter_hash": "e3c3…", "amount_kwh": 120, "timestamp": 123456 } ],
  "receipts": [ { "buyer": "acct", "seller": "energy-0x00", "kwh_delivered": 50, "price_paid": 2500, "treasury_fee": 125, "slash_applied": 0, "meter_hash": "e3c3…" } ]
}
</code></pre>
<ul>
<li><code>MeterReadingPayload</code> schema (shared by <code>oracle-adapter</code>, RPC, CLI, and explorer tooling):</li>
</ul>
<pre><code class="language-jsonc">{
  "provider_id": "energy-0x00",
  "meter_address": "mock_meter_1",
  "kwh_reading": 12000,
  "timestamp": 1710000000,
  "signature": "hex-encoded ed25519/schnorr blob"
}
</code></pre>
<ul>
<li>Error strings bubble up from <code>energy_market::EnergyMarketError</code> and always return <code>{ "error": "&lt;string&gt;" }</code> so clients can check <code>.error</code>. Expect the following failure families:
<ul>
<li><code>ProviderExists</code>, <code>MeterAddressInUse</code>, <code>UnknownProvider</code> when IDs collide.</li>
<li><code>InsufficientStake</code>, <code>InsufficientCapacity</code>, <code>InsufficientCredit</code> for quota/stake mismatches.</li>
<li><code>StaleReading</code>, <code>InvalidMeterValue</code>, <code>CreditExpired</code> when timestamps regress or expiry exceeded.</li>
<li>Signature/format errors: RPC rejects payloads where <code>meter_hash</code> is not 32 bytes, numbers are missing, or signatures fail decoding (and, once the oracle verifier lands, cryptographic verification failures).</li>
</ul>
</li>
<li>Negative tests live next to the RPC module; mimic them for client libraries so bad signatures, stale timestamps, and meter mismatches produce structured failures instead of panics.</li>
<li>Observer tooling: <code>tb-cli energy market --verbose</code> dumps the whole response; <code>tb-cli diagnostics rpc-log --method energy.submit_reading</code> tails submissions with auth metadata so you can trace rate-limit hits.</li>
<li>Per the architecture roadmap, the next RPC/CLI work items are: adding authenticated disputes + receipt listings, wiring explorer/CLI visualizations for param history, and enforcing per-endpoint rate limiting once the QUIC chaos drills complete (tracked in <code>docs/architecture.md#energy-governance-and-rpc-next-tasks</code>).</li>
</ul>
<h2 id="gateway-http-and-cdn-surfaces"><a class="header" href="#gateway-http-and-cdn-surfaces">Gateway HTTP and CDN Surfaces</a></h2>
<ul>
<li><code>node/src/gateway/http.rs</code> hosts HTTP + WebSocket endpoints for content, APIs, and read receipts. Everything goes through the first-party TLS stack (<code>crates/httpd::tls</code>).</li>
<li>Operators tag responses with <code>ReadAck</code> headers so clients can submit proofs later.</li>
<li>Range-boost forwarding and mobile cache endpoints hang off the same router; see <code>docs/architecture.md#gateway-and-client-access</code> for internals.</li>
</ul>
<h2 id="http-client-and-tls-diagnostics"><a class="header" href="#http-client-and-tls-diagnostics">HTTP client and TLS diagnostics</a></h2>
<ul>
<li>Outbound clients live in <code>crates/httpd::{client.rs,blocking.rs}</code>. <code>httpd::Client</code> wraps the runtime <code>TcpStream</code>, supports HTTPS via the in-house TLS connector, and exposes:
<ul>
<li><code>ClientConfig { connect_timeout, request_timeout, max_response_bytes, tls }</code>.</li>
<li><code>Client::with_tls_from_env(&amp;["TB_NODE_TLS","TB_HTTP_TLS"])</code> to reuse the same certs as RPC/gateway surfaces.</li>
<li><code>RequestBuilder::json(value)</code> for canonical JSON encoding and <code>send()</code> for async execution. Blocking variants offer the same API for CLI tools.</li>
</ul>
</li>
<li>TLS rotation:
<ul>
<li>Set <code>TB_NET_CERT_STORE_PATH</code> to control where mutual-TLS certs are stored. <code>tb-cli net rotate-cert</code> and <code>tb-cli net rotate-key</code> (see <code>cli/src/net.rs</code>) wrap the RPCs that rotate QUIC certs and peer keys.</li>
<li>Diagnostics: <code>tb-cli net quic failures --url &lt;rpc&gt;</code> lists handshake failures; <code>tb-cli net overlay-status</code> confirms which overlay backend is active and where the peer DB lives.</li>
</ul>
</li>
<li>HTTP troubleshooting: both the node and CLI honour <code>TB_RPC_TIMEOUT_MS</code>, <code>TB_RPC_TIMEOUT_JITTER_MS</code>, and <code>TB_RPC_MAX_RETRIES</code>. Use <code>tb-cli net dns verify</code> to confirm TXT records and <code>tb-cli net gossip-status</code> to inspect HTTP routing metadata exposed via gossip debug RPCs.</li>
</ul>
<h2 id="dns-and-naming"><a class="header" href="#dns-and-naming">DNS and Naming</a></h2>
<ul>
<li>Publishing: <code>node/src/gateway/dns.rs</code> writes <code>.block</code> zone files or external DNS records using schemas under <code>docs/spec/dns_record.schema.json</code>.</li>
<li>CLI: <code>tb-cli gateway dns publish</code>, <code>tb-cli gateway dns audit</code>.</li>
</ul>
<h2 id="explorer-and-log-indexer"><a class="header" href="#explorer-and-log-indexer">Explorer and Log Indexer</a></h2>
<ul>
<li>Explorer + indexer live under <code>explorer/</code> and share the governance crate with the node. They expose HTTP APIs for proposals, treasury events, storage receipts, compute payouts, and light-client timelines.</li>
<li>SQLite access routes through <code>foundation_sqlite</code> wrappers to keep dependency policy intact.</li>
</ul>
<h2 id="metrics-and-telemetry-apis"><a class="header" href="#metrics-and-telemetry-apis">Metrics and Telemetry APIs</a></h2>
<ul>
<li>Node <code>/metrics</code> endpoint exports Prometheus text via <code>runtime::telemetry::TextEncoder</code>.</li>
<li>Metrics aggregator exposes <code>/metrics</code>, <code>/wrappers</code>, <code>/governance</code>, <code>/treasury</code>, <code>/bridge</code>, <code>/probe</code>, <code>/chaos</code>, <code>/audit</code>, <code>/remediation/*</code>, <code>/telemetry/summary</code>.</li>
<li>Use <code>docs/operations.md#metrics-aggregator-ops</code> for deployment instructions and <code>monitoring/</code> for dashboards.</li>
</ul>
<h2 id="probe-cli"><a class="header" href="#probe-cli">Probe CLI</a></h2>
<ul>
<li>Binary lives in <code>crates/probe</code>. Usage examples:
<ul>
<li><code>probe ping-rpc --url http://127.0.0.1:3050 --timeout 5 --prom</code></li>
<li><code>probe gossip-check --addr 127.0.0.1:3030</code></li>
<li><code>probe mine-one --miner my-node</code></li>
</ul>
</li>
<li>Emits Prometheus lines when <code>--prom</code> is set, so it doubles as a blackbox exporter.</li>
</ul>
<h2 id="storage-and-blob-apis"><a class="header" href="#storage-and-blob-apis">Storage and Blob APIs</a></h2>
<ul>
<li>CLI: <code>tb-cli storage put|get|manifest|repair</code>, <code>tb-cli blob summarize</code>, <code>tb-cli storage providers</code>.</li>
<li>RPC: <code>storage.put_blob</code>, <code>storage.get_manifest</code>, <code>storage.list_providers</code>.</li>
<li>Blob manifests follow the binary schema in <code>node/src/storage/manifest_binary.rs</code>; object receipts encode <code>StoreReceipt</code> structs consumed by the ledger.</li>
</ul>
<h2 id="compute-energy-and-ad-market-apis"><a class="header" href="#compute-energy-and-ad-market-apis">Compute, Energy, and Ad Market APIs</a></h2>
<ul>
<li>Compute RPC/CLI: reserve capacity, post workloads, submit receipts, inspect fairness metrics, cancel jobs (<code>compute.job_cancel</code>). Courier snapshots stream through <code>compute_market.courier_status</code>, proof bundles (with fingerprints + circuit artifacts) are downloadable via <code>compute_market.sla_history(limit)</code>, <code>tb-cli compute proofs --limit N</code> pretty-prints recent SLA/proof entries, <code>tb-cli explorer sync-proofs --db explorer.db</code> ingests them into the explorer SQLite tables (<code>compute_sla_history</code> + <code>compute_sla_proofs</code>), and the explorer HTTP server exposes <code>/compute/sla/history?limit=N</code> so dashboards can render proof fingerprints without RPC access. The <code>snark</code> CLI (<code>cli/src/snark.rs</code>) still outputs attested circuit artifacts for out-of-band prover rollout.</li>
<li>Energy RPC/CLI: <code>energy.register_provider</code>, <code>energy.market_state</code>, <code>energy.settle</code>, and <code>energy.submit_reading</code> expose the <code>crates/energy-market</code> state plus oracle submissions. Governance feeds <code>energy_min_stake</code>, <code>energy_oracle_timeout_blocks</code>, and <code>energy_slashing_rate_bps</code> into this module, so operators can tune the market via proposals rather than recompiling.</li>
<li>Ad market RPC/CLI: reserve impressions, commit deliveries, record conversions, audit ANN proofs, manage mesh queues.</li>
</ul>
<h2 id="light-client-streaming"><a class="header" href="#light-client-streaming">Light-Client Streaming</a></h2>
<ul>
<li>RPC: <code>light.subscribe</code>, <code>light.get_block_range</code>, <code>light.get_device_status</code>, <code>state_stream.subscribe</code>. CLI: <code>tb-cli light sync</code>, <code>tb-cli light snapshot</code>, <code>tb-cli light device-status</code>.</li>
<li>Mobile heuristics (battery, bandwidth, overrides) persist under <code>~/.the_block/light_client.toml</code>.</li>
</ul>
<h2 id="bridge-dex-and-identity-apis"><a class="header" href="#bridge-dex-and-identity-apis">Bridge, DEX, and Identity APIs</a></h2>
<ul>
<li>Bridge RPC: <code>bridge.submit_proof</code>, <code>bridge.challenge</code>, <code>bridge.status</code>, <code>bridge.claim_reward</code>. CLI mirrors the same set.</li>
<li>DEX RPC/CLI: order placement, swaps, trust-line routing, escrow proofs, HTLC settlement.</li>
<li>Identity RPC: DID registration, revocation, handle lookup; CLI uses <code>tb-cli identity</code>.</li>
</ul>
<h2 id="wallet-apis"><a class="header" href="#wallet-apis">Wallet APIs</a></h2>
<ul>
<li>CLI supports multisig, hardware signers, remote signers, and escrow-hash configuration: see <code>cli/src/wallet.rs</code> and <code>node/src/bin/wallet.rs</code>.</li>
<li>Commands include wallet creation/import, address derivation, signing, broadcast, and governance voting (where applicable). Use <code>--format json</code> for automation.</li>
<li>Remote signer workflows emit telemetry and enforce multisig signer-set policies documented in <code>docs/security_and_privacy.md#remote-signers-and-key-management</code>.</li>
</ul>
<h2 id="schemas-and-reference-files"><a class="header" href="#schemas-and-reference-files">Schemas and Reference Files</a></h2>
<ul>
<li>JSON schemas under <code>docs/spec/</code> define fee market inputs (<code>fee_v2.schema.json</code>) and DNS records. Keep them in sync with code when adding fields.</li>
<li>Dependency inventory snapshots live in <code>docs/dependency_inventory*.json</code>; regenerate after dependency changes.</li>
<li>Assets (<code>docs/assets/</code>) include RSA samples, scheduler diagrams, and architecture SVGs referenced across the docs.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="legacy-mapping"><a class="header" href="#legacy-mapping">Legacy Mapping</a></h1>
<p>This index records where every pre-consolidation document landed. Each bullet lists the removed file(s) and the section inside the new handbook that now hosts the canonical content. Use it to answer “where did file X go?” without trawling git history.</p>
<h2 id="overview-docsoverviewmd"><a class="header" href="#overview-docsoverviewmd">Overview (<code>docs/overview.md</code>)</a></h2>
<ul>
<li><code>docs/roadmap.md</code>, <code>docs/progress.md</code>, <code>docs/detailed_updates.md</code>, <code>docs/system_changes.md</code>, <code>docs/changelog.md</code> → <a href="overview.html#document-map"><code>Document Map</code></a></li>
<li><code>docs/architecture/README.md</code>, <code>docs/architecture/node.md</code> → <a href="overview.html#repository-layout-live-tree"><code>Repository Layout</code></a></li>
</ul>
<h2 id="architecture-docsarchitecturemd"><a class="header" href="#architecture-docsarchitecturemd">Architecture (<code>docs/architecture.md</code>)</a></h2>
<ul>
<li><code>docs/consensus.md</code>, <code>docs/difficulty.md</code>, <code>docs/poh.md</code>, <code>docs/dkg.md</code>, <code>docs/sharding.md</code>, <code>docs/macro_block.md</code>, <code>docs/blob_chain.md</code>, <code>docs/hashlayout.md</code>, <code>docs/genesis_history.md</code>, <code>docs/serialization.md</code>, <code>docs/schema_migrations/v7_recent_timestamps.md</code> → <a href="architecture.html#ledger-and-consensus"><code>Ledger and Consensus</code></a></li>
<li><code>docs/transaction_lifecycle.md</code>, <code>docs/mempool.md</code>, <code>docs/mempool_qos.md</code>, <code>docs/account_abstraction.md</code>, <code>docs/identity.md</code>, <code>docs/tokens.md</code> → <a href="architecture.html#transaction-and-execution-pipeline"><code>Transaction and Execution Pipeline</code></a></li>
<li><code>docs/gossip.md</code>, <code>docs/gossip_chaos.md</code>, <code>docs/networking.md</code>, <code>docs/net_bootstrap.md</code>, <code>docs/net_a_star.md</code>, <code>docs/network_partitions.md</code>, <code>docs/network_topologies.md</code>, <code>docs/p2p_protocol.md</code>, <code>docs/network_quic.md</code>, <code>docs/quic.md</code>, <code>docs/swarm.md</code> → <a href="architecture.html#networking-and-propagation"><code>Networking and Propagation</code></a></li>
<li><code>docs/range_boost.md</code>, <code>docs/localnet.md</code>, <code>docs/headless.md</code>, <code>docs/ai_diagnostics.md</code> → <a href="architecture.html#localnet-and-range-boost"><code>LocalNet and Range Boost</code></a> and <a href="architecture.html#auxiliary-services"><code>Auxiliary Services</code></a></li>
<li><code>docs/storage.md</code>, <code>docs/storage_pipeline.md</code>, <code>docs/storage_erasure.md</code>, <code>docs/storage_market.md</code>, <code>docs/storage/repair.md</code>, <code>docs/simple_db.md</code>, <code>docs/wal.md</code>, <code>docs/snapshots.md</code>, <code>docs/state_pruning.md</code>, <code>docs/snapshot.md</code>, <code>docs/schema_migrations/v10_industrial_subsidies.md</code> → <a href="architecture.html#storage-and-state"><code>Storage and State</code></a></li>
<li><code>docs/compute_market.md</code>, <code>docs/compute_market_courier.md</code>, <code>docs/compute_snarks.md</code>, <code>docs/scheduler.md</code>, <code>docs/htlc_swaps.md</code> → <a href="architecture.html#compute-marketplace"><code>Compute Marketplace</code></a></li>
<li><code>docs/dex.md</code>, <code>docs/dex_amm.md</code>, <code>docs/schema_migrations/v9_dex_escrow.md</code> → <a href="architecture.html#dex-and-trust-lines"><code>DEX and Trust Lines</code></a></li>
<li><code>docs/bridges.md</code>, <code>docs/bridge_security.md</code>, <code>docs/schema_migrations/v8_bridge_headers.md</code> → <a href="architecture.html#token-bridges"><code>Token Bridges</code></a></li>
<li><code>docs/wallets.md</code>, <code>docs/gateway.md</code>, <code>docs/gateway_dns.md</code>, <code>docs/mobile_gateway.md</code>, <code>docs/mobile_light_client.md</code>, <code>docs/light_client.md</code>, <code>docs/light_client_stream.md</code>, <code>docs/light_client_incentives.md</code>, <code>docs/read_receipts.md</code> → <a href="architecture.html#gateway-and-client-access"><code>Gateway and Client Access</code></a></li>
<li><code>docs/telemetry.md</code>, <code>docs/metrics.md</code> → <a href="architecture.html#telemetry-and-instrumentation"><code>Telemetry and Instrumentation</code></a></li>
<li><code>docs/service_badge.md</code>, <code>docs/le_portal.md</code>, <code>docs/probe.md</code> → <a href="architecture.html#auxiliary-services"><code>Auxiliary Services</code></a></li>
</ul>
<h2 id="economics-and-governance-docseconomics_and_governancemd"><a class="header" href="#economics-and-governance-docseconomics_and_governancemd">Economics and Governance (<code>docs/economics_and_governance.md</code>)</a></h2>
<ul>
<li><code>docs/economics.md</code>, <code>docs/inflation.md</code>, <code>docs/tokens.md</code>, <code>docs/fee_rebates.md</code>, <code>docs/fees.md</code>, <code>docs/settlement_switch.md</code>, <code>docs/settlement_audit.md</code> → <a href="economics_and_governance.html#ct-supply-and-sub-ledgers"><code>CT Supply</code>, <code>Fee Lanes</code>, and <code>Settlement</code></a></li>
<li><code>docs/treasury.md</code>, <code>docs/ledger_invariants.md</code>, <code>docs/service_badge.md</code> → <a href="economics_and_governance.html#treasury-and-disbursements"><code>Treasury</code>, <code>Service Badges</code>, and <code>Ledger Invariants</code></a></li>
<li><code>docs/governance.md</code>, <code>docs/governance_params.md</code>, <code>docs/governance_release.md</code>, <code>docs/governance_rollback.md</code>, <code>docs/governance_ui.md</code>, <code>docs/commit_reveal.md</code>, <code>docs/system_changes.md</code>, <code>docs/audit_handbook.md</code>, <code>docs/risk_register.md</code> → <a href="economics_and_governance.html#proposal-lifecycle"><code>Proposal Lifecycle</code>, <code>Governance Parameters</code>, and <code>Risk Controls</code></a></li>
</ul>
<h2 id="operations-docsoperationsmd"><a class="header" href="#operations-docsoperationsmd">Operations (<code>docs/operations.md</code>)</a></h2>
<ul>
<li><code>docs/deployment.md</code>, <code>docs/deployment_guide.md</code>, <code>docs/runbook.md</code>, <code>docs/operators/README.md</code>, <code>docs/operators/run_a_node.md</code>, <code>docs/operators/upgrade.md</code> → <a href="operations.html#bootstrap-and-configuration"><code>Bootstrap</code>, <code>Running a Node</code>, and <code>Deployment</code></a></li>
<li><code>docs/monitoring.md</code>, <code>docs/monitoring/README.md</code>, <code>docs/dashboard.md</code>, <code>docs/telemetry_ops.md</code>, <code>docs/metrics.md</code>, <code>docs/operators/telemetry.md</code> → <a href="operations.html#telemetry-wiring"><code>Telemetry Wiring</code>, <code>Metrics Aggregator Ops</code>, and <code>Monitoring</code></a></li>
<li><code>docs/probe.md</code>, <code>docs/operators/incident_playbook.md</code>, <code>docs/settlement_audit.md</code>, <code>docs/settlement_switch.md</code> → <a href="operations.html#probe-cli-and-diagnostics"><code>Probe CLI and Diagnostics</code></a> and <a href="operations.html#incident-response"><code>Incident Response</code></a></li>
<li><code>docs/storage/repair.md</code>, <code>docs/snapshots.md</code>, <code>docs/snapshot.md</code>, <code>docs/wal.md</code> → <a href="operations.html#storage-snapshots-and-wal-management"><code>Storage, Snapshots, and WAL Management</code></a></li>
<li><code>docs/range_boost.md</code>, <code>docs/localnet.md</code> → <a href="operations.html#range-boost-and-localnet-operations"><code>Range Boost and LocalNet Operations</code></a></li>
<li><code>docs/system_changes.md</code>, <code>docs/detailed_updates.md</code>, <code>docs/changelog.md</code> → <a href="operations.html#operator-checklist"><code>Operator Checklist</code></a></li>
</ul>
<h2 id="security-and-privacy-docssecurity_and_privacymd"><a class="header" href="#security-and-privacy-docssecurity_and_privacymd">Security and Privacy (<code>docs/security_and_privacy.md</code>)</a></h2>
<ul>
<li><code>docs/crypto.md</code>, <code>docs/crypto_migration.md</code>, <code>docs/remote_signer_security.md</code>, <code>docs/threat_model/README.md</code>, <code>docs/threat_model/hosting.md</code>, <code>docs/threat_model/storage.md</code> → <a href="security_and_privacy.html#threat-model"><code>Threat Model</code>, <code>Cryptography</code>, and <code>Remote Signers</code></a></li>
<li><code>docs/kyc.md</code>, <code>docs/jurisdiction.md</code>, <code>docs/jurisdiction_authoring.md</code>, <code>docs/privacy_layer.md</code>, <code>docs/privacy_compliance.md</code>, <code>docs/le_portal.md</code> → <a href="security_and_privacy.html#kyc-jurisdiction-and-compliance"><code>KYC, Jurisdiction, and Law-Enforcement</code></a></li>
<li><code>docs/supply_chain.md</code>, <code>docs/repro.md</code>, <code>docs/reproducible_builds.md</code>, <code>docs/provenance.md</code>, <code>docs/release_provenance.md</code>, <code>docs/pivot_dependency_strategy.md</code> → <a href="security_and_privacy.html#release-provenance-and-supply-chain"><code>Release Provenance and Supply Chain</code></a></li>
<li><code>docs/bridge_security.md</code>, <code>docs/htlc_swaps.md</code>, <code>docs/risk_register.md</code>, <code>docs/audit_handbook.md</code> → <a href="security_and_privacy.html#bridge-and-cross-chain-security"><code>Bridge and Cross-Chain Security</code> and <code>Risk Register</code></a></li>
</ul>
<h2 id="developer-handbook-docsdeveloper_handbookmd"><a class="header" href="#developer-handbook-docsdeveloper_handbookmd">Developer Handbook (<code>docs/developer_handbook.md</code>)</a></h2>
<ul>
<li><code>docs/developer_setup.md</code>, <code>docs/development.md</code>, <code>docs/concurrency.md</code>, <code>docs/logging.md</code>, <code>docs/debugging.md</code>, <code>docs/testing.md</code>, <code>docs/performance.md</code>, <code>docs/benchmarks.md</code>, <code>docs/formal.md</code>, <code>docs/simulation_framework.md</code> → <a href="developer_handbook.html#environment-setup"><code>Environment Setup</code>, <code>Coding Standards</code>, <code>Testing</code>, <code>Performance</code>, and <code>Formal Methods</code></a></li>
<li><code>docs/contract_dev.md</code>, <code>docs/wasm_contracts.md</code>, <code>docs/vm_debugging.md</code>, <code>docs/vm.md</code> → <a href="developer_handbook.html#contract-and-vm-development"><code>Contract and VM Development</code></a></li>
<li><code>docs/demo.md</code>, <code>docs/headless.md</code>, <code>docs/explain.md</code>, <code>docs/ai_diagnostics.md</code> → <a href="developer_handbook.html#python--headless-tooling"><code>Python + Headless Tooling</code> and <code>Explainability</code></a></li>
<li><code>docs/pivot_dependency_strategy.md</code>, <code>docs/dependency_inventory.md</code>, <code>docs/first_party_dependency_audit.md</code>, <code>docs/contributing.md</code> → <a href="developer_handbook.html#dependency-policy"><code>Dependency Policy</code> and <code>Contribution Flow</code></a></li>
</ul>
<h2 id="apis-and-tooling-docsapis_and_toolingmd"><a class="header" href="#apis-and-tooling-docsapis_and_toolingmd">APIs and Tooling (<code>docs/apis_and_tooling.md</code>)</a></h2>
<ul>
<li><code>docs/rpc.md</code>, <code>docs/first_party_rpc_blockers.md</code> → <a href="apis_and_tooling.html#json-rpc"><code>JSON-RPC</code></a></li>
<li><code>docs/gateway.md</code>, <code>docs/mobile_gateway.md</code>, <code>docs/gateway_dns.md</code>, <code>docs/wallets.md</code> → <a href="apis_and_tooling.html#gateway-http-and-cdn-surfaces"><code>Gateway HTTP and CDN Surfaces</code></a> and <a href="apis_and_tooling.html#dns-and-naming"><code>DNS and Naming</code></a></li>
<li><code>docs/explorer.md</code>, <code>docs/service_badge.md</code>, <code>docs/telemetry.md</code> → <a href="apis_and_tooling.html#explorer-and-log-indexer"><code>Explorer and Log Indexer</code></a> and <a href="apis_and_tooling.html#metrics-and-telemetry-apis"><code>Metrics and Telemetry APIs</code></a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
