---
title: Task 4 — Real Market Cliffs
---

# Task 4 · Finish the “Real Market” Cliffs

This task completes the proof-of-market strategy around storage incentives and compute SLAs. The existing receipt validation pipeline (`node/src/receipts_validation.rs:551-660`) is the immutable **accounting substrate** (nonces, signatures, dedup) that every SLA rule references. What remains is the **game-theory substrate** that answers: how does the system respond when providers collude, oracles lie, jobs only partially complete, outputs are ambiguous, or entire regions go dark? The goal is not perfect economics; it is a set of rules that are **hard to exploit, easy to explain, and trivial to audit**.

## 1 · Storage Incentives

Storage becomes “done” when the node can pick providers using observable economics, validate every proof, repair corrupted chunks deterministically, and slash dishonest actors with unambiguous receipts. The subsections below describe the signal flows, triggers, and guardrails we need in order to weave those guarantees into the receipt stream.

### 1.1 Provider accountability surface
- **Selection signals:** `storage_market::catalog` stores signed `ProviderProfile` advertisements that include `pricing`, `capacity`, `region`, `latency_hist`, `escrow_balance`, and `provider_nonce` counters. Selection only considers the cheapest candidate whose `stor_id` has at least `TB_STORAGE_SEL_MIN_RECEIPTS` receipts in the past `TB_STORAGE_HISTORY_EPOCHS` epochs and whose success rate (recorded per epoch in `storage_market::bandwidth::SuccessRate`) stays above `TB_STORAGE_MIN_SUCCESS_PCT`. `StorageMarket::discover_providers` merges DHT results with the local cache, normalizes price/latency, and sorts by `score = price_per_byte * (1 + penalty)` where `penalty` grows with `1 - success_rate`.
- **Receipt proof:** Every `StorageReceipt` must include `provider`, `contract_id`, `provider_nonce`, `provider_escrow`, `bytes`, `kpi_latency_ms`, and a BLAKE3 fingerprint of the chunk (`chunk_hash`). The validator rejects receipts missing these fields with `ValidationError::MissingBlockTorchMetadata`. The same metadata feeds `ReceiptId::from_receipt`, so cross-block dedup/partial checks stay deterministic.
  This payload is now forwarded through `storage::slash::record_receipt` during live block production and again inside the deterministic replay helpers, so repair cancellation and replayed nonce checks always see the exact chunk hash, region, and provider nonce that the ledger serialized.
- **Economic counters:** Per-epoch aggregates (`storage_receipts_per_provider`, `storage_receipt_bytes`, `storage_slash_total`) roll up into `EconomicsPrevMetric` so Governor replays can prove that issued rewards + subsidy shares map 1:1 to the receipts archived in the ledger. These aggregates become the `economics_prev_market_metrics_{utilization,provider_margin}_ppm` inputs that open/close the storage gate.

### 1.2 Repair & slashing rules
- **Repair trigger:** Either the on-chain audit RPC (`storage.audit`) or the repair scheduler (`node/src/storage/repair.rs`) finds a missing chunk within `REPAIR_WINDOW` blocks. The provider must emit a `StorageReceipt { repaired: true, repaired_chunk_hash }` within the same window, otherwise the governance path builds a `StorageSlash` receipt that burns `max(provider_escrow, RentPerByte * bytes)` and credits `StorageRepairBounty`.
  The `storage.audit` JSON-RPC mirrors the repair scheduler: it walks stored manifests, locates absent chunk blobs, and calls `storage::slash::report_missing_chunk` with the same rent-per-byte × byte and `Settlement::balance` escrow assumptions so both audit streams reach the same `StorageSlash` deadline before any slash is issued.
- **Repair accounting:** The scheduler now feeds `StorageSlash` the provider’s current escrow (`Settlement::balance(provider)`) plus the configured rent rate so the deadline burn is always `max(provider_escrow, rent_per_byte * bytes)` and the governor publishes the same forecast hash it used to trigger the timeout.
- **Repair reconciliation:** Successful repair jobs immediately clear the pending repair slot (and any associated telemetry) so the remittance can replay the original `RepairKey` and avoid emitting a redundant `StorageSlash` once the chunk reappears in the receipt stream.
- **Collusion guard:** Colluding providers must still vary their `provider_nonce` sequences and chunk hashes. Any duplicate `ReceiptId` after dedup (e.g., `ReplayedNonce`) invalidates *all* receipts that share the replayed nonce/hash combination, and the governor mints slash receipts for each offending provider. This makes replaying another provider’s proof (even if they were paid) economically unattractive.
- **Dark-region detection:** A region is marked dark when its providers miss publishing receipts for `DARK_THRESHOLD` epochs. The governor records the `RegionStatus::Dark` flag in block metadata (the same vector that feeds `metrics::snapshot_from_metrics`) so replay can detect it deterministically. New uploads skip the dark region by routing contracts to `(shadowed)` providers until the region emits a clean streak, and the reputation score (derived from the receipt hash stream and stored in `node/src/storage/region.rs`) keeps that penalty visible to explorers/operators.
- **Metric guardrails:** `storage_slash_total`, `storage_repair_missed`, `storage_region_dark_total`, and the per-provider `storage_success_rate` gauges feed the metrics aggregator. Dashboards and alerts draw directly from `/wrappers` so operators can tell whether slashes correspond to audit failures, replayed nonces, or dark-region reroutes without referring to logs.

## 2 · Compute SLA Arbitration

Compute is “done” when disputes resolve deterministically, partial work can be audited immediately, and providers cannot fake completion without burning more than they steal. Like storage, receipts embed every signal required for arbitration and replay; the sections below spell out the arbitration pipeline, the partial-output contract, and the resilience to lying oracles/dark regions.

### 2.1 Deterministic arbitration pipeline
- **Receipt escrow:** Each `ComputeReceipt` (see `node/src/receipts.rs`) includes `verified`, `proof_latency_ms`, `workload_hash`, `resource_units`, `output_manifest_hash`, `oracle_provider_id`, and a BLAKE3 hash of the proof artifact. `receipts_validation` rejects any `verified` receipt with zero latency or missing metadata. `ReceiptId::from_receipt` factors in `provider`, `job_id`, `signature_nonce`, and `proof_hash`, guaranteeing that deterministic replay cannot confuse which receipt paid which provider.
- **Dispute lifecycle:** Clients submit disputes via `compute_market::dispute::open` including the expected `job_id`, `workload_hash`, `output_manifest_hash`, and `expected_resource_units`. On the next epoch the governor calls `compute_market::dispute::drain_pending`, replays the relevant receipts with `replay_economics_to_tip`, and compares hashed artifacts (manifest, proof, resource usage). Any mismatch emits a `ComputeSlash` receipt that burns the provider’s escrow and mints a reward for the challenger. The slash reason (hash mismatch, missing partial, stale proof) is recorded so explorers can question routers.
- **Lane-aware SLA:** The lane scheduler (`node/src/compute_market/slots.rs`) rotates fairness windows per lane while `compute_market::settlement::SlaOutcome` tracks `fulfilled`, `partial`, `disputed`, and `slashed` counts. When a lane drops below `TB_COMPUTE_MIN_FULFILL_RATE`, `match_loop_latency_seconds{lane}` ticks, and the governor can shadow the lane before permanently gating it.

### 2.2 Partial completion & ambiguous outputs
- **Partial-indexing:** Jobs that produce multiple outputs must emit one receipt per output with `partial_index` + `partial_total` and the shared `output_manifest_hash`. The governor replay reconstructs the sequence and treats duplicates or missing indices as a breach. For example, if a provider submits `partial_index=1/3` twice without ever publishing `2/3` or `3/3`, they automatically forfeit the escrow for that job (`ComputeSlash` includes the rollout reason “missing partial”).
- **Output ambiguity:** The `output_manifest` encodes deterministic lyrics (digest, type, size) for every declared artifact. Any later replay that hashes a different manifest while pointing to the same `job_id` triggers `ValidationError::InvalidBlockTorchMetadata` and produces a `ComputeSlash` receipt, so providers cannot equivocate about what they delivered.
- **Resource accountability:** `resource_units` and `proof_latency_ms` are witness values in the receipt; the governor compares them to the contract’s pledged resource window. Providers that consistently under-report units but over-deliver proofs get marked for `traceability` and may be rerouted to `TB_COMPUTE_STRICT_MODE` lanes that expose extra telemetry (`compute_proof_latency_histogram`).

### 2.3 Oracle & dark-region resilience
- **Oracle lie safeguard:** Receipts include `oracle_provider_id` and an OTP-signed `oracle_reading_ts`. Repudiating readings (timestamp drift, reused hash) triggers `ValidationError::InvalidBlockTorchMetadata` and places that oracle in `OracleStatus::Suspended` until two clean receipts from a different oracle arrive. The governor stores the last-clean oracle hash so replay can verify the behavior deterministically.
- **Dark job reroute:** Regions that go dark for `DARK_THRESHOLD` epochs trigger a reroute. New jobs annotate receipts with `reroute_from` + `reroute_reason` so audits retain the full trail. When rerouted receipts are processed, both the original and fallback providers share the audit data, allowing the governor to refund consumers if both sides fail or to slash just the responsible party when the fallback provider succeeds.
- **Fallback economics:** Rerouted jobs bump the clearing price (`compute_market::courier::reroute_fee_bps`) and increase the `resource_units` escrow required for that lane. That way providers that sit idle in a quiet region cannot exploit the reroute queue—their escrow must cover both the original batch and the fallback penalty if they abandon the reroute.

## 3 · Audit, Governance, and Implementation Checklist

### 3.1 Intent snapshots & operator transparency
- **Intent hashes:** Each economics intent the governor publishes (`governor/decisions/epoch-*.json`) stores the same hashed metrics that triggered a gate. When applying slashes, replay reuses the stored hash so the `StorageSlash`/`ComputeSlash` receipts align with the receipts the governor audited—there is no opportunity to “invent” theft in replay.
- **CLI/Explorer surfaces:** `contract-cli governor status` now prints `economics_sample` and `last_economics_snapshot_hash`. Operators can copy those hashes, replay the receipts via `replay_economics_to_tip`, and confirm the slashes recorded in `/governance/storage/slash_history` and `/governance/compute/slash_history`.
- **Telemetry & dashboards:** The aggregator `/wrappers` endpoint exposes `storage_slash_total`, `storage_region_dark_total`, `compute_slash_burn_total`, `compute_dispute_total`, plus lane-level histograms. Dashboards (`monitoring/grafana_real_market.json`) should include the same names so operators can correlate telemetry spikes with receipt history.

### 3.2 Implementation checklist
- `storage_market::slashing`: build the controller that watches auditor/proof reports, emits `StorageSlash` receipts, updates reputation scores, and records `RegionStatus`. Add regression tests in `node/tests/storage_slash.rs` for missing repairs, replayed nonces, and dark-region reroutes.
- `compute_market::dispute`: implement the deterministic dispute drainer that replays receipts, compares manifests, and emits `ComputeSlash`. Cover partial jobs, ambiguous outputs, and fallback reroutes in `node/tests/compute_dispute.rs`.
- `governor`: wire the new intent hashes and region flags so the same verifier can be used both for gating and for replay validation. Document the rollout steps (shadow mode → apply mode → telemetry) with references to `docs/operations.md#telemetry-wiring`.
- `blockchain`: export the `storage_receipt` and `compute_receipt` metadata (manifest hashes, reroute info, oracle status) in the block header so explorers can index them without scanning logs.
- `metrics-aggregator` + `monitoring`: refresh the Grafana dashboards and `/wrappers` documentation to show the new SLA metrics and ensure operators can replay the thresholds mentioned above.
