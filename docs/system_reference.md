# System Reference Manual

This document consolidates the consensus, networking, storage, marketplace, governance, security, and tooling notes that previously lived across dozens of Markdown files. Every section points back to the exact modules that implement the behaviour so engineers and agents can cross‑check details against code before shipping changes.

---

## 1. Consensus, Serialization, and Sharding

### 1.1 Genesis history, invariants, and test vectors

- The compile‑time genesis block is derived in `node/src/hash_genesis.rs`. Every field of `BlockEncoder` is zeroed except the fixed difficulty (`8`) and subsidy buckets. `calculate_genesis_hash()` is invoked inside a `const` context so `GENESIS_HASH` is embedded during compilation and asserted at runtime (`node/src/consensus/mod.rs`).
- Invariants:
  - `prev`, `fee_checksum`, and `state_root` point to `ZERO_HASH`.
  - All subsidy fields (`storage_sub`, `read_sub_*`, `compute_sub`) plus advertiser counters are zero to guarantee deterministic coinbase accounting.
  - `read_root`, `vdf_commit`, `vdf_output` are 32‑byte zero arrays; `vdf_proof`, `tx_ids`, and `l2_*` arrays are empty.
- Test vectors: `node/src/hash_genesis.rs` exposes `calculate_genesis_hash_runtime()` and a regression test that ensures runtime and compile‑time hashes match. `node/tests/light_sync.rs` ships a minimal light‑client genesis header fixture to validate sync and PoH replay.

### 1.2 PoH and VDF parameters

- PoH ticks are generated by `node/src/poh.rs`. Each tick is a BLAKE3 hash of the previous tick; optional GPU acceleration piggybacks on the compute workloads.
- `timestamp_ticks` for mempool ordering and purge loops are recorded in nanoseconds (`SystemTime::now().duration_since(UNIX_EPOCH).as_nanos()`, see `node/src/lib.rs` around the mempool admission path). Block timestamps continue to use milliseconds for ledger compatibility.
- VDF settings live in `node/src/config.rs::InflationConfig`. Defaults:
  - `vdf_kappa = 1 << 28` rounds (clamped via `set_vdf_kappa` when loading inflation files).
  - Hybrid PoW/PoS difficulty retune feeds the VDF by calling `node/src/consensus/vdf.rs`.
- Checkpoints:
  - Macro blocks emit every 100 micro‑blocks by default (`macro_interval` in `node/src/lib.rs`), anchoring shard roots and VDF outputs.
  - Replay harnesses (`tests/poh.rs`, `demo.py`) enforce deterministic replay by recomputing every tick and verifying `vdf_commit`, `vdf_output`, and proof bytes.
- Replay constraints: verifiers must recompute ticks sequentially, verify VDF proofs (currently sequential hash stand‑ins), and reject headers where `timestamp_ticks` drifts from the PoH timeline.

### 1.3 DKG specifics

- Implementation lives in the standalone `dkg/` crate. `SecretKeySet::random()` generates a deterministic polynomial seed per committee; shares authenticate themselves by XORing the digest with a derived per‑participant token.
- Rotation schedule: `node/src/dkg.rs::run(participants, threshold)` is called by the consensus stack whenever validator sets rotate (macro‑block checkpoints plus governance toggles). `DKG_ROUND_TOTAL` tracks successful rounds so operators can confirm every rotation via telemetry.
- Transcript materials:
  - Each participant keeps a `SecretKeyShare` (`id`, `seed`).
  - Aggregators collect `(participant_id, SignatureShare)` pairs, verify them via `combine_signatures`, and publish the resulting group signature/state inside sled.
  - Failure modes include `NotEnoughShares`, `InvalidShare`, and `MismatchedShares`; all map to RPC/CLI errors and increment telemetry counters for audit.
- Rotations reuse existing seeds to keep transcripts lightweight while the in‑house threshold scheme is finalized. The temporary design keeps transcripts in memory and never writes share material to disk.

### 1.4 Hash layout

`node/src/hashlayout.rs` lists the canonical order for block fields. The encoding still uses legacy names (`coin_c`, `coin_i`) but they represent BLOCK‑denominated buckets, not separate tokens.

| Order | Field | Description |
| --- | --- | --- |
| 1–7 | `index`, `prev`, `timestamp`, `nonce`, `difficulty`, `retune_hint`, `base_fee` | Header core; `retune_hint` is a signed byte summarising the Kalman filter trend. |
| 8–9 | `coin_c`, `coin_i` | Legacy coinbase labels (BLOCK). |
| 10–16 | `storage_sub`, `read_sub`, `read_sub_viewer`, `read_sub_host`, `read_sub_hardware`, `read_sub_verifier`, `read_sub_liquidity` | Read/storage subsidy buckets (BLOCK). |
| 17–23 | `ad_viewer`, `ad_host`, `ad_hardware`, `ad_verifier`, `ad_liquidity`, `ad_miner`, `ad_total_usd_micros` | Ad settlement buckets + USD micros counter. |
| 24–25 | `ad_settlement_count`, `ad_oracle_price_usd_micros` | Ad settlement counts + oracle price. |
| 26–27 | `compute_sub`, `proof_rebate` | Compute subsidy + proof rebate (BLOCK). |
| 28–30 | `read_root`, `fee_checksum`, `state_root` | Merkle roots; `fee_checksum` protects the per‑block fee accumulator. |
| 31–32 | `l2_roots`, `l2_sizes` | Variable‑length arrays; order is encoded as bytes then sizes. |
| 33–36 | `vdf_commit`, `vdf_output`, `len(vdf_proof)`, `vdf_proof` | PoH/VDF proof material. |
| 37–38 | `len(receipts_serialized)`, `receipts_serialized` | Receipts are consensus‑critical and hashed before transactions. |
| 39 | `tx_ids` | Final section: transaction IDs. |

Never reorder or remove fields; instead add new suffix fields and bump the hash layout tests. If you add or rename a field in `BlockEncoder`, update this table and regenerate schema docs under `docs/spec/`.

### 1.5 Schema migrations (v7–v10)

| Version | Scope | Pre‑migration state | Post‑migration state | Operator expectations |
| --- | --- | --- | --- | --- |
| v7 | Recent timestamp journaling (`Blockchain::recent_timestamps`) | Mempool entries only tracked `timestamp_millis`; restarts lost ordering guarantees. | `timestamp_ticks` persisted alongside millis so QoS lanes continue after restarts (`node/tests/reopen.rs`). | Allow the node to reopen once so it can rewrite mempool entries; no manual action beyond monitoring `telemetry::STARTUP_TTL_DROP_TOTAL`. |
| v8 | Bridge header persistence (`simple_db::names::BRIDGE`) | Bridge proofs not materialised in sled; challenge windows could not replay after crash. | `simple_db::names::BRIDGE` stores verified headers and pending withdrawals; CLI exposes `contract-cli bridge pending | disputes`. | Run the node once per bridge shard after upgrading; watch `bridge_head_commit_total`. |
| v9 | DEX escrow snapshots (`simple_db::names::DEX_STORAGE`) | Escrow state stored only in memory; C/R flows risked desync. | `simple_db::names::DEX_STORAGE` now stores `EscrowState` (orders, locks, HTLC proofs). | Before upgrading, pause matching, run `contract-cli dex escrow export`; after upgrade confirm `dex.order_book` RPC matches expectations. |
| v10 | Unified subsidy buckets | Legacy dual‑token labels removed from on‑disk snapshots; subsidies now serialize as `storage_sub`, `read_sub`, `compute_sub` with read/ad splits preserved. | Ledger + RPC surfaces report unified BLOCK values; lane routing stays in mempool/fee policy. | Update dashboards to use unified metric names and re-run settlement audits to confirm invariants. |

---

## 2. Transactions, Fees, and Mempool

### 2.1 Account abstraction specifics

- Handle registry (`node/src/identity/handle_registry.rs`) replaces the former `account_abstraction.md`: handles normalize via NFKC, reject `sys/` & `admin/` prefixes, and require strictly increasing nonces. Records are stored under `simple_db::names::IDENTITY_HANDLES`.
- Fee hooks (`node/src/fees.rs`, `node/src/fee/policy.rs`) enforce percentile‑based fee floors per lane. Wallets and RPC clients read `FeeLane` metadata to decide whether to dip into rebates before BLOCK.
- When submitting transactions:
  - Signature threshold vs signer set is checked before admission; failing multisig payloads are staged in `pending_multisig`.
  - Session keys (account‑level) live in sled with expiration timestamps so headless clients can register ephemeral signing keys.

### 2.2 Base fee algorithm

`node/src/fees.rs::next_base_fee` implements an EIP‑1559‑style update:

```
Δ = prev * (gas_used - target) / target / 8
next = max(prev + Δ, 1)
```

- `TARGET_GAS_PER_BLOCK = 1_000_000`.
- `Δ` is clamped by dividing by 8 (12.5 % per block). The base fee never drops below 1 (micro‑BLOCK per byte).
- Telemetry: `base_fee` gauge plus `fee_floor_current` and `fee_floor_warning_total` / `fee_floor_override_total` counters show nudges per block. `node/tests/base_fee.rs` exercises the path.

### 2.3 QoS accounting, TTL math, and RPC surfaces

- Admission + per‑sender limits (`node/src/mempool/admission.rs`): `AdmissionState::reserve_sender` tracks outstanding slots keyed by `(sender, lane)`. Defaults:
  - `max_pending_per_account = 16` (overrideable via `contract-cli node mempool set-cap` or `TB_MEMPOOL_ACCOUNT_CAP`).
  - `max_mempool_size_{consumer,industrial} = 1_024` entries each.
  - `min_fee_per_byte_{consumer,industrial}` defaults to 1 µBLOCK/byte but is immediately clamped to the rolling floor.
- Rolling floor: every admitted transaction records its `fee_per_byte`. The percentile window is configured by governance (`fee_floor_window`, `fee_floor_percentile`) so QoS can insist on e.g. “p75 of the last 512 admits.” When callers post `mempool.qos_event` we log overrides (`fee_floor_warning_total` / `fee_floor_override_total`).
- TTL ordering: `MempoolEntry::expires_at = timestamp_millis + (tx_ttl * 1000)` (`node/src/lib.rs:806–834`). `mempool_cmp` sorts by:
  1. `tip / serialized_size` (descending),
  2. soonest expiry,
  3. transaction hash.
  Any entry whose `now - timestamp_millis > tx_ttl * 1000` is dropped during `purge_expired()`, contributing to `TTL_DROP_TOTAL`.
- Lane rotation: consumers and industrial transactions live in separate `DashMap`s but are merged before block assembly (`Blockchain::mine_block_with_ts`). Because both lanes share the same comparator they naturally interleave when fees are comparable, yet state accounting stays lane-aware via `FeeLane` and `fee::decompose`.
- Deferred nonces: while iterating the merged list we only include a sender’s next expected nonce. Higher nonces accumulate in `deferred` until the gap closes, preventing starvation from users who pre-sign long sequences.
- Tx errors: `TxAdmissionError` carries the numeric code returned to RPC clients. Relevant constants live near `node/src/lib.rs:6238`. The table below helps map RPC failures to on-chain causes.

| Error | Code (`TxAdmissionError::code`) | Typical RPC code | Notes |
| --- | --- | --- | --- |
| `FeeTooLow` | 10 | `error_value("FeeTooLow")` | Floor check failed – inspect `mempool.stats.fee_floor`. |
| `MempoolFull` | 11 | `error_value("MempoolFull")` | Both lanes at capacity; eviction refused. |
| `PendingLimitReached` | 13 | `error_value("PendingLimitReached")` | Sender already has `max_pending_per_account` entries. |
| `PendingSignatures` | 17 | `status="pending_signatures"` | Multisig payload staged in `pending_multisig`. |
| `SessionExpired` | 18 | `error_value("SessionExpired")` | Session keys (CLI `--session`) expired; renew before resubmitting. |
| `UnknownSender`, `NonceGap`, `InsufficientBalance`, … | See `node/src/lib.rs` | `error_value("<Variant>")` | Surfaced directly from the admission path. |

- QoS telemetry: `submit_tx` increments `TX_SUBMITTED_TOTAL`; successful admissions increment `TX_ADMITTED_TOTAL` and log the caller’s jurisdiction for compliance. Rejections emit `tx_rejected_total{reason}` samples so dashboards can correlate floods.

#### `mempool.stats` payload

RPC `mempool.stats` (`node/src/rpc/mod.rs:1445`) reads the current lane snapshot without mutating the pool. Each field is in milliseconds or micro‑BLOCK as noted below.

| Field | Meaning |
| --- | --- |
| `size` | Number of entries in the requested lane. |
| `age_p50` / `age_p95` | Milliseconds since admission (median / 95th percentile). |
| `fee_p50` / `fee_p90` | Raw `tip` percentiles (µBLOCK). Divide by serialized size for fee per byte. |
| `fee_floor` | Current percentile floor after min‑fee clamps. |

Example:

```json
{
  "size": 742,
  "age_p50": 1184,
  "age_p95": 5443,
  "fee_p50": 1200,
  "fee_p90": 3500,
  "fee_floor": 900
}
```

When consumer latency jumps above `comfort_threshold_p90` (governance parameter) the scheduler raises the base fee which in turn bumps lane floors; watch `fee_floor_current` and `mempool.stats.fee_floor` for confirmation.

### 2.4 Transaction scheduler, fairness windows, and conflict guards

- `node/src/scheduler.rs` drives the service pipeline. `ServiceScheduler` owns three classes (`gossip`, `compute`, `storage`) with weights exposed as governance parameters (`scheduler_weight_gossip`, `scheduler_weight_compute`, `scheduler_weight_storage`). `governance::registry` plumbs new values directly into the atomic defaults so the change is visible to the next block without restarting.
- Pick logic: in re‑entrant builds we maintain `(current_class, budget, last_idx)`. Each dequeue consumes one unit of the class budget; once the budget hits zero or the queue empties we rotate to the next non‑empty class. In non‑re‑entrant builds we rotate a ring buffer of `(class, queue_len)` pairs so large bursts cannot starve lower-weight lanes.
- Batch size: callers (e.g., the gossip reactor or execution pipeline) pass `limit` into `drain(limit)` or `execute_ready(limit)`. A typical block assembly requests `limit = dynamic_block_limit()`; gossip workers usually ask for 64 messages. Because the scheduler enforces per-class weights the effective fairness window equals `weight` dequeues before the class yields to its peers.
- Starvation guards: `ServiceSchedulerStats` exposes `queue_depths` + `weights` via diagnostics so operators can confirm configuration. If a class enters `reentrant_enabled` mode with weight zero its queues will still be scanned periodically to avoid livelock. For transaction execution, `TxScheduler` builds read/write sets (`TxRwSet`) and refuses to schedule transactions whose inputs or outputs overlap an active run (“RW conflict”). Conflicts are surfaced as `ScheduleError::Conflict(txid)` so block builders can defer the losing transaction without discarding it.
- Governance knobs: to bias toward consumer traffic during incidents set `scheduler_weight_gossip=4`, `scheduler_weight_compute=1`, `scheduler_weight_storage=1`, then monitor `scheduler.stats` RPC to ensure queue depths drop as expected. `contract-cli diagnostics scheduler` prints the same struct locally.

---

## 3. Networking, Overlay, and P2P

### 3.1 Wire protocol catalog

`node/src/net/message.rs` defines signed envelopes plus the payloads below (see Appendix D for field tables). Compatibility notes:

- `Payload::Handshake(Hello)` negotiates `proto_version`, feature bits, transport capabilities, and QUIC certificate fingerprints (`p2p/handshake.rs`).
- `Hello(Vec<SocketAddr>)` is the legacy peer advertisement message; it remains for bootstrap compatibility.
- `Tx`, `BlobTx`, `Block`, `Chain` propagate consensus data. Transactions include full serialization so mempools can revalidate.
- `BlobChunk` carries erasure‑coded shards with `index`/`total` metadata; peers deduplicate by root.
- `Reputation(Vec<ReputationUpdate>)` syncs peer scores between overlay nodes.

### 3.2 QUIC handshake and fallback rules

- Hello extensions (`p2p/handshake.rs`):
  - `quic_provider`: optional provider ID; resolved to Quinn, s2n‑quic, or the in‑house transport at runtime.
  - `quic_capabilities`: vector of feature strings (`certificate_rotation`, `mtls`, etc.).
  - `quic_cert` + `quic_fingerprint{,_previous}`: remote certificate payload and rolling fingerprints.
- Validation flow:
  1. Ensure `network_id` and `proto_version` match (`SUPPORTED_VERSION = 1`).
  2. Check required feature bits (governance toggles). Missing bits ➜ `HandshakeError::MissingFeatures`.
  3. If the peer advertises QUIC, infer the provider, verify the cert (in‑house adapter for s2n), and compare fingerprints. Failures increment `QUIC_HANDSHAKE_FAIL_TOTAL{peer,reason}`.
  4. Replay protection: accepted peers are recorded in `PEERS` (lazy mutex). `peer_provider`/`peer_capabilities` surfaces this cache for diagnostics.
- Fallbacks: if QUIC is unavailable or misconfigured, the node retains TCP transport and advertises `Transport::Tcp`.

### 3.3 A* heuristics, swarm presets, and bootstrap

- `node/src/net/a_star.rs` caches ASN latency floors with an LRU of configurable capacity. Heuristic = floor latency + `μ * (1 - uptime)`, where `μ` defaults to `governance::heuristic_mu_milli / 1000`.
- Swarm/bootstrap:
  - Static peers via `TB_BOOTSTRAP_PEERS` or CLI seeds; overlay peer stores live in `p2p_overlay`.
  - Gossip mesh uses `TB_PARTITION_TAG` to segregate drill traffic; range-boost peers are configured via `TB_MESH_STATIC_PEERS`.
- Topologies:
  - Core swarm: fanout picks peers with lowest heuristic, bounded by shard affinity.
  - Range Boost: `node/src/range_boost/mod.rs` maintains TLS/Unix/TCP peer latency caches, resynchronises forwarder threads, and enqueues bundles with exponential backoff.

### 3.4 Partition detection runbook

- `PartitionWatch` (`node/src/net/partition_watch.rs`) tracks unreachable peers; default threshold = 8.
- When `mark_unreachable` pushes the count past the threshold, `PARTITION_EVENTS_TOTAL` increments and `active=true`. Operators should:
  1. Query `contract-cli net partition` (via diagnostics) to list isolated peer IDs.
  2. Inspect `partition_watch.current_marker()` to correlate with dashboards/incident logs.
  3. Run the recovery recipe: flush peer bans, reseed overlay via CLI, confirm `is_partitioned()` flips back to false.

### 3.5 Overlay store and peer persistence

- Backends: `node/src/config.rs` exposes `overlay.backend = inhouse | stub`. The in-house backend (`crates/p2p_overlay::InhouseOverlay`) persists peers under `TB_OVERLAY_DB_PATH` (defaults to `~/.the_block/overlay`). Records are pretty‑printed JSON containing the base58 peer ID, socket address, and `last_seen` epoch seconds. The stub backend (memory only) is useful for deterministic tests.
- Discovery + RPC selection: `node/src/net/discovery.rs` re‑exports the trait so CLI/RPC consumers can issue `net.overlay_status` and `contract-cli net overlay-status --format json`. Switching backend at runtime is safe; `net::overlay_service()` holds the chosen implementation in a read/write lock, and `governance::registry` wires up policy changes.
- Migration: `scripts/migrate_overlay_store.rs` converts legacy bincode sled trees into the new JSON format. Invoke it before flipping `overlay.backend` on existing nodes so peer histories are retained.
- Peer metrics store: `node/src/net/peer_metrics_store.rs` writes compressed records into `sled` (`peer_metrics` tree). Values are encoded via `peer_metrics_binary::{encode, decode}` (compact bincode) and keyed by `<peer-pk><timestamp>` which enables retention pruning. `peer.metrics_export` CLI/RPC commands also read from this tree.
- Ban store: `node/src/net/ban_store.rs` keeps bans in `SimpleDb::open_named(names::NET_BANS, TB_BAN_DB)`. Entries are hex peer IDs with an 8‑byte timestamp payload; `ban_store::purge_expired()` enforces TTL and updates `BANNED_PEERS_TOTAL`.
- Ancillary stores: `TB_PEER_DB_PATH` and `TB_QUIC_PEER_DB_PATH` house the bincode peer reputation cache (`peer.rs`), `TB_PEER_METRICS_PATH` controls on-disk export, and `TB_NET_CERT_STORE_PATH` stores mutual-TLS cert chains. Back them up together when migrating a node.

---

## 4. LocalNet, Range Boost, and Headless/AI Tooling

### 4.1 Range-boost queue semantics

- Queue is a `VecDeque<QueueEntry>` guarded by a mutex; every enqueue records `enqueued_at`.
- Configurable fault modes (`FaultMode::{ForceDisabled,ForceNoPeers,ForceEncode,ForceIo}`) inject chaos for drills.
- Courier/forwarder loop:
  - Sleep durations: idle = 200 ms, retry = 250 ms, disabled = 1 s.
  - Forward failures requeue the bundle at the front to preserve FIFO semantics.
  - Metrics: `RANGE_BOOST_QUEUE_DEPTH`, `RANGE_BOOST_QUEUE_OLDEST_SECONDS`, `RANGE_BOOST_FORWARDER_FAIL_TOTAL`, `MESH_PEER_LATENCY_MS`.

### 4.2 LocalNet proximity proofs

`node/src/localnet/proximity.rs` + `config/localnet_devices.toml` define class‑specific RSSI/RTT envelopes:

| Device class | Minimum RSSI | Maximum RTT |
| --- | --- | --- |
| Phone | −75 dBm | 150 ms |
| Laptop | −80 dBm | 200 ms |
| Router | −85 dBm | 250 ms |

`LocalNet::validate_proximity` returns `false` for out‑of‑range readings; callers include RPC methods (`localnet.submit_receipt`) and diagnostics tools. Receipts (`AssistReceipt`) embed provider, region, device class, RSSI, RTT, and Ed25519 signatures hashed via BLAKE3 for anchoring.

### 4.3 Headless and AI diagnostics

- CLI (`cli/src/ai.rs`):
  - `contract-cli ai diagnose --snapshot metrics.json` loads a JSON blob, evaluates heuristics (currently latency thresholds), and prints remediation tips.
  - `Metrics` struct contains `avg_latency_ms`; extend it as headless tooling matures.
- Governance parameter `ai_diagnostics_enabled` (see §10) gates ANN‑based alerts in node + aggregator.
- Headless flows share the same JSON codec helpers as RPC so suggestions are deterministic and auditable.

---

## 5. Storage, State, SimpleDb, and Persistence

### 5.1 Pipeline stages and knobs

`node/src/storage/pipeline.rs` stages:

| Stage | Knobs / defaults | Effect |
| --- | --- | --- |
| Chunk sizing | `settings::chunk_defaults()` (default 1 MiB with ladder `[256 KiB, 512 KiB, 1 MiB, 2 MiB, 4 MiB]`). | `clamp_to_ladder` adjusts chunk size to hit 3 s target writes. |
| Compression | `settings::compression_level()` and algorithm selection; fallback/per‑algorithm telemetry via `record_coding_result`. | Chooses compressor (e.g., zstd, RLE fallback) per manifest. |
| Encryption | `settings::encryptor()` or per‑manifest override. | Wraps chunk bytes before erasure coding. |
| Erasure coding | `settings::erasure_counts()` plus `ErasureParams`. | Default Reed–Solomon with fountain overlay (see §5.2). |
| Placement | `storage::placement::{NodeCatalog, NodeHandle}`. | Selects providers respecting rent escrow, redundancy, RTT telemetry. |
| Repair | `storage::repair` monitors health, reissues pulls, updates readiness metrics. |

### 5.2 Erasure coding parameters

- Default (governance overrideable): RS data/parity counts from `settings::erasure_counts()`; overlay adds 3 fountain shards over the first 2 RS shards.
- Failure tolerance: RS handles up to `parity_shards` losses; overlay can reconstruct early losses (`try_fill_from_overlay`) even when the first two shards vanish.
- Encoding/decoding record success/failure under `erasure_encode` and `erasure_reconstruct` metrics; any fallback/enforced coder is logged as an emergency event if `TB_CODING_FALLBACK_EMERGENCY_*` envs are set.

### 5.3 Storage-market incentives

- `storage_market/src/lib.rs` models each contract as `ContractRecord { contract: StorageContract, replicas: Vec<ReplicaIncentive> }`.
- `ReplicaIncentive` fields: `allocated_shares`, `price_per_block`, `deposit`, success/failure counters, last proof block, last outcome.
- SLAs recorded via `record_proof_outcome`:
  - Success ➜ increment `proof_successes`, no slashing.
  - Failure ➜ deduct `min(price_per_block, deposit)` and increment `proof_failures`.
- CLI/RPC:
  - `storage_market.provider_balances`, `storage_market.audit_log`, and explorer views show deposits, accrued BLOCK, and slashes.

### 5.4 SimpleDb keyspaces

SimpleDb uses named column families (CFs) declared in `node/src/simple_db/mod.rs::names`. See Appendix C for the complete cf→subsystem map and prefix conventions.

### 5.5 WAL, snapshots, and pruning

- WAL: in-house engine writes JSON lines per CF at `<cf_path>/wal.log` (`crates/storage_engine/src/inhouse_engine.rs`). After each memtable flush the WAL is truncated to keep replay bounded. Operators can force a flush via `SimpleDb::flush_wal()`.
- Ledger snapshots:
  - `node/src/blockchain/snapshot.rs` writes account snapshots plus diffs under `<data_dir>/snapshots/<height>.bin`.
  - CLI helper (`node/src/bin/snapshot.rs`) supports `snapshot create|apply|list`.
  - State snapshots (`state/src/snapshot.rs`) capture trie contents, keep the last `keep` images, and log engine migrations.
- Pruning: `SnapshotManager::prune()` sorts snapshots by mtime, retains `keep`, and deletes the rest. Set `TB_SNAPSHOT_KEEP` to change the retention window.

### 5.6 Manifest + `StoreReceipt` layout

- Binary schema lives in `node/src/storage/manifest_binary.rs`. Each object encodes:
  - Core fields: `version`, `total_len`, `chunk_len`, redundancy (`Redundancy::ReedSolomon { data, parity }` or `None`), encrypted content key, and BLAKE3 digest.
  - Chunk metadata: plaintext/compressed/cipher lengths plus `ChunkRef { id, nodes, provider_chunks[] }` so repair can fall back to either manifest-level providers or per-chunk overrides.
  - Provider hints: `provider_chunks` map providers to the chunk indices they host and carry per-provider encryption keys when overrides are enabled.
- Receipts (`storage::types::StoreReceipt`) mirror the manifest hash, chunk count, redundancy enum, and lane label (`consumer` or `industrial`). They are serialized via the same helper and embedded into coinbase accounting so explorers can correlate manifests with payouts.

### 5.7 Rent escrow, releases, and proofs

- `RentEscrow` (`node/src/storage/fs.rs`) persists deposits in `SimpleDb::open_named(names::STORAGE_FS, TB_STORAGE_PIPELINE_DIR)`; default path is `blobstore` when the env var is unset. API summary:
- `lock(id, depositor, amount, expiry)` writes a record and increments `RENT_ESCROW_LOCKED_TOTAL`.
- `release(id)` removes the record, refunds 90 % to the depositor, burns 10 % (tracked via `RENT_ESCROW_REFUNDED_TOTAL`/`RENT_ESCROW_BURNED_TOTAL`), and returns `(account, refund, burn)`.
  - `purge_expired(now)` sweeps stale locks and emits identical telemetry. Expiry checks happen before every rent escrow lookup so operators only need to call it when clocks skew.
- Receipts tie escrows to manifests: `storage::pipeline` pre-funds rent via escrow IDs, and the settlement engine drops a receipt only when both manifest and escrow landed on disk. Explorer endpoints display the rent escrow balance alongside the `StoreReceipt`.

### 5.8 Provider profile schema and telemetry

- Profiles (`node/src/storage/pipeline.rs::ProviderProfile`) record the EWMA throughput, RTT, and loss history per provider together with rolling success rates and maintenance flags. They ensure we down-rank providers whose recent uploads failed.
- Binary encoding lives in `node/src/storage/pipeline/binary.rs`: 13 fixed fields captured as `f64`/`u32`/`u64` triplets. The profile snapshot (`ProviderProfileSnapshot { provider, profile, quota_bytes }`) drives explorer dashboards and CLI dumps.
- Governance knobs indirectly influence profiles: chunk ladder defaults, erasure parameters, and rent escrow requirements all change the amount of traffic a provider sees, which in turn updates the EWMA. `storage.provider_profiles` RPC returns the serialized profile objects for post-processing.

### 5.9 Storage importer and audit CLI

- Legacy manifests still exist under `storage_market/legacy/`. `storage_market/src/importer.rs` provides an ergonomic wrapper around that filesystem for audits and replay:
  - `contract-cli storage importer audit --dir <market>` scans the pending/migrated manifest directories, samples up to 32 entries (`AUDIT_SAMPLE_LIMIT`), and reports duplicates/missing keys.
  - `contract-cli storage importer rerun --overwrite` replays a manifest into the sled `market/contracts` tree. Modes: `InsertMissing` (default) or `OverwriteExisting`. Use `--dry-run` to see the counts first.
  - `contract-cli storage importer verify --scope {contracts,all}` compares manifest checksums (deterministic hash over `(<cf>, key, value)` pairs) with the live database.
- All importer commands honor `--allow-absent` so CI can skip legacy steps when the manifests were already migrated. Reports can be written to disk (`--out report.json`) and are encoded with `foundation_serialization::json` for reproducibility.

---

## 6. Compute Marketplace

### 6.1 Courier retries and resume semantics

- `CourierStore::flush` (sync) and `flush_async` attempt up to five deliveries per receipt. Backoff starts at 100 ms and doubles (100 → 200 → 400 → 800 → 1 600 ms). Failures increment `COURIER_FLUSH_FAILURE_TOTAL`.
- Receipts persist in a sled tree (`courier` namespace). On restart, the flush loop replays unacknowledged receipts; `acknowledged=true` entries are left untouched for audit until the queue is compacted.
- Capability gating: `send_for_capability` checks scheduler inventory before enqueuing work, preventing overload on providers that lack the requested hardware.

### 6.2 SNARK receipts

- `node/src/compute_market/receipt.rs` fields: `(version, job_id, buyer, provider, quote_price, units, dry_run flag, issued_at, idempotency_key, FeeLane)`.
- `idempotency_key` is BLAKE3(`job_id | buyer | provider | price | units | version | lane`), guaranteeing deduplicated settlement entries even if retries occur.
- SNARK receipts now embed a `ProofBundle` produced by `node/src/compute_market/snark.rs`. The helper wraps the Groth16 backend, hashes wasm bytes into circuit digests, caches compiled circuits per digest, selects CPU or GPU provers automatically, records telemetry (`snark_prover_latency_seconds{backend}`, `snark_prover_failure_total{backend}`), and emits bundles containing the circuit/output/witness commitments plus serialized proof bytes.
- Each proof bundle includes a `CircuitArtifact { circuit_hash, wasm_hash, generated_at }`, allowing offline re-verification and matching against CLI/explorer exports without re-running compilation.
- CLI support lives under `cli/src/snark.rs`. `snark compile` now writes attested circuit artifacts (digest + wasm hash + timestamp) so operators can cache proving parameters offline, and `contract-cli compute proofs --limit N` streams the latest `compute_market.sla_history` proofs (fingerprint, backend, commitments, artifact metadata) for audits.
- Settlement persists every accepted proof via `Settlement::record_proof`, retaining the full vector of bundles per SLA and exposing them through `compute_market.sla_history`. `contract-cli explorer sync-proofs` writes the same `Vec<ProofBundle>` blobs into the explorer tables (`compute_sla_history`, `compute_sla_proofs`), and the explorer HTTP route `/compute/sla/history?limit=N` re-exports the decoded fingerprints/artifacts for dashboards and auditors.

### 6.3 SLA slashing and dashboards

- Settlement engine (`node/src/compute_market/settlement.rs`) maintains:
  - `SettleMode` (`DryRun`, `Armed`, `Real`) toggled via governance or CLI.
  - `SlaRecord { job_id, provider, buyer, provider_bond, consumer_bond, deadline, scheduled_at, proofs: Vec<ProofBundle> }`.
  - `SlaResolution { outcome: Completed | Cancelled { reason } | Violated { reason }, burned, refunded, proofs }`.
- Automation:
- Violations call `SlaOutcome::Violated { reason, automated }`, burn the lesser of the bond and configured slash amount (`COMPUTE_SLA_AUTOMATED_SLASH_TOTAL`, `SLASHING_BURN_TOTAL`).
  - Dashboards plot `COMPUTE_SLA_PENDING_TOTAL`, `COMPUTE_SLA_VIOLATIONS_TOTAL`, and `COMPUTE_SLA_NEXT_DEADLINE_TS`.
- Operator workflow:
  1. Arm settlement via `compute_arm_real` RPC, wait for the `activate_at` block height, then advance to `SettleMode::Real`.
  2. Watch aggregator dashboards for SLA thresholds; use `contract-cli compute settlement` to inspect queued records.
  3. Use the courier appendix (below) when diagnosing stuck carry-to-earn flows and pull `compute_market.sla_history` when auditing proof material alongside SLA outcomes.

### 6.4 Lane-aware matcher semantics

- `node/src/compute_market/matcher.rs` maintains per-lane order books. Each `LaneState` carries `LaneMetadata { fairness_window, max_queue_depth }`. Defaults come from env (`TB_COMPUTE_FAIRNESS_MS`, `TB_COMPUTE_LANE_CAP`); fairness defaults to 5 ms, capacity to 1 024 entries.
- `match_batch(batch)` snapshots the set of lanes, then loops:
  1. Iterate lanes in order, skipping empty queues.
  2. While `matched.len() < batch`, peek `front()` bid/ask. If the bid price covers the ask price the pair is popped and recorded; otherwise break.
  3. If the lane already matched once during this pass and `Instant::now() > fairness_deadline`, we break early so other lanes get time.
  4. Set `state.last_match_at` and clear starvation warnings whenever progress occurs.
- Batch size defaults to `TB_COMPUTE_MATCH_BATCH` (32). After every full pass we stop if no progress was made; otherwise we loop until `matched.len() == batch`.
- Starvation guard: `collect_starvation(Duration::from_secs(TB_COMPUTE_STARVATION_SECS))` inspects queue heads and records `LaneWarning { lane, oldest_job, waited_for, updated_at }` whenever a job waits longer than the threshold (default 30 s). RPC `compute_market.stats` exposes these warnings so operators can alert on stuck jobs.

### 6.5 Price board, backlog, and units

- `node/src/compute_market/price_board.rs` keeps a sliding window of `(price, weighted_price)` entries for each lane. `record_price` appends entries, `bands(lane)` returns `(min, weighted_median, max)`, and `spot_price_per_unit` falls back to raw medians when weighted samples are unavailable.
- `backlog_utilization()` summarises outstanding demand (`industrial_backlog`) and realised throughput (`industrial_utilization`). Both figures appear in `compute_market.stats` and the CLI.
- Workload normalisation lives in `node/src/compute_market/workload.rs` (`ComputeUnits`, `compute_units(data)`, `calibrate_gpu`). Every workload is expressed in units per second so bids/asks remain comparable regardless of raw byte size.

### 6.6 Scheduler metrics RPC

- `scheduler.metrics` (`node/src/rpc/compute_market.rs`) simply forwards the JSON from `scheduler::metrics()` which contains `reputation` (provider score map) and `utilization` (capability → units consumed). Use `contract-cli compute scheduler metrics --json` for scripting.
- `scheduler.stats` returns a richer struct (`SchedulerStats`): outcome counters (`success`, `capability_mismatch`, `reputation_failure`), queue depths per priority (`queued_high/normal/low`), pending jobs with effective priority, and preemption counters.
- Together with `compute_market.stats` these RPCs explain why a job failed to match (e.g., reputation failure vs lack of capability) without scraping node logs.

### 6.7 Energy market (providers, credits, receipts)

- **Core structs** (`crates/energy-market/src/lib.rs`):
  - `EnergyProvider { provider_id, owner, location, capacity_kwh, available_kwh, price_per_kwh, reputation_score, meter_address, total_delivered_kwh, staked_balance, last_fulfillment_latency_ms, last_meter_value, last_meter_timestamp }`.
  - `MeterReading { provider_id, meter_address, total_kwh, timestamp, signature }` with a `hash()` helper (BLAKE3 over provider/meter/kWh/timestamp/signature len + bytes) that becomes the credit key.
  - `EnergyCredit { amount_kwh, provider, timestamp, meter_reading_hash }`, `EnergyReceipt { buyer, seller, kwh_delivered, price_paid, block_settled, treasury_fee, meter_reading_hash, slash_applied }`, and `EnergyMarketConfig { min_stake, treasury_fee_bps, ewma_alpha, jurisdiction_fee_bps, oracle_timeout_blocks, slashing_rate_bps }`.
  - `EnergyMarket` maintains provider + meter maps, pending credits, settled receipts, EWMA totals, and `next_provider_id`. Errors bubble through `EnergyMarketError` (provider exists, meter claimed, insufficient stake/capacity/credit, stale reading, invalid meter value, expired credit) and map 1:1 to RPC error strings.
- **Persistence & governance** (`node/src/energy.rs`):
  - Wraps the market in `SimpleDb::open_named(names::ENERGY_MARKET, TB_ENERGY_MARKET_DIR.unwrap_or("energy_market"))`. Every mutation serializes the full market via `EnergyMarket::to_bytes()` and writes it using SimpleDb’s fsync+rename discipline.
  - `GovernanceEnergyParams { min_stake, oracle_timeout_blocks, slashing_rate_bps }` lives in a `Lazy<Mutex<_>>`. `set_governance_params` updates the struct, applies it to the runtime config (`apply_params_to_market`), and persists the snapshot. Treasury fees/slashes flow into `NODE_GOV_STORE.record_treasury_accrual`.
  - `check_energy_market_health()` warns when `pending_credit_count()` exceeds 25 and logs settlement heartbeats so dashboards can alert without scraping additional RPCs.
- **Provider trust roots**:
  - `config/default.toml` exposes `energy.provider_keys = [{ provider_id = "...", public_key_hex = "..." }, …]`. Reloads invoke `node::energy::configure_provider_keys`, clearing and repopulating the sled-backed verifier registry so ops can rotate or revoke keys without restarts. Entries are authoritative—omitting a provider removes it from the registry and puts it back into shadow mode.
- **RPC + CLI** (`node/src/rpc/energy.rs`, `cli/src/energy.rs`):
  - RPC methods: `energy.register_provider`, `energy.market_state`, `energy.submit_reading`, `energy.settle`. Helpers enforce payload shape (`require_string`, `require_u64`, `decode_hash`, `decode_signature`) and emit canonical JSON for providers/credits/receipts.
  - CLI mirrors the RPC schema: `contract-cli energy register <capacity> <price> --meter-address … --jurisdiction … --stake … --owner …`, `contract-cli energy market [--provider-id … --verbose]`, `contract-cli energy submit-reading --reading-json '…'`, `contract-cli energy settle <provider> <kwh> --meter-hash … --buyer …`.
  - All tooling (oracle adapters, explorer, dashboards) reuse the schema documented in `docs/apis_and_tooling.md#energy-rpc-payloads-auth-and-error-contracts` so meter hashes/receipts stay byte-identical everywhere.
- **Oracle adapter + mock service** (`crates/oracle-adapter`, `services/mock-energy-oracle`):
- Adapter defines `SignatureVerifier`; the default (`Ed25519SignatureVerifier`) enforces signatures for every provider with a registered key. `MeterReadingPayload` implements `MeterReading` and exposes the canonical `signing_bytes()` digest (BLAKE3 of provider, meter, total kWh, timestamp) so verifiers can sign/verify consistently.
  - Mock service (in-house `httpd` router) listens on `MOCK_ENERGY_ORACLE_ADDR` (default `127.0.0.1:8080`), exposes `/meter/:id/reading` (increments totals by 250 kWh, updates timestamp, returns payload) and `/meter/:id/submit` (accepts posted readings). Used by `scripts/deploy-worldos-testnet.sh`.
- **Telemetry**:
  - Gauges: `energy_providers_count`, `energy_avg_price`. Counters: `energy_kwh_traded_total`, `energy_settlements_total{provider}`, `energy_signature_failure_total{provider,reason}`. Histograms: `energy_provider_fulfillment_ms`, `oracle_reading_latency_seconds`.
  - Logs: `node::energy::check_energy_market_health` warns on backlog spikes and settlement stalls. Metrics aggregator wiring (`/wrappers`, `/telemetry/summary`) plus Grafana panels (provider counts, pending credits, slash totals, settlement rates) are tracked in `docs/architecture.md#energy-governance-and-rpc-next-tasks`.
- **Governance linkage**:
  - `ParamKey::{EnergyMinStake, EnergyOracleTimeoutBlocks, EnergySlashingRateBps}` live in `governance/src/{lib.rs,codec.rs,params.rs}`. Runtime hooks clamp values before writing to `Params` and call `node::energy::set_governance_params`.
  - `node/tests/gov_param_wiring.rs` covers these round-trips; update it plus explorer/CLI timelines when adding new energy parameters (batch vs real-time settlement toggles, dependency graph validation, etc.).
- **Snapshot/restore**:
  - Use `TB_ENERGY_MARKET_DIR` to relocate the sled DB, snapshot it alongside other SimpleDb stores (§5.5), and restore during drills. `docs/testnet/ENERGY_QUICKSTART.md` and `scripts/deploy-worldos-testnet.sh` document the canonical register ➜ submit ➜ settle ➜ dispute workflow until dedicated dispute RPCs land.

---

## 7. DEX, Trust Lines, and HTLCs

### 7.1 AMM pool math

- Constant-product pools live in `dex/src/amm.rs` with `base_reserve` and `quote_reserve`.
- Share minting:
  - First LP: `share = sqrt(base * quote)`.
  - Subsequent LPs: `share = min(total_shares * base / base_reserve, total_shares * quote / quote_reserve)`.
- Swaps preserve `k = base_reserve * quote_reserve`; slippage is purely the constant-product curve (no extra fee yet). Governance can wrap this module to add fees later without redesigning the math.

### 7.2 Trust-line routing

- Ledger (`node/src/dex/trust_lines.rs`) keeps a `HashMap<(from,to), TrustLine { balance, limit, authorized }>`; `balance` is signed (positive means `from` owes `to`, negative means the reverse) and must stay within `limit`.
- Path finding:
  - BFS yields any viable path.
  - `find_best_path` uses Dijkstra for the shortest hop path, then `max_slack_path` to maximise residual capacity (slack = `limit - |balance| - amount`).
  - Fallback route excludes edges chosen by the slack path to provide a disjoint backup.
- `settle_path` applies debits/credits across hops atomically; failures roll back all adjustments.

### 7.3 HTLC proofs and replay constraints

- `dex/src/htlc_router.rs` keeps pending intents (`VecDeque<HtlcIntent { chain, amount, hash, timeout }`).
- Submit flow:
  1. Caller posts an intent; router matches against existing intents with identical hash+amount.
  2. Upon match, the router returns both intents plus deterministic scripts `htlc:<hash_hex>:<timeout>` for each chain. Scripts embed the hashed secret to avoid mismatched preimages.
- Replay protection: matched pairs are removed from the queue; repeated submissions with the same hash+amount/time window are rejected. Timeouts must be ordered (chain A < chain B) per best practices.

### 7.4 Escrow state machine and explorer payloads

- `dex/src/escrow.rs` implements a Merkle-based escrow ledger:
  - `EscrowEntry { from, to, total, released, payments, root, algo }` tracks aggregate state and supports both BLAKE3 and SHA3 hash commitments.
  - Every partial release appends to `payments`, recomputes the Merkle root, and returns `PaymentProof { leaf, path, algo }` so wallets can prove redemption without revealing unrelated payments.
  - When `released == total` the entry is deleted and `total_locked` drops accordingly (`dex_escrow_locked` gauge).
- Persistence and RPC:
  - Escrow tables live in sled (`dex::storage::DexStore`) and are exported via `node/src/dex/storage_binary.rs` (bincode helpers for order books, escrow snapshots, proofs).
  - RPC `dex_escrow_status`/`dex_escrow_release`/`dex_escrow_proof` expose the state machine; CLI mirrors them via `contract-cli dex escrow status|release|proof`.
- Explorer + metrics:
  - `GET /dex/order_book` and `GET /dex/trust_lines` return the indexed order book and trust-line graph, respectively.
  - Metrics `dex_escrow_locked` and `dex_escrow_pending` (see `node/src/telemetry.rs`) feed the default dashboards so operators can spot runaway collateral.

---

## 8. Bridges and Security

### 8.1 Header layout and proof encoding

- External chains share a canonical header schema (`bridges/src/header.rs::PowHeader`):
  - `chain_id` (string), `height` (u64), `merkle_root` (target tree), `signature` (BLAKE3 hash of prior fields), `nonce`, and difficulty `target`.
  - `hash_header` concatenates those bytes and feeds BLAKE3; `verify_pow` compares the 64-bit prefix against `target`.
- Light-client payloads (`bridges/src/light_client.rs`):
  - `Header { chain_id, height, merkle_root, signature }` plus `Proof { leaf, path[] }`.
  - `header_hash` is `BLAKE3(chain_id || height || merkle_root)` and must match `signature`.
  - Merkle paths are stored as `Vec<[u8;32]>`; verification always hashes `(acc || sibling)` in insertion order, so callers must supply the path exactly as emitted by the origin chain.
- Within the node:
  - Headers are persisted via `simple_db::names::BRIDGE_HEADERS` and replayed during startup.
  - RPC `bridge.verify_deposit` decodes `ExternalSettlementProof`, validates the Merkle path, then records a `DepositReceipt`.
  - Proof encoding lives in `bridge_types::settlement_proof_digest`; receipts carry the digest so explorers can hyperlink deposits ↔ withdrawals.

### 8.2 Challenge math, duty windows, and errors

- `PendingWithdrawal` (`node/src/bridge/mod.rs:389`) tracks `commitment`, `asset`, `user`, `amount`, `initiated_at`, `deadline = initiated_at + channel.config.challenge_period_secs`, and `challenged` flags. Default challenge window is 30 s, overrideable per channel.
- Duty timing:
  1. `bridge.request_withdrawal` locks the relayer bond (`bridge_min_bond`, sourced from governance `BridgeIncentiveParameters`) and schedules a duty (`DutyRecord`) with deadline `initiated_at + bridge_duty_window_secs`.
  2. Challengers call `bridge.challenge_withdrawal { commitment, challenger }`. If accepted, `BridgeError::AlreadyChallenged` prevents duplicates and `bridge_challenge_slash` refunds challengers after burning the bond.
  3. If no challenge arrives before `deadline`, `bridge.finalize_withdrawal` releases funds and credits `bridge_duty_reward`.
- Error surface (`BridgeError`):
  - `InvalidProof`, `SettlementProofHashMismatch`, `SettlementProofChainMismatch` return `-32071` style RPC codes.
  - `ChallengeWindowOpen` / `AlreadyChallenged` bubble to clients so they know whether to retry later.
  - Missing rows (`UnknownChannel`, `WithdrawalMissing`) map to `-32072`.
- Diagnostics: `bridge.pending_withdrawals`, `bridge.active_challenges`, `bridge.duty_log`, and explorer dashboards show live commitments, deadlines, and slashing events. Telemetry families `BRIDGE_CHALLENGE_TOTAL`, `BRIDGE_SETTLEMENT_RESULTS_TOTAL{result,reason}`, and `BRIDGE_DISPUTE_OUTCOMES_TOTAL{kind,outcome}` back the Grafana bridge row.

### 8.3 Risk register and audit playbooks

- `docs/security_and_privacy.md` consolidates the previous risk register:
  - Consensus: monitor QUIC stalls, DKG leaks.
  - Networking: peer DB corruption, overlay exhaustion.
  - Storage/Compute: erasure threshold violations, SLA slashing anomalies.
  - Governance: treasury drains, badge forgery attempts.
- Audit tooling:
  - Aggregator `/audit` endpoint stores incident logs.
  - `scripts/settlement_audit.rs`, `tools/settlement_audit`, and `contract-cli gov treasury audit` compare ledger projections vs settlement state.
  - Risk reviews must cite relevant sections here before patching code.

---

## 9. Gateway, DNS, and Read Receipts

### 9.1 DNS auctions and CLI flows

- `node/src/gateway/dns.rs` maintains auctions (`DomainAuctionRecord`) and stakes (`StakeEscrowRecord`) under `SimpleDb::open_named(names::GATEWAY_DNS, path)`.
- Auction lifecycle:
  1. Seller lists via `contract-cli gateway domain list <domain> <min_bid> --seller <acct> ...`.
  2. Bidders lock stake references (`register_stake`) and submit bids (`contract-cli gateway domain bid ... --stake-ref ref1`). Bids must exceed both `min_bid` and previous `highest_bid`.
  3. Seller (or governance) completes the sale (`contract-cli gateway domain complete`) once `end_ts` passes; ledger events (`LedgerEventRecord`) debit bidders, credit seller/royalty/treasury, and refund unused stake.
  4. If no bids, `cancel` reopens the domain.
- Escrow bookkeeping handles stake reuse, lock/unlock flows, and ledger references for audit reports.
- DNS TXT schema: publications must conform to `docs/spec/dns_record.schema.json` (`{domain, txt, pubkey, sig}`). Example TXT record for `club.block`:

  ```
  club.block TXT "tb-domain=club.block;pubkey=0c2e...;sig=5fb9..."
  ```

  `pubkey`/`sig` are lowercase hex (Ed25519). `dns_lookup` fetches TXT via the configured resolver (default `runtime::net::lookup_txt`), caches verdicts for `VERIFY_TTL = 3600s`, and persists them under `TB_DNS_DB_PATH` (`dns_db` by default).
- Verification and auditing:
  - RPC `gateway.policy` returns the active TXT policy parsed via `gateway::dns::gateway_policy`.
  - `gateway.dns_lookup` exposes the cached verdict (`verified`, `pending`, `failed`) so explorers can show trust badges.
  - CLI `contract-cli net dns verify <domain>` exercises the same path and prints cache hits/misses.

### 9.2 Read receipts, batching, and audit workflow

- Client acknowledgements (`node/src/read_receipt.rs::ReadAck`) contain manifest ID, path hash, bytes served, client hash, domain/provider metadata, optional ad readiness proofs, and Ed25519 signatures. `ReadAck::verify()` checks both signature and privacy proof.
- Gateway receipts (`node/src/gateway/read_receipt.rs`):
  - Stored per hour (`current_epoch(ts) = ts / 3600`). Files are binary CBOR or legacy CBOR for compatibility.
  - `batch(epoch)` loads all receipts, computes a BLAKE3 Merkle root, writes `<epoch>.root`, combines with execution receipts (`exec::batch`), and submits the final anchor to the settlement engine.
  - `reads_since(epoch, domain)` returns `(count, last_ts)` for CLI/RPC reporting.
- CLI runbook:
  1. `contract-cli gateway reads-since --domain example.block --epoch $(date -u +%s)/3600` (custom script) polls the RPC `gateway.reads_since`.
  2. `contract-cli gateway mobile-cache flush` before maintenance to force anchors.
  3. Inspect anchors via `compute_market.recent_roots` to ensure READ_SUB credits landed.

### 9.3 Mobile gateway cache

- Config (`MobileCacheConfig::from_env`):
  - TTL: `TB_MOBILE_CACHE_TTL_SECS` (default 300 s).
  - Sweep interval: `TB_MOBILE_CACHE_SWEEP_SECS` (default 30 s).
  - Entry cap: `TB_MOBILE_CACHE_MAX_ENTRIES` (default 512) and payload cap (`64 KiB` default).
  - Queue cap: `TB_MOBILE_CACHE_MAX_QUEUE` (default 256) for offline submissions.
  - Encryption key: `TB_MOBILE_CACHE_KEY_HEX` (falls back to `TB_NODE_KEY_HEX`).
- The cache uses ChaCha20‑Poly1305; entries persist `stored_at`, `expires_at`, and ciphertext bytes. TTL enforcement sweeps expired records, increments `MOBILE_CACHE_STALE_TOTAL`, and updates `MOBILE_CACHE_QUEUE_BYTES`.
- CLI:
  - `contract-cli gateway mobile-cache status --url http://node:26658 --pretty`.
  - `contract-cli gateway mobile-cache flush` to drop encrypted responses and offline queue state.

### 9.4 Light clients, state streams, and explorer payloads

- Streaming protocol (`node/src/rpc/state_stream.rs`):
  - Upgrades the JSON-RPC connection to a WebSocket and repeatedly sends `StateChunk { seq, tip_height, accounts[], root, compressed }`.
  - Accounts are serialized as sorted sequences (`AccountChunk { address, balance, account_seq, proof }`); proofs are the Merkle siblings from `state::MerkleTrie::prove`.
  - Default cadence: 1 chunk per second; set by the loop delay inside `run_stream`. Errors close the socket; clients should reconnect and resume from the latest `seq`.
- Light-client crate (`crates/light-client`):
  - `SyncOptions` gate by Wi-Fi/charging/battery and compute `GatingReason`. Device probes feed `DeviceStatusSnapshot` which feeds CLI via `contract-cli light-client device status --json`.
  - Headers (`light_client::Header`) mirror the canonical layout (height, prev hash, Merkle roots, VDF data, optional validator key/sig). `verify_and_append` enforces PoW and checkpoint rules.
  - `state_stream` consumers persist deltas under `~/.the_block/light_client.toml` and reuse `fetch_signed` to load jurisdiction-specific overrides.
- Rebate + explorer endpoints:
  - RPC `light_client.rebate_status`/`light_client.rebate_history` query the on-disk proof tracker (`node/src/light_client/proof_tracker.rs`). Results power explorer endpoints:

    ```json
    GET /light_client/top_relayers -> [
      {"id":"aa12…", "pending":3, "total_proofs":42, "total_claimed":100, "last_claim_height":18432}
    ]
    GET /light_client/rebate_history?limit=2 -> {
      "receipts":[
        {"height":17700,"amount":15,"relayers":[{"id":"aa12…","amount":10},{"id":"bb34…","amount":5}]}
      ],
      "next":null
    }
    ```

- CLI quick start:
  1. `contract-cli light-client rebate-status --url http://node:26658` prints local balances.
  2. `contract-cli light-client rebate-history --limit 5 --json` paginates receipts (set `--relayer <hex>` to filter).
  3. `contract-cli light-client device status --json` confirms gating vs overrides before enabling background sync.

- Failure modes:
  - Missing proofs ➜ `state_stream` closes the socket and logs `StateStreamError::InvalidProof`.
  - Device gating ➜ `SyncOptions::gating_reason` returns `WifiUnavailable`, `RequiresCharging`, or `BatteryTooLow`, bubbled to CLI and RPC so users know why sync paused.

---

## 10. Governance, Economics, and Treasury

### 10.1 Emission curves and multipliers

- Logistic base reward `R₀(N) = R_max / (1 + e^{ξ (N - N*)})` (see `node/src/consensus/leader.rs`). Default `R_max` and slope parameters live in `governance::Params`.
- Subsidy multipliers per lane follow the “one dial” formula described in the economic model; governance can clamp or trigger `kill_switch_subsidy_reduction`.
- Explorer and CLI display live multipliers via shared serialization (`foundation_serialization`).

### 10.2 Fee rebates

- Rebates are ledger entries keyed to sender accounts. `node/src/fees/mod.rs` charges consumer traffic by applying rebates first; `FeeLane::Industrial` bypasses rebates and bills direct BLOCK.
- RPC: `peer.rebate_status`, `peer.rebate_claim`. CLI: `contract-cli fees status`, `contract-cli fees claim`.
- Ordering: when submitting a transaction `submit_tx` calculates `total_consumer = amount_consumer + fee`, subtracts available rebate credits, and only then debits BLOCK.

### 10.3 Governance parameters

- `governance/src/params.rs` holds the canonical list and defaults. Appendix E summarises the most important knobs: snapshot cadence, fee floors, subsidy scalars, badge thresholds, scheduler weights, runtime/storage policy selectors, bridge incentive constants, etc.
- Parameters can be toggled via governance proposals or RPC `gov_params`. Explorer `/governance/dependency_policy` exposes history.

### 10.4 Release, rollback, and UI flows

- CLI:
  - `contract-cli gov release propose --hash <build_hash> --artifact <url>` registers new releases; provenance signatures must match `config/release_signers.txt` or env overrides.
  - `contract-cli gov release approve --proposal <id>` collects signatures/quorum.
  - `contract-cli gov release rollback --proposal <id>` triggers rollback windows; explorer `GET /releases` surfaces history.
- UI flows mirror CLI commands via explorer endpoints; log indexer ensures wallet + explorer display identical release states.
- Provenance controls (`node/src/provenance.rs`):
  - Signer list: load from `TB_RELEASE_SIGNERS`, `TB_RELEASE_SIGNERS_FILE`, or `config/release_signers.txt`. `contract-cli gov release signers` prints the active keys.
  - Attestation payloads look like:

    ```json
    {
      "build_hash": "f8c3c5a…",
      "artifact": "https://artifacts.theblock.dev/the-block-f8c3.tar.zst",
      "signatures": [
        {"signer":"41b2…","signature":"0f9d…"},
        {"signer":"9ad0…","signature":"ab21…"}
      ]
    }
    ```

    Each signature is computed over `release:{build_hash}` using Ed25519 keys published in the signer list.
  - `verify_release_signature` ensures at least one configured signer matches; `verify_artifact_signature` checks the downloaded artifact hash against the attestation. Failures increment `BUILD_PROVENANCE_INVALID_TOTAL` and the release proposal is marked quarantined until remediated.

### 10.5 Treasury invariants

- `governance/src/treasury.rs` enforces:
  - Non‑negative unified BLOCK balance tracked in sled.
  - Streaming caps and kill switches (`kill_switch_subsidy_reduction`, `treasury_percent`).
  - Audit log size ≤ 256 entries; older entries roll off but anchor hashes persist.
- Executor snapshots (CLI `contract-cli gov treasury executor`) must show monotonic `last_tick_at`. Settlement anchors are hashed and appended to the audit log for replayability.

### 10.6 Service badge lifecycle

- Tracker (`node/src/service_badge.rs`):
  - Maintains uptime counters, latency samples, and expiry metadata. Badges mint once `total_epochs >= BADGE_MIN_EPOCHS` (default 90) and uptime ≥ `BADGE_ISSUE_UPTIME` (99 %). Revocation triggers when uptime falls below `BADGE_REVOKE_UPTIME` (95 %) or `expiry` passes (`BADGE_TTL_SECS`, default 30 days).
  - Minting calls `register_physical_presence(provider)`, stores a `token`, increments `BADGE_ISSUED_TOTAL`, and records timestamps (`BADGE_LAST_CHANGE_SECONDS`). Burning calls `revoke_provider_badge` and updates `BADGE_ACTIVE`.
- CLI (`cli/src/service_badge.rs`):
  - `contract-cli service-badge issue|revoke` wrap the RPC methods for automation.
  - `contract-cli service-badge verify <token>` confirms authenticity (good for explorer tooltips).
  - `contract-cli service-badge venue register|rotate|status <name>` manages venue presence tokens tracked in `VENUE_TOKENS`.
- Explorer + telemetry:
  - Explorer surfaces badges under `/auxiliary/services` alongside venue crowd snapshots (`VENUE_COUNTS`).
  - Metrics `BADGE_ACTIVE`, `BADGE_LAST_CHANGE_SECONDS`, and `service_badge_token_age_seconds` back the default dashboards so operators can verify SLA compliance before badges expire.

---

## 11. Security, Privacy, and Provenance

### 11.1 Crypto migration steps

- PQ upgrades compile by enabling the `pq-crypto` feature; `node/src/commit_reveal.rs` switches commits to Dilithium, while `dkg/` and `wallet/remote_signer` can operate in both classic and PQ modes.
- Migration playbook:
  1. Build both PQ and classic binaries; verify `crypto_suite` self‑tests.
  2. Rotate commit‑reveal keys using CLI (`contract-cli gov commit-reveal rotate` when available) while telemetry `CRYPTO_MIGRATION_*` metrics stay zero.
  3. Roll out PQ wallets by updating `TB_WALLET_SCHEME=dalek|dilithium`; the wallet crate auto‑detects remote signer capability.

### 11.2 Remote signer security

- `crates/wallet/src/remote_signer.rs` enforces:
  - Mutual TLS with rotating certificates (`RemoteSigner::connect_multi`).
  - Threshold enforcement for multisig (errors: `insufficient_signers`, `invalid_signature`, `timeout`).
  - Telemetry: `remote_signer_request_total`, `remote_signer_success_total`, `remote_signer_latency_seconds`, `remote_signer_key_rotation_total`, `remote_signer_error_total`.
- Threat model mitigations: CLI refuses mixing seeds and remote signers, remote signer addresses must be provided explicitly, and governance audits endpoints via aggregator dashboards.

### 11.3 KYC and jurisdiction authoring

- `crates/jurisdiction` parses signed policy packs: `PolicyPack { region, consent_required, features, parent }`, `SignedPack { pack, signature }`.
- Authoring flow:
  1. Draft JSON (`examples/jurisdiction/*.json`), embed inheritance via `parent`.
  2. Sign with Ed25519; store signature as byte array or base64.
  3. Publish via RPC/CLI (`jurisdiction.set`), referencing governance proposals for region changes.
- Verification: nodes fetch packs via HTTP (`TB_JURISDICTION_FEED`), verify signatures against configured keys, and expose diffs through `jurisdiction.policy_diff`.

### 11.4 Privacy compliance mappings

- `docs/security_and_privacy.md` now owns retention guidance:
  - Read receipts store signatures only, hashed client IDs, and optional ad proofs. TTL per region is defined in jurisdiction packs.
  - Law-enforcement portal logs metadata (request, action, result) and can redact evidence while keeping hash commitments.
  - Mobile cache encrypts everything at rest; eviction policy ensures no stale responses leak when TTL expires.

### 11.5 Release provenance and supply chain

- Provenance enforcement (`node/src/provenance.rs`) loads signer lists from `TB_RELEASE_SIGNERS` or `config/release_signers.txt`, verifies `release:{hash}` signatures, and checks `env!("BUILD_BIN_HASH")` against the current executable.
- Artifacts: `provenance.json` & `checksums.txt` (CI generated) plus `cargo vendor` snapshots. Tagging is blocked unless the dependency registry audit passes.
- CLI `contract-cli system dependencies` + aggregator `/wrappers` expose runtime/transport/storage/coding backend info for compliance review.

---

## 12. APIs, Explorer, and Tooling

### 12.1 RPC method index

See Appendix A for the full method → namespace → params → errors table. Highlights:

- Governance: `gov_list`, `gov_params`, `gov_propose`, `gov_vote`, `gov.release_signers`, `gov.treasury.*`.
- Bridge: `bridge.relayer_status`, `bridge.request_withdrawal`, `bridge.challenge_withdrawal`, etc., all returning structured JSON with error codes mapped from `BridgeError`.
- Compute: `compute_market.stats`, `compute.job_status`, `compute.job_cancel`.
- Gateway: `dns.*`, `gateway.mobile_cache_*`, `gateway.reads_since`.
- Diagnostics: `net.peer_stats*`, `anomaly.label`, `metrics` (OTLP snapshots), `mempool.stats`.

### 12.2 Explorer endpoints and schemas

`explorer/src/lib.rs::router` exposes REST endpoints:

| Endpoint | Purpose |
| --- | --- |
| `GET /blocks/:hash`, `/blocks/:hash/summary`, `/blocks/:hash/payouts`, `/blocks/:hash/proof` | Block inspection + proof generation. |
| `GET /txs/:hash` | Transaction details. |
| `GET /gov/proposals/:id`, `/governance/treasury/*`, `/governance/dependency_policy` | Governance history, treasury status, dependency overrides. |
| `GET /releases` | Release history with attestation metadata. |
| `GET /storage/providers`, `/storage/manifests` | Storage pipeline views. |
| `GET /dex/order_book`, `/dex/trust_lines` | DEX/per‑path data. |
| `GET /compute/jobs` | Active compute orders and receipts. |
| `GET /light_client/top_relayers`, `/light_client/rebate_history` | Light-client stream telemetry. |
| `GET /receipts/provider/:id`, `/receipts/domain/:id` | Read-receipt history for audit. |
| `GET /ad/policy/snapshots*`, `/ad/readiness/status` | Ad policy and readiness snapshots. |

Explorer uses `foundation_sqlite` and shares codec crates, so HTTP responses line up with RPC schemas.

### 12.3 First-party RPC blockers (edge cases & migration tips)

- Long‑running methods (`storage_upload`, `compute.job_cancel`) stream progress; legacy clients may block. Use the HTTP client in `crates/httpd` or `cli` wrappers to inherit timeouts/backoffs.
- Governance RPC errors map to `-320xx` codes (see `AuctionError::code`, bridge errors, treasury errors). Legacy JSON-RPC 1.0 clients that assume `code=-32603` must be updated.
- `metrics` endpoint emits Prometheus text; when migrating from external telemetry stacks ensure scrapers handle embedded wrapper metrics and TLS (mutual TLS by default).
- Large responses (e.g., `storage.manifests`, `dex.order_book`) require pagination; explorer endpoints and CLI support filtering to reduce payloads.

### 12.4 Schema inventory

| Schema | Location | When to update |
| --- | --- | --- |
| Fee submission schema | `docs/spec/fee_v2.schema.json` | Any time RPC fee payloads gain/rename fields. |
| DNS record schema | `docs/spec/dns_record.schema.json` | When adding DNS TXT attributes or `.block` metadata. |

Keep the schemas in sync with RPC structs and rerun `mdbook build docs` before merging.

---

## 13. Telemetry, Monitoring, and Probe CLIs

### 13.1 Metric inventory and alerting

See Appendix B for detailed metric descriptions. Key highlights:

| Metric | Meaning | Labels | Units | Suggested alert |
| --- | --- | --- | --- | --- |
| `base_fee` | Current per-byte base fee | n/a | micro‑BLOCK | Alert if `>10x` baseline for >5 min (congestion). |
| `fee_floor_current` | Current fee floor (global); per‑lane floors are in `mempool.stats.fee_floor`. | n/a | micro‑BLOCK | Alert if the floor moves >50 % in <60 s (possible spam). |
| `mempool_evictions_total` | Evicted tx count | n/a | count | Alert if derivative spikes unexpectedly. |
| `partition_events_total` | Partition incidents | n/a | count | Alert on increments; cross‑link to runbook. |
| `quic_handshake_fail_total` | Failed QUIC handshakes | peer,reason | count | Alert on surge for specific provider. |
| `range_boost_queue_depth` | Pending mesh bundles | n/a | count | Alert if > queue cap (default 256). |
| `MOBILE_CACHE_*` | Cache hits/misses/sweeps | action | count | Alert on sustained misses and queue overflows. |
| `compute_sla_violations_total` | SLA breaches | n/a | count | Alert >0 while in `SettleMode::Real`. |
| `bridge_dispute_outcomes_total` | Bridge disputes | asset | count | Alert if >0 without operator action. |
| `treasury_balance_current` | Treasury BLOCK balance | n/a | BLOCK | Alert if drops >X% per day outside scheduled disbursements. |
| `dependency_policy_violation_total` | Dependency drift | wrapper | count | Alert immediately; governance override required. |

### 13.2 Probe CLI cookbook

| Command | Purpose | Expected output | Typical use |
| --- | --- | --- | --- |
| `probe ping-rpc --url http://node:3050 --timeout 5 --prom` | Measures RPC latency and emits Prometheus sample `probe_rpc_ping_seconds`. | JSON summary + `probe_rpc_ping_seconds{}` sample. | Blackbox RPC monitoring. |
| `probe gossip-check --addr 1.2.3.4:4040 --expect-peers 16` | Ensures gossip port reachable and peer count ≥ threshold. | Exit 0 when `peer_count >= expect`. | Overlay health checks. |
| `probe mine-one --miner my-node` | Mines a single block via RPC template submit. | Block hash or error. | Regression testing for PoW/PoS pipeline. |
| `probe tip --rpc http://node:3050` | Prints current height/hash; optional Prom output. | `{"height":123,...}`. | Alert when local tip stalls vs peers. |

Pair probes with dashboards (see `monitoring/README.md`) and aggregator targets for unified observability.

---

## 14. Developer Handbook Highlights

- **Concurrency & Logging**: Use `concurrency::{MutexExt, DashMap}` to avoid poisoned locks; log via `diagnostics::tracing` with structured fields (`component`, `peer`, `lane`, etc.). Never log PII; privacy filters exist for the LE portal only.
- **Debugging**: `contract-cli diagnostics {mempool,gossip,mesh,tls}` surfaces cached stats. Enable `RUST_LOG=trace` plus diagnostics subscriber when reproducing issues. Probe CLI (above) complements runtime diagnostics.
- **Performance & Benchmarks**: Bench harnesses under `benches/` and Grafana dashboards compare results against thresholds in `config/benchmarks/*.thresholds`. Export histograms via `TB_BENCH_PROM_PATH`.
- **Formal Methods & Simulation**: `formal/` crate plus `docs/formal.md` house model checks. `sim/` contains scenario harnesses (dependency faults, DKG latency, bridge threats); run them before altering consensus or governance logic.
- **WASM/VM Debug**: `cli/src/wasm.rs`, `node/src/vm`, `node/src/vm/debugger.rs` provide contract deployment, tracing, and debugging. Workflow: `contract-cli wasm build` → `contract-cli contract deploy` → `contract-cli contract call` → `contract-cli vm trace --tx <hash>`.

---

## Appendices

### Appendix A · RPC Method Index

_All methods speak JSON-RPC 2.0 over HTTP(S) via the in-house `httpd` router. Every request must carry `{"jsonrpc":"2.0","id":<id>,"method":...,"params":...}`; responses mirror the standard result/error envelope. Objects below list field names, types, and whether they are optional (`?`). Unless noted otherwise, numeric fee values are expressed in micro-BLOCK and heights/epochs use unsigned 64-bit integers._

#### ad_market.* (`node/src/rpc/ad_market.rs`)
**Objects**
- `DomainTier`: enum string `"premium" | "reserved" | "community" | "unverified"` derived from `.block` auctions (`node/src/gateway/dns.rs`). Premium/reserved entries require proof of stake ownership before targeting.
- `PresenceBucket`: `{bucket_id,String, kind:"localnet"|"range_boost", region?:String, radius_meters,u16, minted_at_micros?, expires_at_micros?, confidence_bps,u16}`. Proofs originate from LocalNet/Range Boost receipts and expire per `TB_PRESENCE_TTL_SECS`.
- `CohortKeyV2`: `{domain,String, domain_tier:DomainTier, domain_owner?:AccountId, provider?:String, badges:Vec<BadgeId>, interest_tags:Vec<InterestTagId>, presence_bucket?:PresenceBucket, selectors_version,u16}`. Keys serialize under `cohort_v2:<hash>`; v1 tuple keys remain readable for downgrade migrations.
- `SelectorBidSpec`: `{selector_id,String, clearing_price_usd_micros,u64, shading_factor_bps,u16, slot_cap,u32, max_pacing_ppm,u32}`. `clearing_price_usd_micros` is the baseline per‑MiB price for the selector.
- `CampaignSummary`: `{id, advertiser_account, remaining_budget_usd_micros, reserved_budget_usd_micros, selector_budget:Vec<SelectorBidSpec>, creatives[], badges[], interest_tags[], presence_filters[], domain_filters?{include?:DomainTier[],exclude?:DomainTier[]}}`.
- `DistributionPolicy`: `{viewer_percent, host_percent, hardware_percent, verifier_percent}` (unchanged) but now annotated in responses with `selector_overrides?` whenever governance sets selector-specific payouts.
- `CohortPriceSnapshot`: `{key:CohortKeyV2, price_per_mib_usd_micros, selector_budget:Vec<SelectorBidSpec>, target_utilization_ppm, observed_utilization_ppm, privacy_budget_remaining_ppm}`.
- `BudgetSnapshot`: `{generated_at_micros,u64, config:BudgetBrokerConfig, privacy_budget:{total_ppm,u32, remaining_ppm,u32}, campaigns:[{id, kappa,f64, epoch_spend_total_usd_micros, remaining_usd_micros, selector_budget:Vec<SelectorBidSpec>, cohorts[CohortKeyV2]}], uplift?:{selector:String, holdout_fraction_bps,u16, estimated_roas,f64}[]}`.
- `ReadinessSnapshot`: `{ready,bool, window_secs,u32, unique_viewers,u64, host_count,u64, provider_count,u64, thresholds:{min_unique_viewers,min_host_count,min_provider_count}, segments:{domain_tier->{supply_ppm, readiness_score}, interest_tag->{supply_ppm, readiness_score}, presence_bucket->{freshness_histogram, ready_slots}}, privacy_budget:{remaining_ppm,u32, denied:int}, uplift_summary:{selector:String, holdout_fraction_bps,u16, deltar_ppm,i64}, distribution?}`.
- `PolicySnapshot`: stored in `node/ad_policy_snapshot.rs`; serialized as `{epoch,u64, cohort_schema_version,u16, selector_weights[], governance_hash}`.
- `CampaignRegistration`: payload accepted by `ad_market::campaign_from_value`. Schema: `{id,String, advertiser_account,String, creatives[], metadata, selector_budget:Vec<SelectorBidSpec>, interest_tags:Vec<InterestTagId>, badges:Vec<BadgeId>, presence_filters?:{allow?:PresenceBucketSpec[], deny?:PresenceBucketSpec[]}, domain_filters?:{include?:DomainTier[], exclude?:DomainTier[]}, conversion_value_rules?:{currency_code?,"USD" default, default_value_usd_micros?, value?, attribution_window_secs?, selector_weights?:[{selector_id, weight_ppm,u32}]}}`.
- `ConversionRecord`: `{campaign_id, creative_id, advertiser_account, assignment:{fold,u8,in_holdout,bool,propensity,f64}, value_usd_micros?, value?, currency_code?, attribution_window_secs?, selector_weights?:[{selector_id,String, weight_ppm,u32}], occurred_at_micros?, device_link?:{device_hash,String, opt_in,bool}}`. Requires `Authorization: Advertiser <account>:<token>`; token hash stored under `metadata["conversion_token_hash"]`.
- `PresenceCohortSummary`: `{bucket:PresenceBucket, ready_slots,u64, privacy_guardrail:String, selector_prices:Vec<SelectorBidSpec>, freshness_histogram:{under_1h_ppm,u32, hours_1_to_6_ppm,u32, hours_6_to_24_ppm,u32, over_24h_ppm,u32}}` returned by `ad_market.list_presence_cohorts`.
- **Selection proof verification**: Snark-backed ad selection receipts always enforce circuit revision, commitment, transcript digest, witness commitment count, proof length, proof bytes digest, and verifying-key digest checks. The `integration-tests` feature no longer bypasses validation for unit tests; synthetic fixtures must provide internally consistent metadata and proof payloads.
- **Receipt budgets + pricing loop**: Receipt inclusion is guarded by dual deterministic budgets rather than a single magic count. Blocks enforce `B_cap = RECEIPT_BYTE_BUDGET` (bytes) and `V_cap = RECEIPT_VERIFY_BUDGET` (verify units), with a derived fuse `N_max = min(HARD_RECEIPT_CEILING, floor(B_cap / MIN_RECEIPT_BYTE_FLOOR), floor(V_cap / MIN_RECEIPT_VERIFY_UNITS))`. Verify units are deterministic per receipt (base signature cost plus small type bumps; storage scales modestly with bytes, compute adds a verified flag premium, energy/ad add tiny fixed bumps) so no node can “buy” unlimited CPU with tiny receipts. Budget targets drive two log-domain EIP-1559 controllers: `ln F_{b+1} = ln F_b + clamp((B_used - B_target)/(B_target * d_B), -gamma_B, +gamma_B)` and `ln G_{b+1} = ln G_b + clamp((V_used - V_target)/(V_target * d_V), -gamma_V, +gamma_V)`, where `B_target = RECEIPT_BUDGET_TARGET_FRACTION * B_cap` (same for `V_target`). History rent follows `ln R_{b+1} = ln R_b + clamp((H_b - H_target)/(H_target * d_H), -gamma_H, +gamma_H)`; per-receipt price is `price(r) = F_b * b(r) + G_b * v(r) + R_b * b(r) * tau(r) + tip` with `tau` encoding retention (1 permanent, <1 expiring, >1 toxic waste discouragement). This keeps the chain bandwidth-safe now and landfill-safe over decades.
- **Receipt sharding + DA window**: Receipt commitments are moving from a single global list to per-shard roots included in macro-blocks. Each shard produces `receipt_root[shard_id]`; macro-block headers commit the vector of roots so receipt throughput scales with shard parallelism. Receipt validation is stateless-ish: append-only commitments with minimal state writes; block builders aggregate `(count, bytes, verify_units)` per shard and feed those into the global EIP-1559 controllers. High-volume receipts (energy oracle, ad publisher) SHOULD aggregate signatures (threshold or batch/aggregate) so `v(r)` drops from “N sig verifications” to “1 aggregated check,” effectively raising the usable `V_cap` without changing consensus rules. Commitment receipts MAY reference blobs transported over the blob chain; blobs must stay available for a fixed window `W` days for disputes/audit, after which persistence falls to the storage market. Audit/dispute flows use the on-chain commitment root plus blob inclusion proofs; plaintext/content keys never sit on-chain.

**Common errors**: `-32602` invalid params, `-32603` internal/disabled, `-32000` duplicate campaign, `-32001` unknown campaign, `-32002` unknown creative, `-32030..-32033` advertiser auth failures.
Additional selector-specific errors: `-32034` invalid presence bucket (expired/unsupported), `-32035` forbidden selector combination per privacy policy, `-32036` unknown interest tag/domain tier, `-32037` insufficient privacy budget, `-32038` holdout overlap, `-32039` mismatched selector weights.

| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `ad_market.inventory` | `{selector_filter?:{domain_tier?,interest_tag?,presence_bucket_id?,badge?}}` | `{status:"ok",distribution:DistributionPolicy,oracle:{price_usd_micros},campaigns:[CampaignSummary],cohort_prices:[CohortPriceSnapshot],selector_prices:Vec<SelectorBidSpec>}` | `-32603` when the market handle is not configured, `-32035` selector violates privacy |
| `ad_market.list_campaigns` | `{selector_filter?}` | `{status:"ok",campaigns:[CampaignSummary]}` | `-32603` when disabled |
| `ad_market.distribution` | `{selector_id?}` | `{status:"ok",distribution:DistributionPolicy,selector_overrides?:Vec<SelectorBidSpec>}` | `-32603` |
| `ad_market.budget` | `{campaign_id?}` | `{status:"ok",config:BudgetBrokerConfig,campaigns:[...],delta:{...},privacy_budget:{...}}` mirroring `budget_snapshot_to_value` | `-32603` |
| `ad_market.broker_state` | none | Same payload as `ad_market.budget` plus pacing deltas and competitiveness analytics per selector | `-32603` |
| `ad_market.readiness` | `{selector_filter?}` | `ReadinessSnapshot` including per-selector inventory depth, presence-proof freshness histograms, and privacy budget gauges; optional `distribution` only when live market data is attached | `-32603` when readiness handle unset, `-32037` privacy guardrail triggered |
| `ad_market.policy_snapshot` | `{epoch}` | Snapshot JSON from `ad_policy_snapshot::load_snapshot` or `{status:"not_found"}` | `-32602` missing epoch |
| `ad_market.policy_snapshots` | `{start_epoch?,end_epoch?}` (`end_epoch` defaults to `chain_height/120`) | `{snapshots:[PolicySnapshot]}` | none (empty list when no snapshots) |
| `ad_market.register_campaign` | `CampaignRegistration` | `{status:"ok"}` on success | `-32603` when market disabled/persistence error, `-32602` invalid payload, `-32000` duplicate campaign |
| `ad_market.record_conversion` | `ConversionRecord` body + advertiser auth header | `{status:"ok",conversion_summary:{authenticated_total,rejected_total,error_counts,last_error?,last_authenticated_at?}}` | `-32603` disabled/internal, `-32602` invalid payload, `-32001` unknown campaign, `-32002` unknown creative, `-32030` auth missing, `-32031` advertiser mismatch, `-32032` token missing, `-32033` invalid token, `-32039` selector weight mismatch |
| `ad_market.list_presence_cohorts` | `{region?,domain_tier?,min_confidence_bps?,interest_tag?}` | `{status:"ok",cohorts:[PresenceCohortSummary],privacy_budget:{remaining_ppm,denied_ppm,cooldown_remaining,denied_count}}` | `-32602` invalid filter, `-32034` stale bucket, `-32037` privacy guardrail |
| `ad_market.reserve_presence` | `{campaign_id,String, presence_bucket_id,String, slot_count,u32, expires_at_micros?, selector_budget?:Vec<SelectorBidSpec>}` | `{status:"ok",reservation_id,String,expires_at_micros,u64}` | `-32001` unknown campaign, `-32034` invalid bucket, `-32035` forbidden selector combo, `-32037` insufficient privacy budget |

#### Analytics, anomaly labeling, and settlement probes
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `analytics` (`node/src/rpc/analytics.rs`) | `{domain?}` filters aggregator buckets | `AnalyticsStats { domain, bytes, requests, distribution }` as defined in `crate::rpc::analytics` | `-32603` when telemetry feature disabled |
| `anomaly.label` | `{label}` string used for ML feedback | `{status:"ok"}` | none |
| `settlement_status` | `{provider?}` | `{mode:"dryrun"|"real"|"armed", balance?}` when provider supplied | none |
| `settlement.audit` | none | Full settlement audit JSON (`compute_market::settlement_audit`) with balances, SLA queue, outstanding receipts | propagates `MarketError` codes (`-32040` range) |

#### Account queries and ledger helpers
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `balance` | `{address}` | `{amount}` balance pulled from `accounts` map (0 when missing) | none |
| `ledger.shard_of` | `{address}` | `{shard}` numeric shard ID as computed by `ledger::shard_of` | none |

#### Bridge (`node/src/rpc/bridge.rs`)
**Objects**
- `RelayerStatusRequest`: `{relayer,String, asset?}`.
- `BondRelayerRequest`: `{relayer,String, amount,u64}`.
- `ClaimRewardsRequest`: `{relayer,String, amount,u64, approval_key}`.
- `VerifyDepositRequest`: `{asset?, relayer?, user?, amount?, header:PowHeader, proof:Proof, relayer_proofs:RelayerProof[]}`.
- `RequestWithdrawalRequest`: `{asset?, relayer?, user?, amount?, relayer_proofs[]}`.
- `ChallengeWithdrawalRequest`: `{asset?, commitment, challenger}`.
- `FinalizeWithdrawalRequest`: `{asset?, commitment}`.
- `SubmitSettlementRequest`: `{asset?, settlement:ExternalSettlementProof}`.
- `PendingWithdrawalsRequest`: `{asset?, relayer?, limit?, cursor?}` → paginated list of `PendingWithdrawalInfo`.
- `ActiveChallengesRequest`: `{asset?, limit?, cursor?}`.
- `RelayerQuorumRequest`: `{asset?, epoch?}` returns `RelayerQuorumInfo`.
- `RewardClaimsRequest` / `RewardAccrualsRequest`: `{asset?, relayer?, limit?, cursor?}`.
- `SettlementLogRequest`: `{asset?, limit?, cursor?}` returning historical `SettlementRecord` entries.
- `RelayerAccountingRequest`: `{asset?, relayer}` summarising stakes, duties, and payouts.
- `DutyLogRequest`: `{asset?, relayer?, duty_kind?, cursor?, limit?}`.
- `DepositHistoryRequest`: `{asset?, relayer?, user?, cursor?, limit?}`.
- `SlashLogRequest`: `{asset?, relayer?, cursor?, limit?}`.
- `AssetsRequest`: `{}` → returns `BridgeAssetSnapshot` entries.
- `ConfigureAssetRequest`: `{asset, channel_config:ChannelConfig}` for governance.

**Common errors** map bridge errors to RPC codes (`-32002` invalid proof, `-32007` duplicate withdrawal, `-32010` challenge window open, etc.) plus `-32602` invalid params and `-32000` busy DB lock.

| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `bridge.relayer_status` | `RelayerStatusRequest` | `RelayerInfo {relayer,bond,pending_rewards,duty_state}` | bridge error codes |
| `bridge.bond_relayer` | `BondRelayerRequest` | `{status:"ok"}` | `-32014` insufficient bond, storage errors |
| `bridge.claim_rewards` | `ClaimRewardsRequest` | `{claimed, pending}` | `-32015/-32017` reward validation errors |
| `bridge.verify_deposit` | `VerifyDepositRequest` | `DepositReceipt` (+ emission details) when proof checks succeed | proof errors `-32002`, replay `-32006`, storage `-32013` |
| `bridge.request_withdrawal` | `RequestWithdrawalRequest` | `PendingWithdrawalInfo` plus commitment hash | duplicates `-32007`, insufficient bond `-32014` |
| `bridge.challenge_withdrawal` | `ChallengeWithdrawalRequest` | `{status:"ok", challenge:ChallengeRecord}` | `-32008` missing withdrawal, `-32009` already challenged |
| `bridge.finalize_withdrawal` | `FinalizeWithdrawalRequest` | `{status:"ok"}` upon releasing funds | `-32010` challenge window open |
| `bridge.submit_settlement` | `SubmitSettlementRequest` | `{status:"ok"}` after settlement proof accepted | settlement proof errors `-32018..-32023` |
| `bridge.pending_withdrawals` | `PendingWithdrawalsRequest` | `{items:[PendingWithdrawalInfo], next_cursor?}` | none |
| `bridge.active_challenges` | `ActiveChallengesRequest` | `{items:[ChallengeRecord], next_cursor?}` | none |
| `bridge.relayer_quorum` | `RelayerQuorumRequest` | `RelayerQuorumInfo {epoch, members[], threshold}` | none |
| `bridge.reward_claims` | `RewardClaimsRequest` | `{items:[RewardClaimRecord], next_cursor?}` | none |
| `bridge.reward_accruals` | `RewardAccrualsRequest` | `{items:[RewardAccrualRecord], next_cursor?}` | none |
| `bridge.settlement_log` | `SettlementLogRequest` | `{items:[SettlementRecord], next_cursor?}` | none |
| `bridge.dispute_audit` | `DisputeAuditRequest` | `{items:[DisputeAuditRecord], next_cursor?}` | none |
| `bridge.relayer_accounting` | `RelayerAccountingRequest` | `{relayer, stake, duty_totals, slash_totals}` | none |
| `bridge.duty_log` | `DutyLogRequest` | `{items:[DutyRecord], next_cursor?}` | none |
| `bridge.deposit_history` | `DepositHistoryRequest` | `{items:[DepositReceipt], next_cursor?}` | none |
| `bridge.slash_log` | `SlashLogRequest` | `{items:[SlashRecord], next_cursor?}` | none |
| `bridge.assets` | `AssetsRequest` | `{assets:[BridgeAssetSnapshot]}` | none |
| `bridge.configure_asset` | `ConfigureAssetRequest` | `{status:"ok"}` after writing channel config | governance rejects via bridge error codes |

#### Domain registry and gateway surfaces (`node/src/gateway/dns.rs`, `node/src/service_badge.rs`, `node/src/gateway/mobile_cache.rs`)
DNS methods share the JSON schema under `docs/spec/dns_record.schema.json`. Auctions/stakes use sled-backed objects `{domain, owner, reserve, bidders[], expires_at}`.

| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `dns.publish_record` | `DnsRecord` JSON (TXT + proof metadata) | `{status:"ok", record_hash}` | schema validation errors surfaced via `gateway::dns::Error` codes (forwarded via `rpc_error`) |
| `dns.list_for_sale` | `{domain,reserve, ttl_blocks}` | `{status:"ok"}` once sale is staged | same as above |
| `dns.place_bid` | `{domain,bid,bidder}` | `{status:"ok", leading_bid}` | bidding window/price errors via dns error codes |
| `dns.complete_sale` | `{domain}` | `{status:"ok", new_owner}` | `not_found`, `auction_open` style dns errors |
| `dns.cancel_sale` | `{domain}` | `{status:"ok"}` | dns errors |
| `dns.register_stake` | `{domain,stake}` | `{status:"ok", stake_id}` | dns errors |
| `dns.withdraw_stake` | `{stake_id}` | `{status:"ok"}` | dns errors |
| `dns.stake_status` | `{stake_id?}` | `{stakes:[{domain, owner, stake, expires_at}]}` or single entry when ID supplied | dns errors |
| `dns.auctions` | `{domain?}` filter | `{auctions:[{domain,status,reserve,bids,closes_at}]}` | dns errors |
| `gateway.policy` | `{}` or filters (`region?`) | Current gateway policy JSON (rate limits, read-ack requirements, privacy toggles) | none |
| `gateway.reads_since` | `{since_ms}` | `{count, buckets:[{ts_ms,count}]}` built from read-ack batches | none |
| `gateway.venue_status` | `{venue_id}` | `{status:"ok", crowd_size, last_seen}` from `service_badge::venue_status_detail` | `-32602` missing venue_id |
| `gateway.venue_register` | `{venue_id}` | `{status:"ok", venue_id, token, expires_at}` | `-32602` missing ID |
| `gateway.venue_rotate` | `{venue_id}` | `{status:"ok", token, expires_at}` with freshly issued badge | `-32602` missing ID |
| `gateway.dns_lookup` | `{domain}` | DNS proof bundle as returned by `gateway::dns::dns_lookup` (includes TXT contents and audit trail) | dns errors |
| `gateway.mobile_cache_status` | none | Snapshot from `gateway::mobile_cache::status_snapshot()` (entry counts, hits/misses) | none |
| `gateway.mobile_cache_flush` | none | `{status:"ok", flushed_entries}` | none |

#### Compute control plane (`node/src/rpc/compute_market.rs`, `node/src/compute_market/matcher.rs`)
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `compute.job_requirements` | `{job_id}` | Requirements for a queued job (`{slices, profile, deadline_ms, fairness_lane}`) | `MarketError` codes (propagated) |
| `compute.job_status` | `{job_id}` | `{status, submitted_at, matched_at?, provider?, receipts[]}` | market errors |
| `compute.job_cancel` | `{job_id, reason?, requester}` (WebSocket upgrade supported) | `{status:"ok"}` and cancels active batch | `MarketError::Canceled` style codes |
| `compute.provider_hardware` | `{provider}` | Provider hardware manifest (CPU, GPU, mem, lanes) from registry | `MarketError::UnknownProvider` |
| `compute.reputation_get` | `{provider}` | `{score, last_updated, evidence[]}` | same as above |
| `compute_arm_real` | `{}` | `{status:"ok"}` after toggling settlement to `SettleMode::Real` | restricted; returns `-32601` when not compiled |
| `compute_cancel_arm` | none | `{status:"ok"}` switching back to dry-run | restricted |
| `compute_back_to_dry_run` | none | `{status:"ok"}` (alias) | restricted |

| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `compute_market.stats` | `{accelerator?:\"fpga\"|\"tpu\"}` filter | Aggregated market stats `{lanes:[{lane, queued, matched, avg_fee}], receipts_pending}` | none |
| `compute_market.scheduler_stats` | none | Raw scheduler view (per-lane queues, fairness windows) | none |
| `compute_market.scheduler_metrics` | none | `scheduler::Metrics` structure (latency, starvation counters, lane deadlines) | none |
| `compute_market.recent_roots` | `{n?}` | Latest `n` micro-shard roots used for receipts | none |
| `compute_market.provider_balances` | none | `{providers:[{provider, balance}]}` | none |
| `compute_market.audit` | none | Extended audit log incl. fairness window snapshots, outstanding SLA records | `MarketError` codes |

#### Configuration and consensus
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `config.reload` | `{compaction_secs?, sample_rate_ppm?}` | `{status:"ok", sample_rate_ppm, compaction_secs}` and updates telemetry config | none (values sanitized) |
| `consensus.difficulty` | `{window?}` | Returns difficulty window stats (EMA short/med/long, next target) | none |
| `consensus.pos.register` | `{validator, stake, proof}` | `{status:"ok"}` once validator registered | PoS errors (`-32060` range) |
| `consensus.pos.bond` | `{validator, stake_delta}` | `{status:"ok"}` | same |
| `consensus.pos.unbond` | `{validator, amount}` | `{status:"ok"}` after queueing unbond | same |
| `consensus.pos.slash` | `{validator, reason, proof}` | `{status:"ok"}` after slash recorded | same |
| `pow.get_template` | `{miner}` optional metadata | `{block_template}` with header, transactions, and PoH tick info | `-32070` when mining disabled |
| `pow.submit` | `{block}` serialized candidate | `{status:"ok", accepted:bool}` | `-32071` invalid PoW |

#### Identity, jurisdiction, KYC, and legal portals
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `register_handle` | `{address, handle}` | `{status:"ok"}` once handle stored | `-32602` invalid strings, `-32080` handle taken |
| `resolve_handle` | `{handle}` | `{address}` or `{status:"unknown"}` | none |
| `identity.anchor` | `{address, did_document}` uses DID schema | `{status:"ok", anchor_hash}` | `kyc::Error` mapped to `-32081` family |
| `identity.resolve` | `{address}` | Latest DID document + timeline | none |
| `jurisdiction.status` | `{}` | Active jurisdiction pack metadata (hashes, version, enabled modules) | none |
| `jurisdiction.set` | `{pack_json, signature}` | `{status:"ok"}` once stored in sled-backed `GovStore` | signature errors -> `-32090` |
| `jurisdiction.policy_diff` | `{from, to}` pack hashes | `{diff:{added[], removed[], changed[]}}` | none |
| `kyc.verify` | `{provider, payload}` forwarded to configured KYC plugin | `{status, provider_receipt}` | provider-specific errors |
| `record_le_request` | `{request:LawEnforcementRequest}` per `node/src/le_portal.rs` | `{status:"ok", request_id}` | `-32603` when LE portal disabled |
| `le.list_requests` | `{limit?, cursor?}` | `{items:[{id, agency, type, opened_at, status}], next_cursor?}` | none |
| `le.record_action` | `{request_id, action, actor, notes?}` | `{status:"ok"}` | `-32095` missing request |
| `le.upload_evidence` | `{request_id, filename, content_base64}` | `{status:"ok"}` plus evidence hash | `-32095` |
| `warrant_canary` | `{status, signed_at, signature}` | `{status:"ok"}` after atomic append to log | signature check failures -> `-32096` |

#### Light clients (`node/src/rpc/light.rs`, `node/src/rpc/light_client.rs`, `node/src/rpc/state_stream.rs`)
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `light.headers` | `{from_height, count}` | `{headers:[{height,header_fields...}]}` chunked for device syncing | none |
| `light.latest_header` | none | `{height, header}` for best tip | none |
| `light_client.rebate_status` | `{device_id}` | `{pending, claimed, last_claim_ts, proofs[]}` | `-32030` unauthorized when device not registered |
| `light_client.rebate_history` | `{relayer?, limit?, cursor?}` | `{receipts:[{height, amount, relayers:[{id, amount}]}], next?}` | same as above |
| `state_stream.subscribe` | WebSocket upgrade, no body | Streams `{height, root, accounts_changed, proofs}` batches | HTTP 429 when subscriber cap reached |

#### LocalNet and range-boost helpers
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `localnet.submit_receipt` | `{receipt}` hex-encoded `AssistReceipt` | `{status:"ok"}` if new; `{status:"ignored"}` when duplicate | `-32602` invalid hex or receipt, `-32002` invalid proximity |
| `mempool.stats` | `{lane:"consumer"|"industrial"}` | `{queue_depth, avg_fee, ttl_ms, evictions, qos:{admission_slots, lane}, fee_floor}` from `Blockchain::mempool_stats` | none |
| `mempool.qos_event` | `{lane, window_ms}` recorded for QoS debugging | `{status:"ok"}` | none |
| `mesh.peers` | none | Current range-boost mesh peers (`{id, addr, last_seen_ms}`) | none |
| `microshard.roots.last` | `{n?}` (default 1) | Last `n` micro-shard roots with timestamps | none |

#### Metrics, telemetry, and observability
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `metrics` | none | Prometheus text exposition from `runtime::telemetry::TextEncoder` | none |
| `telemetry.configure` | `{sample_rate_ppm?, compaction_secs?}` (telemetry builds only) | `{status:"ok", sample_rate_ppm, compaction_secs}` after adjusting runtime knobs | `-32603` when telemetry compiled out |

#### Networking and peer management (`node/src/rpc/net.rs` plus inlined helpers)
Most methods share the JSON structures defined in `node/src/net/peer.rs`: `PeerMetrics`, `PeerReputation`, QUIC cert summaries, etc.

| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `net.overlay_status` | none | `{backend:"inhouse"|"stub", path, last_persist_ms, peers:{active,persisted}}` | none |
| `net.peer_stats` | `{peer_id}` (base58) | Full `PeerMetrics` struct converted to JSON | `-32602` invalid peer ID |
| `net.peer_stats_all` | none | `{peers:{id:PeerMetrics}}` | none |
| `net.peer_stats_reset` | `{peer_id}` | `{status:"ok"}` after zeroing counters | validation errors |
| `net.peer_stats_export` | `{path}` | `{status:"ok", path}` after writing JSON export | IO errors -> `-32010` style messages |
| `net.peer_stats_export_all` | `{path}` | same as above but includes all peers | IO errors |
| `net.peer_stats_persist` | none | `{status:"ok"}` after writing sled snapshot of peer metrics | none |
| `net.peer_throttle` | `{peer_id, backoff_secs}` | `{status:"ok"}` after updating throttle map | invalid IDs -> `-32602` |
| `net.backpressure_clear` | `{peer_id}` | `{status:"ok"}` clearing throttle/backpressure state | invalid IDs |
| `net.reputation_sync` | `{source?}` | `{status:"ok", synced}` after forcing reputation gossip | none |
| `net.rotate_cert` / `net.key_rotate` | `{}` | `{status:"ok", fingerprint}` after rotating TLS/peer certs | `-32020` when rotation disabled |
| `net.handshake_failures` | none | `{failures:{reason:count}}` aggregated counters | none |
| `net.quic_stats` | none | QUIC stats snapshot (`{established,total_handshakes,rtt_ms}`) | none |
| `net.quic_certs` | none | `{current:{fingerprint, expires_at}, previous?}` | none |
| `net.quic_certs_refresh` | none | `{status:"ok"}` after forcing re-read from disk | none |
| `net.config_reload` | `{}` | `{status:"ok"}` after reloading peer config from disk | IO errors |
| `net.reputation_show` | `{peer_id}` | `{score,last_update}` | invalid IDs |
| `net.gossip_status` | none | `{fanout, duplicates_dropped, partition_tags[]}` | none |
| `net.dns_verify` | `{domain}` | `{status:"ok", proof}` after verifying `.block` record | dns errors |

#### Node RPC toggles
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `node.get_ack_privacy` | none | `{mode:"enforce"|"permissive"|"off"}` reflecting `ReadAckPrivacyMode` | none |
| `node.set_ack_privacy` | `{mode}` | `{status:"ok"}` after updating `ReadAckPrivacyMode` (enforced via governance guardrails) | `-32602` invalid mode |

#### Peer rebate, rent escrow, scheduler and storage helpers
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `peer.rebate_status` | `{peer_id}` | `{pending, claimed, receipts[]}` | invalid ID |
| `peer.rebate_claim` | `{peer_id}` | `{status:"ok", claimed}` once receipts consumed | invalid ID |
| `rent.escrow.balance` | `{provider}` | `{balance, locked_until}` from `RentEscrow` sled | none |
| `scheduler.stats` | none | Scheduler debug payload `{lanes:[{lane, queued, fairness_deadline_ms, slice_quota}], starvation_counters}` | none |

#### Governance, treasury, and service badges
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `gov_propose` / `submit_proposal` | `{proposer, key, new_value, min, max, deps?, epoch, vote_deadline?}` | `{proposal_id}` | `-32040` invalid proposal, `-32041` when deps invalid |
| `gov_vote` / `vote_proposal` | `{voter, proposal_id, choice:"yes"|"no"|"abstain", epoch}` | `{status:"ok"}` | `-32042` unknown proposal |
| `gov_list` | none | `{proposals:[... active DAG ...]}` | none |
| `gov_params` | `{epoch?}` | `{params}` snapshot from `GOV_PARAMS` | none |
| `gov_rollback_last` / `gov_rollback` | `{epoch}` / `{proposal_id, epoch}` | `{status:"ok"}` after reverting governance store | `-32043` rollback guardrails |
| `gov.release_signers` | none | `{signers:[{name,pubkey}]} ` | none |
| `gov.treasury.balance` | none | `{balance, last_snapshot?, executor?}` from treasury sled | none |
| `gov.treasury.balance_history` | `{cursor?, limit?}` | `{snapshots:[{id, balance, delta, recorded_at, event, disbursement_id?}], next_cursor?, current_balance}` | none |
| `gov.treasury.disbursements` | `{limit?, cursor?}` | `{items:[{proposal_id, amount, recipient, executed_at}], next_cursor?}` | none |
| `service_badge_issue` | `{subject, badge}` (requires signer badge header) | `{status:"ok"}` and records issuance | `-32602` invalid payload, `-32050` when badge invalid |
| `service_badge_revoke` | `{subject, badge}` | `{status:"ok"}` | same |
| `service_badge_verify` | `{token}` | `{valid:bool, subject?, expires_at?}` | none |

#### Settlement toggles, start/stop mining, and TPU aids
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `set_snapshot_interval` | `{interval}` blocks | `{status:"ok"}` updating snapshot cadence (`SimpleDb::snapshot_interval`) | `-32602` invalid interval, `-32050` when below minimum |
| `start_mining` / `stop_mining` | none | `{status:"ok"}` toggling local POW miner thread | `-32601` when miner compiled out |
| `set_difficulty` | `{value}` (debug) | `{status:"ok"}` force-sets PoW difficulty (tests only) | debug-only |
| `tpu` | `{action}` (legacy hook) | `{status:"ok"}` stub used by integration tests | none |

#### Economic tooling, DEX/HTLC, industrial stats, and price board
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `industrial` | none | `{backlog, utilization_ppm, multiplier}` from subsidy governor | none |
| `price_board_get` | `{lane}` | `{lane, base_fee, tip_histogram}` used by wallet coaching | none |
| `dex_escrow_status` | `{escrow_id}` | `{status, deposits, releases}` from `dex::escrow` sled | `-32060` unknown escrow |
| `dex_escrow_release` | `{escrow_id, proof}` | `{status:"ok"}` | same as above |
| `dex_escrow_proof` | `{escrow_id}` | Proof bundle for explorer/CLI | same |
| `htlc_status` | `{swap_id}` | `{status, expires_at, tx_refs}` | unknown swap -> `-32061` |
| `htlc_refund` | `{swap_id}` | `{status:"ok"}` after enforcing refund | same |
| `analytics` already covered; `metrics` above.

#### VM and contract debugging (`node/src/rpc/vm.rs`)
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `vm.estimate_gas` | `{bytecode, calldata, state_root?}` | `{gas_used, trace?}` for WASM execution | `-32070` invalid contract |
| `vm.exec_trace` | `{tx_hash}` | `ExecTrace {steps[], state_changes}` | same |
| `vm.storage_read` | `{contract, key}` | `{value}` | none |
| `vm.storage_write` | `{contract, key, value}` | `{status:"ok"}` (debug only) | none |

#### Miscellaneous roots (whoami, metrics, handles)
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `whoami` | none | `{agent, version, features}` | none |
| `analytics` / `metrics` already covered.
#### Transactions and fee markets
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `submit_tx` | `{tx}` hex-encoded `SignedTransaction` plus optional `{max_fee, tip}` overrides | `{status:"ok"}` when enqueued; `{status:"error", error}` when mempool rejects | `-32602` malformed hex/transaction, propagation errors as strings |
| `price_board_get` | `{lane}` | `{lane, base_fee, tip_histogram:[{percentile, tip_micro}]}` mirroring `node/src/fees.rs` board | none |
| `industrial` | none | `{backlog_bytes, utilization_ppm, multiplier}` from subsidy governor gauges (see § BLOCK subsidy) | none |
| `inflation.params` | none | `{emission_curve:{start_height,end_height,rate}, decay_factor, treasury_split}` read from `docs/economics_and_governance.md` constants | none |

#### Staking / PoS helpers (`node/src/rpc/pos.rs`)
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `stake.role` | `{id, role?}` (role defaults to `validator`) | `{id, role, stake}` from in-memory PoS state | `-32602` missing ID |

#### Storage pipeline convenience RPCs
| Method | Request | Response | Error codes |
| --- | --- | --- | --- |
| `storage_upload` | `{object_id, provider_id, original_bytes, shares, price_per_block, start_block, retention_blocks}` | Delegates to `storage::upload`, returning manifest receipt (`{status:"ok", contract_id}`) | storage errors bubble up via `rpc_error(-32603,...)` |
| `storage_challenge` | `{object_id, provider_id?, chunk_idx, chunk_data, proof, current_block}` | `{status:"ok", challenge_id}` if proof accepted | `-32602` invalid inputs; storage errors |
| `storage.manifests` | `{limit?}` | `{manifests:[{object_id, provider_id, shares, retention_blocks, status}]}` limited to latest `limit` entries | none |
| `storage_provider_profiles` | none | `{providers:[{id, capacity_bytes, maintenance, uptime_ppm}]}` | none |
| `storage_provider_set_maintenance` | `{provider, maintenance:bool}` | `{status:"ok"}` after toggling provider availability | `-32030` unknown provider |
| `storage_incentives` | none | `{rent_escrow:{credits, debits}, rebate_rates:{lane, ppm}}` | none |
 
Storage challenges now expect `chunk_data` (hex-encoded bytes for the challenged chunk) and `proof` (hex-encoded `MerkleProof.path`) so providers must present the actual content that links back to `storage_root`.

### Appendix B · Metric Index

The table below is generated directly from `node/src/telemetry.rs`. Run `python tools/extract_metrics.py --format markdown` whenever the telemetry file changes to keep the appendix in sync. For deployment guidance see `docs/operations.md#metrics-aggregator-ops` and `monitoring/README.md`.

| Name | Type | Labels | Units | Description |
| --- | --- | --- | --- | --- |
| `active_miners` | IntGauge | – | unitless | effective active miners |
| `ad_budget_snapshot_generated_at_micros` | IntGauge | – | microseconds | Timestamp of the most recent budget snapshot in microseconds |
| `aggregator_ingest_total` | IntCounter | – | count | Total peer metric ingests |
| `amm_swap_total` | IntCounter | – | count | Total AMM swaps executed |
| `anomaly_alarm_total` | IntCounter | – | count | Total number of anomaly alarms raised |
| `anomaly_labels_total` | IntCounter | – | count | Anomaly labels submitted for model feedback |
| `badge_active` | IntGauge | – | unitless | Whether a service badge is active (1/0) |
| `badge_issued_total` | IntCounter | – | count | Service badges issued |
| `badge_last_change_seconds` | IntGauge | – | seconds | Unix timestamp of the last badge mint/burn |
| `badge_revoked_total` | IntCounter | – | count | Service badges revoked |
| `balance_overflow_reject_total` | IntCounter | – | count | Transactions rejected due to balance overflow |
| `banned_peers_total` | IntGauge | – | count | Total peers currently banned |
| `base_fee` | IntGauge | – | unitless | current base fee |
| `base_reward` | IntGauge | – | BLOCK | base reward after logistic factor |
| `block_apply_fail_total` | IntCounter | – | count | Blocks that failed atomic application |
| `block_mined_total` | IntCounter | – | count | Total mined blocks |
| `build_provenance_invalid_total` | IntCounter | – | count | Build provenance checks that failed |
| `build_provenance_valid_total` | IntCounter | – | count | Build provenance checks that succeeded |
| `cluster_peer_active_total` | IntGauge | – | count | Unique peers tracked by aggregator |
| `codec_payload_bytes` | HistogramVec | codec, direction, profile, version | bytes | Serialized payload size grouped by codec profile |
| `compute_job_timeout_total` | IntCounter | – | count | Jobs exceeding declared deadlines |
| `compute_sla_automated_slash_total` | IntCounter | – | count | Count of SLA penalties applied automatically by the settlement engine |
| `compute_sla_next_deadline_ts` | IntGauge | – | unitless | Unix timestamp of the next pending compute SLA deadline |
| `compute_sla_pending_total` | IntGauge | – | count | Number of compute jobs with active SLA tracking |
| `config_reload_last_ts` | IntGauge | – | unitless | Unix timestamp of the last successful config reload |
| `consumer_fee_p50` | IntGauge | – | unitless | Median consumer fee |
| `consumer_fee_p90` | IntGauge | – | unitless | p90 consumer fee |
| `courier_flush_attempt_total` | IntCounter | – | count | Total courier receipt flush attempts |
| `courier_flush_failure_total` | IntCounter | – | count | Failed courier receipt flush attempts |
| `dex_escrow_locked` | IntGauge | – | count | Total funds locked in DEX escrow |
| `dex_escrow_pending` | IntGauge | – | unitless | Number of pending DEX escrows |
| `dex_liquidity_locked_total` | IntGauge | – | count | Total liquidity currently locked in DEX escrow |
| `dex_trade_volume` | IntCounter | – | count | Total matched trade quantity across all DEX pairs |
| `did_anchor_total` | IntCounter | – | count | Total number of anchored DID documents |
| `difficulty_clamp_total` | IntCounter | – | count | Retarget calculations clamped to bounds |
| `difficulty_retarget_total` | IntCounter | – | count | Number of difficulty retarget calculations |
| `difficulty_window_long` | IntGauge | – | milliseconds | Long-term EMA of block intervals in ms |
| `difficulty_window_med` | IntGauge | – | milliseconds | Medium-term EMA of block intervals in ms |
| `difficulty_window_short` | IntGauge | – | milliseconds | Short-term EMA of block intervals in ms |
| `dkg_round_total` | IntCounter | – | count | Completed DKG rounds |
| `dns_verification_fail_total` | IntCounter | – | count | Total DNS verification failures |
| `drop_not_found_total` | IntCounter | – | count | drop_transaction failures for missing entries |
| `dup_tx_reject_total` | IntCounter | – | count | Transactions rejected as duplicate |
| `evictions_total` | IntCounter | – | count | Total mempool evictions |
| `fee_floor_current` | IntGauge | – | unitless | Current dynamically computed fee floor |
| `fee_floor_reject_total` | IntCounter | – | count | Transactions rejected for low fee |
| `fee_floor_window_changed_total` | IntCounter | – | ratio | Total governance-triggered fee floor policy reconfigurations |
| `fib_window_base_secs` | IntGauge | – | seconds | base seconds for Fibonacci smoothing |
| `gossip_convergence_seconds` | Histogram | – | seconds | Time for all peers to agree on the network tip |
| `gossip_duplicate_total` | IntCounter | – | count | Duplicate gossip messages dropped |
| `gossip_fanout_gauge` | IntGauge | – | unitless | Current gossip fanout |
| `gossip_latency_seconds` | Histogram | – | seconds | Observed latency hints used for adaptive gossip fanout |
| `gossip_ttl_drop_total` | IntCounter | – | count | Gossip dedup entries removed due to TTL expiry |
| `gov_open_proposals` | IntGauge | – | unitless | Open governance proposals |
| `gov_proposals_pending` | IntGauge | – | unitless | Governance proposals pending activation |
| `gov_quorum_required` | IntGauge | – | unitless | Governance quorum |
| `haar_eta_milli` | IntGauge | – | unitless | eta parameter for burst veto x1000 |
| `hash_ops_total` | IntCounter | – | ratio | Total number of hash operations measured via perf counters |
| `heuristic_mu_milli` | IntGauge | – | unitless | A* heuristic mu x1000 |
| `htlc_created_total` | IntCounter | – | count | HTLC contracts created |
| `htlc_refunded_total` | IntCounter | – | count | HTLC contracts refunded |
| `identity_nonce_skips_total` | IntCounter | – | count | Non-contiguous nonce submissions |
| `identity_replays_blocked_total` | IntCounter | – | count | Rejected identity replay attempts |
| `industrial_admitted_total` | IntCounter | – | count | Industrial lane transactions admitted |
| `industrial_backlog` | IntGauge | – | unitless | Pending industrial compute slices |
| `industrial_deferred_total` | IntCounter | – | count | Industrial lane submissions deferred |
| `industrial_multiplier` | IntGauge | – | unitless | Current industrial subsidy multiplier |
| `industrial_price_per_unit` | IntGauge | – | unitless | Latest price per compute unit |
| `industrial_units_total` | IntCounter | – | count | Total normalized compute units processed |
| `industrial_utilization` | IntGauge | – | percent | Industrial compute utilisation percentage |
| `inter_shard_replay_evict_total` | IntCounter | – | count | Total inter-shard replay cache evictions |
| `invalid_selector_reject_total` | IntCounter | – | count | Transactions rejected for invalid fee selector |
| `job_age_seconds` | Histogram | – | seconds | Time a job waited in the scheduler queue |
| `job_resubmitted_total` | IntCounter | – | count | Jobs resubmitted after provider failure |
| `key_rotation_total` | IntCounter | – | count | Successful peer key rotations |
| `kill_switch_trigger_total` | IntCounter | – | count | Times the subsidy kill switch was activated |
| `light_client_stream_overhead_bytes_total` | IntCounter | – | bytes | Bytes of overhead for light-client streaming |
| `liquidity_rewards_disbursed_total` | IntCounter | – | count | Liquidity mining rewards distributed |
| `lock_poison_total` | IntCounter | – | count | Lock acquisition failures due to poisoning |
| `log_correlation_fail_total` | IntCounter | – | count | Correlation lookups that returned no matching log entries |
| `log_entries_indexed_total` | IntCounter | – | count | Total JSON log entries processed by the offline indexer |
| `log_size_bytes` | Histogram | – | bytes | Size of serialized log events in bytes |
| `match_loop_latency_seconds` | HistogramVec | lane | seconds | Settlement loop latency |
| `mempool_evictions_total` | IntCounter | – | count | Total transactions evicted from the mempool |
| `mempool_size` | IntGaugeVec | lane | unitless | Current mempool size |
| `mempool_size` | GaugeVec | lane | unitless | Current mempool size |
| `miner_reward_recalc_total` | IntCounter | – | count | Times the miner reward logistic factor was recalculated |
| `mobile_cache_entry_bytes` | IntGauge | – | bytes | Total bytes stored in the mobile cache |
| `mobile_cache_entry_total` | IntGauge | – | count | Active mobile cache entries |
| `mobile_cache_evict_total` | IntCounter | – | count | Expired or purged mobile cache entries |
| `mobile_cache_hit_total` | IntCounter | – | count | Total mobile cache hits |
| `mobile_cache_miss_total` | IntCounter | – | count | Total mobile cache misses |
| `mobile_cache_queue_bytes` | IntGauge | – | bytes | Bytes buffered in the mobile offline queue |
| `mobile_cache_queue_total` | IntGauge | – | count | Offline transactions queued for replay |
| `mobile_cache_reject_total` | IntCounter | – | count | Mobile cache insertions rejected by limits |
| `mobile_cache_stale_total` | IntCounter | – | count | Mobile cache entries dropped due to TTL expiry |
| `mobile_cache_sweep_total` | IntCounter | – | count | Number of mobile cache TTL sweeps |
| `mobile_cache_sweep_window_seconds` | IntGauge | – | seconds | Configured sweep interval for the mobile cache |
| `mobile_tx_queue_depth` | IntGauge | – | unitless | Queued mobile transactions awaiting send |
| `orphan_sweep_total` | IntCounter | – | count | Transactions dropped because the sender account is missing |
| `parallel_execute_seconds` | Histogram | – | seconds | Elapsed wall-clock time for ParallelExecutor batches |
| `partition_events_total` | IntCounter | – | count | Number of detected network partitions |
| `partition_recover_blocks` | IntCounter | – | unitless | Blocks replayed during partition recovery |
| `peer_metrics_active` | IntGauge | – | unitless | Number of peers currently tracked for telemetry |
| `peer_metrics_dropped_total` | IntCounter | – | count | Websocket peer metrics frames dropped |
| `peer_metrics_memory_bytes` | IntGauge | – | bytes | Approximate memory used by peer metrics map |
| `peer_metrics_subscribers` | IntGauge | – | unitless | Active peer metrics websocket subscribers |
| `price_weight_applied_total` | IntCounter | – | count | Total price entries adjusted by reputation weight |
| `priority_boost_total` | IntCounter | – | count | Jobs whose priority was boosted due to aging |
| `privacy_sanitization_total` | IntCounter | – | count | Total sanitized payloads |
| `proof_rebates_amount_total` | IntCounter | – | BLOCK | Total BLOCK awarded via proof rebates |
| `proof_rebates_claimed_total` | IntCounter | – | count | Total proof rebate claims |
| `proof_rebates_pending_total` | IntGauge | – | BLOCK | Pending BLOCK rebates awaiting claim |
| `quic_bytes_recv_total` | IntCounter | – | bytes | Total bytes received over QUIC |
| `quic_bytes_sent_total` | IntCounter | – | bytes | Total bytes sent over QUIC |
| `quic_conn_latency_seconds` | Histogram | – | seconds | QUIC connection handshake latency |
| `quic_endpoint_reuse_total` | IntCounter | – | count | Total QUIC endpoint reuse count |
| `quic_fallback_tcp_total` | IntCounter | – | count | Total times QUIC connections fell back to TCP |
| `quic_retransmit_total` | IntCounter | – | count | Total QUIC packet retransmissions |
| `range_boost_enqueue_error_total` | IntCounter | – | count | RangeBoost enqueue attempts dropped due to injection |
| `range_boost_forwarder_fail_total` | IntCounter | – | count | RangeBoost forwarder failures observed |
| `range_boost_queue_depth` | IntGauge | – | unitless | Current number of bundles pending in the RangeBoost queue |
| `range_boost_queue_oldest_seconds` | IntGauge | – | seconds | Age in seconds of the oldest RangeBoost queue entry |
| `range_boost_toggle_latency_seconds` | Histogram | – | seconds | Latency observed between RangeBoost enable/disable toggles |
| `read_selection_proof_latency_seconds` | HistogramVec | attestation | seconds | Selection proof verification latency by attestation |
| `rebate_claims_total` | IntCounter | – | count | Peer rebate claims submitted |
| `rebate_issued_total` | IntCounter | – | count | Rebate vouchers issued |
| `receipt_corrupt_total` | IntCounter | – | count | Corrupted receipt entries on load |
| `receipt_persist_fail_total` | IntCounter | – | count | Receipt persistence failures |
| `release_installs_total` | IntCounter | – | count | Nodes booted with governance-approved releases |
| `release_quorum_fail_total` | IntCounter | – | count | Release submissions rejected due to insufficient provenance signatures |
| `remote_signer_key_rotation_total` | IntCounter | – | count | Remote signer key rotations |
| `remote_signer_latency_seconds` | Histogram | – | seconds | Remote signer latency |
| `remote_signer_request_total` | IntCounter | – | count | Total remote signer requests |
| `remote_signer_success_total` | IntCounter | – | count | Successful remote signer responses |
| `rent_escrow_burned_total` | IntCounter | – | BLOCK | Total BLOCK burned from rent escrow |
| `rent_escrow_locked_total` | IntGauge | – | BLOCK | Total BLOCK locked in rent escrow |
| `rent_escrow_refunded_total` | IntCounter | – | BLOCK | Total BLOCK refunded from rent escrow |
| `reputation_gossip_fail_total` | IntCounter | – | count | Reputation updates that failed verification or were stale |
| `reputation_gossip_latency_seconds` | Histogram | – | seconds | Propagation latency for reputation updates |
| `retrieval_failure_total` | IntCounter | – | count | Total failed proof-of-retrievability challenges |
| `retrieval_success_total` | IntCounter | – | count | Total successful proof-of-retrievability challenges |
| `rpc_bans_total` | IntCounter | – | count | Total RPC bans issued |
| `rpc_latency_seconds` | HistogramVec | module | seconds | Latency histogram per RPC module |
| `rpc_rate_limit_attempt_total` | IntCounter | – | count | RPC requests checked against the rate limiter |
| `rpc_rate_limit_reject_total` | IntCounter | – | count | RPC requests rejected by the rate limiter |
| `runtime_pending_tasks` | IntGauge | – | unitless | Pending async tasks managed by the runtime |
| `runtime_spawn_latency_seconds` | Histogram | – | seconds | Latency observed when spawning tasks on the runtime |
| `scheduler_accelerator_fail_total` | IntCounter | – | count | Accelerator jobs that failed or were cancelled |
| `scheduler_accelerator_miss_total` | IntCounter | – | count | Jobs requiring accelerators that could not be matched |
| `scheduler_accelerator_util_total` | IntCounter | – | count | Jobs requiring accelerators that started successfully |
| `scheduler_active_jobs` | IntGauge | – | unitless | Number of currently assigned jobs |
| `scheduler_class_wait_seconds` | HistogramVec | class | seconds | Wait time per service class before execution |
| `scheduler_match_latency_seconds` | Histogram | – | seconds | Time to perform a scheduler match |
| `scheduler_priority_miss_total` | IntCounter | – | count | High-priority jobs exceeding wait threshold |
| `scheduler_provider_reputation` | Histogram | – | unitless | Distribution of provider reputation scores |
| `scheduler_thread_count` | IntGauge | – | unitless | Current compute scheduler worker threads |
| `session_key_expired_total` | IntCounter | – | count | Expired session keys encountered |
| `session_key_issued_total` | IntCounter | – | count | Session keys issued |
| `settle_applied_total` | IntCounter | – | count | Receipts applied |
| `settle_audit_mismatch_total` | IntCounter | – | count | Receipts failing settlement audit |
| `shard_cache_evict_total` | IntCounter | – | count | Total shard cache evictions |
| `shielded_pool_size` | IntGauge | – | unitless | Number of pending shielded nullifiers |
| `shielded_tx_total` | IntCounter | – | count | Total shielded transactions accepted |
| `sigverify_ops_total` | IntCounter | – | count | Total number of signature verifications measured via perf counters |
| `slashing_burn_total` | IntCounter | – | BLOCK | Total BLOCK burned from slashing penalties |
| `snapshot_created_total` | IntCounter | – | count | Total snapshots created |
| `snapshot_duration_seconds` | Histogram | – | seconds | Snapshot operation duration |
| `snapshot_fail_total` | IntCounter | – | ratio | Total snapshot operation failures |
| `snapshot_interval` | IntGauge | – | unitless | Snapshot interval in blocks |
| `snapshot_interval_changed` | IntGauge | – | unitless | Last requested snapshot interval |
| `snapshot_restore_fail_total` | IntCounter | – | count | Failed snapshot restores |
| `snark_fail_total` | IntCounter | – | count | Failed SNARK proof verifications |
| `snark_verifications_total` | IntCounter | – | count | Successfully verified SNARK proofs |
| `snark_prover_latency_seconds` | HistogramVec | backend | seconds | SNARK prover latency segmented by backend |
| `snark_prover_failure_total` | IntCounterVec | backend | count | SNARK prover failures segmented by backend |
| `startup_ttl_drop_total` | IntCounter | – | count | Expired mempool entries dropped during startup |
| `state_stream_lag_alert_total` | IntCounter | – | count | Clients falling behind alert count |
| `state_stream_subscribers_total` | IntCounter | – | count | Total websocket state stream subscribers |
| `state_sync_overhead_bytes_total` | IntCounter | – | bytes | Bytes of overhead for state sync streaming |
| `storage_chunk_size_bytes` | Histogram | – | bytes | Size of chunks put into storage |
| `storage_compaction_total` | IntCounter | – | ratio | Number of RocksDB compaction operations |
| `storage_compression_ratio` | HistogramVec | algorithm | ratio | Compression ratios achieved per algorithm |
| `storage_contract_created_total` | IntCounter | – | count | Total number of storage contracts created |
| `storage_disk_full_total` | IntCounter | – | count | Number of storage writes that failed due to disk exhaustion |
| `storage_final_chunk_size` | IntGauge | – | unitless | Final preferred chunk size after upload |
| `storage_initial_chunk_size` | IntGauge | – | unitless | Initial chunk size used for object upload |
| `storage_provider_loss_rate` | HistogramVec | provider | unitless | Observed provider loss rate |
| `storage_provider_rtt_ms` | HistogramVec | provider | milliseconds | Observed provider RTT in milliseconds |
| `storage_put_chunk_seconds` | HistogramVec | erasure, compression | seconds | Time to put a single chunk |
| `storage_put_eta_seconds` | IntGauge | – | seconds | Estimated time to upload object in seconds |
| `storage_put_object_seconds` | HistogramVec | erasure, compression | seconds | End-to-end latency for StoragePipeline::put_object |
| `storage_repair_bytes_total` | IntCounter | – | bytes | Total bytes reconstructed by repair loop |
| `subsidy_auto_reduced_total` | IntCounter | – | count | Multiplier auto-reduction events due to inflation guard |
| `subsidy_cpu_ms_total` | IntCounter | – | milliseconds | Total subsidized compute time in ms |
| `telemetry_alloc_bytes` | IntGauge | – | bytes | Telemetry memory allocation in bytes |
| `threshold_signature_fail_total` | IntCounter | – | count | Failed threshold signature verifications |
| `token_bridge_volume_total` | IntCounter | – | count | Volume bridged via token bridge |
| `tokens_created_total` | IntCounter | – | count | Total number of registered tokens |
| `treasury_executor_last_error_seconds` | IntGauge | – | seconds | Timestamp of the last treasury executor error |
| `treasury_executor_last_submitted_nonce` | IntGauge | – | unitless | Latest nonce submitted by the treasury executor |
| `treasury_executor_last_success_seconds` | IntGauge | – | seconds | Timestamp of the last successful treasury executor run |
| `treasury_executor_last_tick_seconds` | IntGauge | – | seconds | Timestamp of the last treasury executor tick |
| `treasury_executor_lease_last_nonce` | IntGauge | – | unitless | Lease watermark nonce retained across treasury executor holders |
| `treasury_executor_lease_released` | IntGauge | – | unitless | Flag indicating the active treasury executor lease has been released |
| `treasury_executor_pending_matured` | IntGauge | – | unitless | Matured treasury disbursements pending execution |
| `treasury_executor_staged_intents` | IntGauge | – | unitless | Staged treasury execution intents awaiting submission |
| `ttl_drop_total` | IntCounter | – | ratio | Transactions dropped due to TTL expiration |
| `tx_admitted_total` | IntCounter | – | count | Total admitted transactions |
| `tx_submitted_total` | IntCounter | – | count | Total submitted transactions |
| `util_var_threshold_milli` | IntGauge | – | unitless | utilisation variance threshold x1000 |
| `vm_gas_used_total` | IntCounter | – | count | Total gas consumed by VM executions |
| `vm_out_of_gas_total` | IntCounter | – | count | VM executions that ran out of gas |
| `vm_trace_total` | IntCounter | – | count | Total VM trace sessions |
| `wal_corrupt_recovery_total` | IntCounter | – | count | WAL entries skipped due to checksum mismatch |
| `wasm_contract_executions_total` | IntCounter | – | count | Total WASM contract executions |
| `wasm_gas_consumed_total` | IntCounter | – | count | Total gas used by WASM contracts |

### Appendix C · SimpleDb Column Family and Prefix Map

| Name | Purpose | Common prefixes/keys |
| --- | --- | --- |
| `default` | Legacy chain state (fallback). | `chain`, `accounts`. |
| `bridge` | Bridge headers, pending withdrawals. | `header:<height>`, `withdrawal:<commitment>`. |
| `compute_settlement` | Settlement engine state (balances, audit log). | `ledger`, `sla_queue`, `sla_history` (legacy `ledger_it` no longer used). |
| `dex_storage` | Order book, trades, escrow locks, AMM pools. | `book`, `trade:*`, `escrow`, `amm/<pool_id>`. |
| `gateway_dns` | Domain auctions, stakes, ownership. | `auction:<domain>`, `stake:<ref>`, `ownership:<domain>`. |
| `gateway_ad_readiness` | Readiness snapshots for ad flows. | `snapshot:<epoch>`. |
| `gossip_relay` | Peer relay state for overlay dedup. | `peer:<id>`. |
| `identity_did` | DID registry entries. | `did:<addr>`. |
| `identity_handle_registry` | Handle → account mappings. | `handles/<handle>`, `owners/<address>`, `nonces/<address>`. |
| `light_client_proofs` | Light-client proof cache. | `header:<height>`. |
| `localnet_receipts` | LocalNet assist receipts. | `receipt:<hash>`. |
| `net_peer_chunks` | Gossip chunk tracking. | `chunk:<root>:<idx>`. |
| `net_bans` | Peer ban records. | `peer:<id>`. |
| `rpc_bridge` | RPC bridge caches. | Implementation-specific. |
| `storage_fs` | Storage pipeline manifests, receipts. | `manifest:<hash>`. |
| `storage_pipeline` | Chunk placement metadata. | `chunk:<id>`. |
| `storage_repair` | Repair state/counters. | `repair:<provider>`. |

### Appendix D · P2P Wire Payloads

| Payload | Fields (order) | Notes |
| --- | --- | --- |
| `Handshake` | `Hello` struct: `network_id[4], proto_version, feature_bits, agent, nonce, transport, quic_addr?, quic_cert?, quic_fingerprint?, quic_fingerprint_previous[], quic_provider?, quic_capabilities[]`. | First message every session. |
| `Hello` | `Vec<SocketAddr>` | Legacy peer advertisement. |
| `Tx` | `SignedTransaction` (foundation serialization). | Used for gossip/broadcast. |
| `BlobTx` | `BlobTx` struct (commitment, payload). | For L2 blobspace. |
| `Block` | `(ShardId, Block)` | Shard IDs live under `ledger::address::ShardId`. |
| `ChainRequest` | `{ from_height }` | Chain sync pull requests. |
| `Chain` | `Vec<Block>` | Fork resolution snapshots. |
| `BlobChunk` | `root[32], index, total, data` | Erasure-coded shard. |
| `Reputation` | `Vec<ReputationUpdate { peer, delta, reason }>` | Synchronises overlay reputation. |

All payloads are serialized via `foundation_serialization::binary_cursor` to avoid serde drift.

### Appendix E · Governance Parameter Catalog (partial)

| Key | Default | Description |
| --- | --- | --- |
| `snapshot_interval_secs` | 30 | Block cadence for ledger snapshots. |
| `consumer_fee_comfort_p90_microunits` | 2 500 | Wallet fee guidance (p90). |
| `fee_floor_window` / `fee_floor_percentile` | 256 / 75 | Rolling window size and percentile for fee floors. |
| `industrial_admission_min_capacity` | 10 | Minimum queue capacity before opening industrial lane. |
| `fairshare_global_max_ppm` | 250 000 | QoS cap per account in parts‑per‑million. |
| `burst_refill_rate_per_s_ppm` | 500 000 (≈30 tx/min) | Token bucket refill rate. |
| `lane_based_settlement_enabled` | 0 | Enable lane-based settlement routing. |
| `lane_consumer_capacity` / `lane_industrial_capacity` | 1000 / 500 | Lane capacities for utilisation tracking. |
| `lane_consumer_congestion_sensitivity` / `lane_industrial_congestion_sensitivity` | 300 / 500 | Congestion sensitivity (k = value / 100). |
| `lane_industrial_min_premium_percent` | 50 | Minimum industrial premium. |
| `lane_target_utilization_percent` | 70 | PI target utilisation. |
| `lane_market_signal_half_life` | 50 | EMA half-life for demand signal (blocks). |
| `lane_market_demand_max_multiplier_percent` | 300 | Max demand multiplier. |
| `lane_market_demand_sensitivity_percent` | 200 | Demand sensitivity. |
| `lane_pi_proportional_gain_percent` / `lane_pi_integral_gain_percent` | 10 / 1 | PI gains (Kp/Ki). |
| `beta_storage_sub`, `gamma_read_sub`, `kappa_cpu_sub`, `lambda_bytes_out_sub` | 50/20/10/5 | Subsidy multipliers (basis points). |
| `read_subsidy_*_percent` | Derived defaults | Split `READ_SUB` across viewer/host/hardware/verifier/liquidity. |
| `treasury_percent` | 0 | Treasury share of fees. |
| `proof_rebate_limit` | 1 | Max proof rebate share. |
| `rent_rate_per_byte` | 0 | Storage rent rate. |
| `kill_switch_subsidy_reduction` | 0 | Emergency knob to damp multipliers. |
| `miner_reward_logistic_target`, `logistic_slope_milli`, `miner_hysteresis` | 100 / ~230 / 10 | Shape of logistic emission curve. |
| `badge_expiry_secs`, `badge_issue/revoke_uptime_percent` | 30 days / 99 % / 95 % | Service badge policy. |
| `jurisdiction_region` | 0 | Active policy pack region (int ID). |
| `ai_diagnostics_enabled` | 0 | Toggles ANN diagnostics. |
| `kalman_r_{short,med,long}` | 1 / 3 / 8 | Kalman filter variance terms for difficulty retune. |
| `scheduler_weight_{gossip,compute,storage}` | 3 / 2 / 1 | Weight of fairness window per workload type. |
| `runtime_backend_policy`, `transport_provider_policy`, `storage_engine_policy` | 0× defaults | Governance-enforced backend selections. |
| `bridge_min_bond`, `bridge_duty_reward`, `bridge_failure_slash`, `bridge_challenge_slash`, `bridge_duty_window_secs` | From `BridgeIncentiveParameters` | Bridge incentive/penalty knobs. |

(Refer to `governance/src/params.rs` for the exhaustive list and serialization order.)

### Appendix F · DNS & Read-Receipt Flowchart

1. `contract-cli gateway domain list` → RPC `dns.list_for_sale` → SimpleDb `auction:<domain>` record.
2. `contract-cli gateway domain bid` (optionally register stake first) → RPC `dns.place_bid`.
3. `contract-cli gateway domain complete` → RPC `dns.complete_sale` → ledger events (seller, royalties, treasury) + CLI confirmation.
4. Gateway serves content; clients issue reads with `ReadAck` headers (`X-TB-Read-Ack`).
5. Gateway writes receipts (`read/<epoch>/<seq>.bin`), batches hourly via RPC `gateway.reads_since` & `gateway.batch_read_receipts`, anchors root through settlement.

### Appendix G · Compute Courier and SLA Quick Reference

| Component | Behaviour |
| --- | --- |
| Courier queue | sled `courier` tree storing `CourierReceipt`. Automatic retries (5 attempts, exponential backoff: 100, 200, 400, 800 ms). |
| SLA scheduler | `Settlement::sla` vector holds active deadlines; `SLA_HISTORY_LIMIT = 256`. |
| CLI hooks | `contract-cli compute courier status`, `contract-cli compute settlement audit` (future extension) read `compute_market.*` RPC responses. |

### Appendix H · AMM & HTLC Math Reference

- AMM invariants:
  - `k = base_reserve * quote_reserve`.
  - Share minting uses geometric mean for first LP; later LPs mint shares proportional to contributions.
  - No fee term yet; add `fee_bps` multiplier before recomputing `k` if needed.
- HTLC scripts:
  - Two intents match if `hash` and `amount` match. Scripts follow `htlc:<hash_hex>:<timeout>`.
  - Ensure `timeout_B > timeout_A + safety_margin` so refunds propagate in opposite order.
  - Replay safety: matched intents are removed; duplicates require a new hash.

---

**Reminder:** always cross‑check these summaries against the referenced source files before modifying behaviour. If a doc section drifts from the code, patch this file first to keep the expectations self‑consistent.
